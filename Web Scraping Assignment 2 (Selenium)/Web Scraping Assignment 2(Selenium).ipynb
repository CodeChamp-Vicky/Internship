{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe41ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installed selenium\n",
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Required Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f77b2",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location.\n",
    "\n",
    "    You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "    You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caabadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')   # creating instance of webdriver\n",
    "def scraper1(a):\n",
    "    driver.get(a)      # Loading Webpage\n",
    "    time.sleep(5)      # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding search field for designation on webpage using its xpath\n",
    "    search_field_designation=driver.find_element_by_class_name('suggestor-input')\n",
    "    search_field_designation.send_keys('Data Analyst')  # sending input to be searched in search field\n",
    "    \n",
    "    # finding search field for location on webpage using its xpath\n",
    "    search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "    search_field_location.send_keys('Bangalore')  # sending location input to be filled in the location search field\n",
    "    \n",
    "    # finding search button on webpage using its xpath\n",
    "    search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "    search_button.click() # clicking search button to get to the desired page\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Empty lists to store desired information\n",
    "    job_titles=[]\n",
    "    company_names=[]\n",
    "    experience_list=[]\n",
    "    location_list=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage\n",
    "    count = 0\n",
    "    while (count < 10):\n",
    "        \n",
    "        title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "        company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "        experience_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]')\n",
    "        location_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above\n",
    "        for i in range(len(title_tags)):\n",
    "            if count >=10:\n",
    "                break\n",
    "            else:\n",
    "                job_titles.append(title_tags[i].text)\n",
    "                company_names.append(company_tags[i].text)\n",
    "                experience_list.append(experience_tags[i].text)\n",
    "                location_list.append(location_tags[i].text)\n",
    "            count += 1\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    job_vacancies={'Job-Titles':job_titles,'Company-Names':company_names,'Experience-Required':experience_list,'Job-Locations':location_list}\n",
    "    df=pd.DataFrame(job_vacancies)  # creating dataframe\n",
    "    df.index=range(1,len(df)+1)     # assigning indices to the dataframe\n",
    "    print(df)                       # printing dataframe\n",
    "    \n",
    "    \n",
    "url='https://www.naukri.com/'       # url of the website\n",
    "scraper1(url)                       # calling scraper1 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cfcac",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location.\n",
    "    You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5aeaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')    # creating instance of webdriver\n",
    "def scraper2(b):\n",
    "    driver.get(b)                              # loading website\n",
    "    time.sleep(5)                              # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding search field for designation on webpage using its xpath     \n",
    "    search_field_designation=driver.find_element_by_class_name('suggestor-input')  \n",
    "    search_field_designation.send_keys('Data Scientist')  # sending input to be searched in search field\n",
    "    \n",
    "    # finding search field for location on webpage using its xpath\n",
    "    search_field_location=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "    search_field_location.send_keys('Bangalore') # sending input location to be searched in search location field\n",
    "    \n",
    "    # finding search button on webpage using its xpath\n",
    "    search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "    search_button.click() # clicking search button to get to the desired page\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Empty lists to store desired information    \n",
    "    job_titles=[]\n",
    "    company_names=[]\n",
    "    location_list=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage    \n",
    "    count = 0\n",
    "    while (count < 10):\n",
    "        title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "        location_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]')\n",
    "        company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above        \n",
    "        for i in range(len(title_tags)):\n",
    "            if count >= 10:\n",
    "                break\n",
    "            else:\n",
    "                job_titles.append(title_tags[i].text)\n",
    "                company_names.append(company_tags[i].text)\n",
    "                location_list.append(location_tags[i].text)\n",
    "            count += 1\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    job_vacancies={'Job-Titles':job_titles,'Company-Names':company_names,'Job-Locations':location_list}\n",
    "    df=pd.DataFrame(job_vacancies)  # creating dataframe\n",
    "    df.index=range(1,len(df)+1)     # assigning indices to the dataframe\n",
    "    print(df)                       # printing dataframe\n",
    "    \n",
    "    \n",
    "url='https://www.naukri.com/'       # url of the website\n",
    "scraper2(url)                       # calling scraper2 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1180dbc",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "        \n",
    "        You have to use the location and salary filter.\n",
    "        You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "        You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is\n",
    "        “Delhi/NCR”. \n",
    "        The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "    \n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')      # creating instance of webdriver\n",
    "def scraper3(c):\n",
    "    driver.get(c)                                # loading website\n",
    "    time.sleep(5)                                # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding search designation field on webpage using its xpath\n",
    "    search_field_designation=driver.find_element_by_class_name('suggestor-input')\n",
    "    search_field_designation.send_keys('Data Scientist')   # sending input to be searched to the search designation field\n",
    "    \n",
    "    # finding search button on webpage using its xpath\n",
    "    search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "    search_button.click()\n",
    "    time.sleep(5)                               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding location filter on webpage using its xpath\n",
    "    loc_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[3]/label/i\")\n",
    "    loc_filter.click()                           # clicking on location filter\n",
    "    time.sleep(5)                                # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding salary filter on webpage using its xpath\n",
    "    sal_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "    sal_filter.click()                          # clicking on salary filter\n",
    "    time.sleep(5)                               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Empty lists to store desired information        \n",
    "    job_titles=[]\n",
    "    company_names=[]\n",
    "    experience_list=[]\n",
    "    location_list=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage    \n",
    "    count = 0\n",
    "    while (count < 10):\n",
    "        \n",
    "        title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "        company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "        experience_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]')\n",
    "        location_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]')\n",
    "    \n",
    "        # Appending fetched data to the respective empty lists created above        \n",
    "        for i in range(len(title_tags)):\n",
    "            if count >=10:\n",
    "                break\n",
    "            else:\n",
    "                job_titles.append(title_tags[i].text)\n",
    "                company_names.append(company_tags[i].text)\n",
    "                experience_list.append(experience_tags[i].text)\n",
    "                location_list.append(location_tags[i].text)\n",
    "            count += 1\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    job_vacancies={'Job-Titles':job_titles,'Company-Names':company_names,'Experience-Required':experience_list,'Job-Locations':location_list}\n",
    "    df=pd.DataFrame(job_vacancies)        # creating dataframe\n",
    "    df.index=range(1,len(df)+1)           # assigning indices to the dataframe\n",
    "    print(df)                             # printing dataframe\n",
    "    \n",
    "    \n",
    "url='https://www.naukri.com/'            # url of the website\n",
    "scraper3(url)                            # calling scraper3 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3913c",
   "metadata": {},
   "source": [
    "Q4. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    \n",
    "    1. Brand\n",
    "    2. Product Description\n",
    "    3. Price\n",
    "\n",
    "    The attributes which you have to scrape is ticked marked in the below image:\n",
    "    To scrape the data you have to go through following steps:\n",
    "        \n",
    "        1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "        2. Enter “sunglasses” in the search field where “search for products, brands andmore” is written and click the \n",
    "        search icon\n",
    "        3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the required data \n",
    "        as usual.\n",
    "        4. After scraping data from the first page, go to the “Next” Button at the bottom ofthe page, then click on it.\n",
    "        5. Now scrape data from this page as usual\n",
    "        6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver=webdriver.Chrome('chromedriver.exe')        # creating instance of webdriver\n",
    "def scraper4(a):                                   \n",
    "    driver.get(a)                                  # loading the website\n",
    "    time.sleep(5)                                  # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding close button of pop-up window using xpath and clicking on it\n",
    "    close_popup=driver.find_element_by_xpath('/html/body/div[2]/div/div/button').click()\n",
    "    \n",
    "    # finding search field using xpath and sending desired input to the field\n",
    "    search_field_items=driver.find_element_by_class_name('_3704LK').send_keys('sunglasses')\n",
    "    \n",
    "    # finding search button/option and clicking on it\n",
    "    search_option=driver.find_element_by_class_name('L0Z3Pu').click()\n",
    "    time.sleep(5)               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Empty lists to store desired information        \n",
    "    brand_names= []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "    discount=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage    \n",
    "    count = 0\n",
    "    while (count < 100):\n",
    "        \n",
    "        brand_tags = driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "        desc_tags = driver.find_elements_by_xpath(\"//a[starts-with (@class,'IRpwTa')]\")\n",
    "        price_tags = driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "        discount_tags=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]/span')\n",
    "        # Appending fetched data to the respective empty lists created above        \n",
    "        for i in range(len(brand_tags)):\n",
    "            if count >= 100:\n",
    "                break\n",
    "            else:\n",
    "                brand_names.append(brand_tags[i].text)\n",
    "                descriptions.append(desc_tags[i].text)\n",
    "                prices.append(price_tags[i].text[1:])\n",
    "                discount.append(discount_tags[i].text)\n",
    "            count += 1\n",
    "            \n",
    "        # finding next button to go the next page using xpath\n",
    "        next_button=(driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')[-1])  \n",
    "        next_button.click()      # clicking on next button\n",
    "        time.sleep(5)            # Assigning time of 5 secs to load the page properly\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    product_details={'Brand Of Product':brand_names,'Description Of Product':descriptions,'Discounted Price':prices,'Discount Offered':discount}\n",
    "    df=pd.DataFrame(product_details)     # creating dataframe\n",
    "    df.index=range(1,len(df)+1)          # assigning indices to the dataframe\n",
    "    print(df)                            # printing dataframe\n",
    "    \n",
    "    \n",
    "url='https://www.flipkart.com/'          # url of the website\n",
    "scraper4(url)                            # calling scraper4 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6de4f",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "\n",
    "    You have to go the link:\n",
    "    https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n",
    "   \n",
    "    When you will open the above link you will reach to the below shown webpage.\n",
    "\n",
    "    As shown in the above page you have to scrape the tick marked attributes.\n",
    "\n",
    "    These are:\n",
    "    1. Rating\n",
    "    2. Review summary\n",
    "    3. Full review\n",
    "    4. You have to scrape this data for first 100 reviews.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21352711",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')          # creating instance of webdriver\n",
    "def scraper5(b):\n",
    "    driver.get(b)                                    # loading the website\n",
    "    time.sleep(5)                                    # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding all review button and clicking on it\n",
    "    all_review_button=driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]').click()\n",
    "    time.sleep(5)           # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Empty lists to store desired information            \n",
    "    ratings=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage        \n",
    "    count=0\n",
    "    while count<100: \n",
    "        ratings_tags=driver.find_elements_by_xpath('//div[@class=\"row\"]//child::div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        review_summary_tags=driver.find_elements_by_xpath('//div[@class=\"row\"]//child::div[@class=\"_3LWZlK _1BLPMq\"]//following-sibling::p')\n",
    "        full_review_tags=driver.find_elements_by_xpath('//div[@class=\"row\"]//child::div[@class=\"t-ZTKy\"]/div')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above                \n",
    "        for i in range(len(ratings_tags)):\n",
    "            if count>=100:\n",
    "                break\n",
    "            else:\n",
    "                ratings.append(ratings_tags[i].text)\n",
    "                review_summary.append(review_summary_tags[i].text)\n",
    "                full_review.append(full_review_tags[i].text.replace(\"\\n\",\" \").replace(\"READ MORE\",\" \").strip())\n",
    "            count+=1\n",
    "            \n",
    "        # finding next button to go the next page using xpath and clicking on it\n",
    "        next_button=driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]//child::span[contains(.,\"Next\")]').click()\n",
    "        time.sleep(5)     # Assigning time of 5 secs to load the page properly\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    review_dict={'Ratings':ratings,'Review Summary':review_summary,'Full Review':full_review}\n",
    "    df=pd.DataFrame(review_dict)  # creating dataframe\n",
    "    df.index=range(1,len(df)+1)   # assigning indices to the dataframe\n",
    "    print(df)                     # printing dataframe\n",
    "    \n",
    "# url of the webpage\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "scraper5(url)            # calling scraper5 function and passing url of the webpage in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5ad57",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "    \n",
    "    You have to scrape 4 attributes of each sneaker:\n",
    "        1. Brand\n",
    "        2. Product Description\n",
    "        3. Price\n",
    "        \n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbde704",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')     # creating instance of webdriver\n",
    "def scraper6(a): \n",
    "    driver.get(a)                               # loading the website\n",
    "    time.sleep(5)                               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding close button of pop-up window on webpage using its xpath and clicking on it\n",
    "    close_popup=driver.find_element_by_xpath('/html/body/div[2]/div/div/button').click()\n",
    "    \n",
    "    # finding search field on webpage using xpath and sending desired input to the field    \n",
    "    search_field_items=driver.find_element_by_class_name('_3704LK').send_keys('sneakers')\n",
    "    \n",
    "    # finding search button/option on webpage using its xpath and clicking on it\n",
    "    search_option=driver.find_element_by_class_name('L0Z3Pu').click()\n",
    "    time.sleep(5)                               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Empty lists to store desired information                \n",
    "    brand_names= []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "    discount=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage        \n",
    "    count = 0\n",
    "    while (count < 100):\n",
    "        brand_tags = driver.find_elements_by_xpath('//div[@class=\"_13oc-S\"]//child::div[@class=\"_2WkVRV\"]')\n",
    "        desc_tags = driver.find_elements_by_xpath('//div[@class=\"_13oc-S\"]//child::div[@class=\"_2WkVRV\"]//following-sibling::a[1]')\n",
    "        price_tags = driver.find_elements_by_xpath('//div[@class=\"_25b18c\"]//child::div[@class=\"_30jeq3\"]')\n",
    "        discount_tags=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]/span')\n",
    "        # Appending fetched data to the respective empty lists created above                \n",
    "        for i in range(len(brand_tags)):\n",
    "            if count >=100:\n",
    "                break\n",
    "            else:\n",
    "                brand_names.append(brand_tags[i].text)\n",
    "                descriptions.append(desc_tags[i].text)\n",
    "                prices.append(price_tags[i].text)\n",
    "                discount.append(discount_tags[i].text)\n",
    "            count += 1\n",
    "            \n",
    "        # finding next button to go the next page using xpath and clicking on it\n",
    "        next_button=(driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')[-1]).click()\n",
    "        time.sleep(5)                           # Assigning time of 5 secs to load the page properly\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    product_details={'Brand Of Product':brand_names,'Description Of Product':descriptions,'Discounted Price':prices,'Discount Offered':discount}\n",
    "    df=pd.DataFrame(product_details)         # creating dataframe\n",
    "    df.index=range(1,len(df)+1)              # assigning indices to the dataframe\n",
    "    print(df)                                # printing dataframe\n",
    "    \n",
    "    \n",
    "url='https://www.flipkart.com/'              # url of the website\n",
    "scraper6(url)                                # calling scraper6 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a96407",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "        \n",
    "1.Set Price filter to “Rs. 7149 to Rs. 14099 ”, Color filter to “Black”, as shown in the below image.\n",
    "  And then scrape First 100 shoes data you get.\n",
    "  The data should include “Brand” of the shoes,Short Shoe description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and \n",
    "      there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')     # creating instance of the webdriver\n",
    "def scraper7(beta):\n",
    "    driver.get(beta)                            # loading website\n",
    "    time.sleep(5)                               # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Empty lists to store desired information                    \n",
    "    brands=[]\n",
    "    shoe_description=[]\n",
    "    prices=[]\n",
    "    \n",
    "    # finding color filter using xpath\n",
    "    color_filter=driver.find_element_by_xpath('//div[@class=\"vertical-filters-filters\"][contains(.,\"Color\")]/ul/li[contains(.,\"Black\")]//div')\n",
    "    color_filter.click()      # clicking on color filter\n",
    "    time.sleep(5)             # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding price filter and clicking on it\n",
    "    price_filter=driver.find_element_by_xpath('(//input[@class=\"price-input\"]//following-sibling::div)[2]').click()\n",
    "    time.sleep(5)                                # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage        \n",
    "    count=0\n",
    "    while count<100:\n",
    "        \n",
    "        brand_tags=driver.find_elements_by_xpath('//div[@class=\"product-productMetaInfo\"]//child::h3')\n",
    "        desc_tags=driver.find_elements_by_xpath('//h4[@class=\"product-product\"]')\n",
    "        price_tags=driver.find_elements_by_xpath('//div[starts-with(@class,\"product-price\")]/span[1]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above                 \n",
    "        for i in range(len(brand_tags)):\n",
    "            if count>=100:\n",
    "                break\n",
    "            else:\n",
    "                brands.append(brand_tags[i].text.strip())\n",
    "                shoe_description.append(desc_tags[i].text.strip())\n",
    "                prices.append(price_tags[i].text.split('Rs.')[1].strip())\n",
    "            count+=1\n",
    "            \n",
    "        # finding next button to go the next page using xpath and clicking on it\n",
    "        next_button=driver.find_element_by_xpath('//li[@class=\"pagination-next\"]').click()\n",
    "        time.sleep(5)                        # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    product_details={'Brand Name':brands,'Description Of Shoe':shoe_description,'Price Of Shoe':prices}\n",
    "    df=pd.DataFrame(product_details)      # creating dataframe\n",
    "    df.index=range(1,len(df)+1)           # assigning indices to the dataframe\n",
    "    print(df)                             # printing dataframe\n",
    "    \n",
    "url='https://www.myntra.com/shoes'        # url of th website\n",
    "scraper7(url)                             # calling scraper7 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c15bf",
   "metadata": {},
   "source": [
    "Q8. Go to webpage https://www.amazon.in/\n",
    "\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "        1. Title\n",
    "        2. Ratings\n",
    "        3. Price\n",
    "As shown in the below image as the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')           # creating instance of webdriver\n",
    "def scraper8(delta):\n",
    "    driver.get(delta)                                 # loading website\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    driver.maximize_window()                          # To maximize the browser window\n",
    "    \n",
    "    # finding search field on webpage and sending desired input to it\n",
    "    search_field=driver.find_element_by_xpath('//div[@class=\"nav-search-field \"]/input').send_keys('Laptop')\n",
    "    \n",
    "    # finding search button on webpage using its xpath and clicking on it\n",
    "    search_button=driver.find_element_by_xpath('//input[@id=\"nav-search-submit-button\"]').click()\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding desired cpu filter on webpage using its xpath and clicking on it\n",
    "    cpu_filter1=driver.find_element_by_xpath('//*[text()=\"Intel Core i7\"]//preceding-sibling::div//i').click()\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "      \n",
    "    # Empty lists to store desired information                    \n",
    "    titles=[]\n",
    "    ratings=[]\n",
    "    prices=[]\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage            \n",
    "    count=0\n",
    "    while(count < 10):\n",
    "        title_tags=driver.find_elements_by_xpath('//div[@class=\"sg-col-inner\"]//h2//span')\n",
    "        ratings_tags=driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]//child::i/span[@class=\"a-icon-alt\"]')\n",
    "        price_tags=driver.find_elements_by_xpath('//span[@class=\"a-price\"]//parent::span[@class=\"a-price-whole\"]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above                    \n",
    "        for i in range(len(title_tags)):\n",
    "            if (count>=10):\n",
    "                break\n",
    "            else:\n",
    "                titles.append(title_tags[i].text)\n",
    "                prices.append(price_tags[i].text)\n",
    "                ratings.append(ratings_tags[i].text)\n",
    "            count+=1\n",
    "                \n",
    "    # creating dictionary to store all the scraped data from webpage\n",
    "    info_dict={'Product_Title':titles,'Price Of Product':prices,'Product Ratings':ratings}\n",
    "    df=pd.DataFrame(info_dict)           # creating dataframe\n",
    "    df.index=range(1,len(df)+1)          # assigning indices to the dataframe\n",
    "    print(df)                            # printing dataframe\n",
    "        \n",
    "url='https://www.amazon.in/'             # url of the website\n",
    "scraper8(url)                            # calling scraper8 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c8a97",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location.\n",
    "    You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "    \n",
    "    This task will be done in following steps:\n",
    "    \n",
    "        1. First get the webpage https://www.ambitionbox.com/\n",
    "        2. Click on the Job option as shown in the image\n",
    "        3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills”,\n",
    "           Enter “Data Scientist” and click on search button.\n",
    "        4. You will reach to the following web page click on location and in place of “Search location”,\n",
    "           Enter “Noida” and select location “Noida”.\n",
    "        5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "        6. Finally create a dataframe of the scraped data.\n",
    "    \n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')              # creating instance of webdriver\n",
    "def scraper9(theta):\n",
    "    driver.get(theta)                                    # loading website\n",
    "    time.sleep(5)                                        # Assigning time of 5 secs to load the page properly\n",
    "    driver.maximize_window()                             # To maximize the browser window    \n",
    "    \n",
    "    # Empty lists to store desired information                    \n",
    "    company_names=[]\n",
    "    days_ago_list=[]\n",
    "    ratings_list=[]   \n",
    "    \n",
    "    # finding jobs option on webpage using its xpath and clicking on it\n",
    "    job_option=driver.find_element_by_xpath('//a[@class=\"link jobs\"]').click()\n",
    "    time.sleep(5)                         # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding search field on webpage using its xpath\n",
    "    search_field=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input')\n",
    "    search_field.send_keys('Data Scientist')      # passing desired input into the field to be searched\n",
    "    \n",
    "    # finding search button on webpage using its xpath and clicking on it\n",
    "    search_button=driver.find_element_by_xpath('//button[@class=\"ab_btn search-btn round\"]').click()\n",
    "    time.sleep(5)                        # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding location dropdown on webpage using its xpath and clicking on it\n",
    "    location_dropdown=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i').click()\n",
    "    \n",
    "    # finding search location field in the dropdown on the webpage and sending desired input to it\n",
    "    search_field_location=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input').send_keys('Noida')\n",
    "    time.sleep(5)                         # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding desired suggestion using its xpath and clicking on it\n",
    "    select_location=driver.find_element_by_id('location_Noida').click()\n",
    "    time.sleep(5)                         # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage        \n",
    "    count=0\n",
    "    while count<10:        \n",
    "        company_tags=driver.find_elements_by_xpath('//div[@class=\"info\"]//div[@class=\"company-info\"]/p')\n",
    "        days_ago_tags=driver.find_elements_by_xpath('//div[@class=\"other-info\"]//child::span[1]')\n",
    "        ratings_tags=driver.find_elements_by_xpath('//span[@class=\"body-small\"]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above                    \n",
    "        for i in range(len(company_tags)):\n",
    "            if count>=10:\n",
    "                break\n",
    "            else:\n",
    "                company_names.append(company_tags[i].text.strip())\n",
    "                days_ago_list.append(days_ago_tags[i].text.strip())\n",
    "                ratings_list.append(ratings_tags[i].text.strip())\n",
    "            count+=1\n",
    "                \n",
    "        # finding load more button on webpage using its xpath\n",
    "        load_more_button=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[2]/div[2]/div[2]/div/div[2]/button')\n",
    "        load_more_button.click()     # clicking on load more button\n",
    "        time.sleep(5)                # Assigning time of 5 secs to load the page properly\n",
    "        \n",
    "    # creating dictionary to store all the scraped data from website\n",
    "    Job_info_dict={'Name Of Company':company_names,'Job Posted On(time ago)':days_ago_list,\"Company's Ratings\":ratings_list}\n",
    "    df=pd.DataFrame(Job_info_dict)               # creating dataframe\n",
    "    df.index=range(1,len(df)+1)                  # Assigning indices to the dataframe\n",
    "    print(df)                                    # printing dataframe\n",
    "    \n",
    "\n",
    "url='https://www.ambitionbox.com/'               # url of the webpage\n",
    "scraper9(url)                                    # calling scraper9 function and passing url in it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c02f79",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "\n",
    "The above task will be, done as shown in the below steps:\n",
    "\n",
    "    1. First get the webpage https://www.ambitionbox.com/\n",
    "    2. Click on the salaries option as shown in the image.\n",
    "    3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "       then click on “Data Scientist”.\n",
    "       You have to scrape the data ticked in the above image.\n",
    "    4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average salary, \n",
    "       minimum  salary, maximum salary, experience required.\n",
    "    5. Store the data in a dataframe.\n",
    "    \n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')           # creating instance of webdriver\n",
    "def scraper10(gamma):\n",
    "    driver.get(gamma)                                 # loading website\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    driver.maximize_window()                          # To maximize the browser window\n",
    "\n",
    "    # Empty lists to store desired information                    \n",
    "    company_names=[]\n",
    "    experience_list=[]\n",
    "    num_of_sals=[] \n",
    "    avg_salary=[]\n",
    "    max_sal=[]\n",
    "    min_sal=[]\n",
    "    \n",
    "    # finding salaries option on the webpage using its xpath and clicking on it\n",
    "    salaries_option=driver.find_element_by_xpath('//a[@title=\"Company Salaries\"]').click()\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding search field on the webpage using its xpath and passing input to it\n",
    "    search_field=driver.find_element_by_xpath('//input[@id=\"jobProfileSearchbox\"]').send_keys('Data Scientist')\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # finding desired suggestion using its xpath and clicking on it\n",
    "    suggested_option=driver.find_element_by_xpath('//div[@class=\"top-row\"]//child::p[text()=\"Data Scientist\"]').click()\n",
    "    time.sleep(5)                                     # Assigning time of 5 secs to load the page properly\n",
    "    \n",
    "    # Logic to fetch desired data from the webpage            \n",
    "    count=0\n",
    "    while(count<10):\n",
    "        company_tags=driver.find_elements_by_xpath('//div[@class=\"result-row\"]//a')\n",
    "        experience_tags=driver.find_elements_by_xpath('//div[@class=\"company-info\"]/div[2]')\n",
    "        num_of_sal_tags=driver.find_elements_by_xpath('//div[@class=\"name\"]/span')\n",
    "        max_sal_tags=driver.find_elements_by_xpath('//div[@class=\"salary-values\"]/div[2]')\n",
    "        min_sal_tags=driver.find_elements_by_xpath('//div[@class=\"salary-values\"]/div[1]')\n",
    "        avg_sal_tags=driver.find_elements_by_xpath('//p[@class=\"averageCtc\"]')\n",
    "        \n",
    "        # Appending fetched data to the respective empty lists created above                    \n",
    "        for i in range(len(company_tags)):\n",
    "            if count>=10:\n",
    "                break\n",
    "            else:                       \n",
    "                company_names.append(company_tags[i].text)\n",
    "                experience_list.append(experience_tags[i].text.split(\".\")[1].strip())\n",
    "                num_of_sals.append(num_of_sal_tags[i].text)\n",
    "                max_sal.append(max_sal_tags[i].text)\n",
    "                min_sal.append(min_sal_tags[i].text)\n",
    "                avg_salary.append(avg_sal_tags[i].text)\n",
    "            count+=1\n",
    "\n",
    "    # creating dictionary to store all the scraped data from webpage                \n",
    "    Job_info_dict={'Name Of Company':company_names,'Experience':experience_list,'Number Of Salaries':num_of_sals,'Minimum Salary':min_sal,'Average Salary':avg_salary,'Maximum Salary':max_sal}\n",
    "    df=pd.DataFrame(Job_info_dict)             # creating dataframe\n",
    "    df.index=range(1,len(df)+1)                # assigning indices to the dataframe\n",
    "    print(df)                                  # printing the dataframe\n",
    "    \n",
    "url='https://www.ambitionbox.com/'             # url of the website\n",
    "scraper10(url)                                 # calling scraper10 function and passing url in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To close the browser window \n",
    "# driver.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
