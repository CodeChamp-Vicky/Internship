{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1819b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries and Modules\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1774514",
   "metadata": {},
   "source": [
    "## 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "\n",
    "### Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "\n",
    "You need to find following details:\n",
    "    A) Rank\n",
    "    B) Name\n",
    "    C) Artist\n",
    "    D) Upload date\n",
    "    E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132afa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')             # creating the instance of webdriver\n",
    "def scraper1(a):                                        # defining a function with 'a' as a parameter\n",
    "    driver.get(a)                                       # loading the webpage\n",
    "    driver.maximize_window()                            # maximizing the browser window\n",
    "    time.sleep(10)                                      # making the driver wait for 10 seconds for loading the website properly\n",
    "    \n",
    "    # Creating a Dataframe to store the Scraped information\n",
    "    top_videos_df=pd.DataFrame([], columns=['Rank','Title','Artist','Total Views(Billions)','Upload Date'])\n",
    "    \n",
    "    # Logic to scrape All the required information from the website\n",
    "    rows=driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr')\n",
    "    for i in range(1,len(rows)+1):\n",
    "        \n",
    "        # scraping rank of the youtube video\n",
    "        rank=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[1]'.format(tr=str(i))).text.replace('.','')   \n",
    "        \n",
    "        # scraping title of the youtube video\n",
    "        try:\n",
    "            title=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[2]/a'.format(tr=str(i))).get_attribute('title')\n",
    "        except Exception as e:\n",
    "            title=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[2]'.format(tr=str(i))).text.split('[')[0].replace('\"','')\n",
    "        \n",
    "        # scraping name of the artist of youtube video\n",
    "        try:\n",
    "            artist=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[3]/a'.format(tr=str(i))).get_attribute('title').split('(')[0].strip()\n",
    "        except Exception as e:\n",
    "            artist=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[3]'.format(tr=str(i))).text\n",
    "        \n",
    "        # scraping total number of views on the youtube video\n",
    "        views=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[4]'.format(tr=str(i))).text\n",
    "        \n",
    "        # scraping the date on which video was uploaded \n",
    "        upload_date=driver.find_element_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr[{tr}]/td[5]'.format(tr=str(i))).text.replace(',','')\n",
    "        \n",
    "        Values=[rank,title,artist,views,upload_date]        # Adding all the scraped information of the youtube video to a list\n",
    "        top_videos_df.loc[len(top_videos_df)]=Values        # Passing All the content of list to the Dataframe\n",
    "    \n",
    "    print(top_videos_df.to_string(index=False))             # Printing Dataframe\n",
    "    \n",
    "url=\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"  # URL of the webpage\n",
    "scraper1(url)                                                           # calling function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb5b73",
   "metadata": {},
   "source": [
    "## 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "## Url = https://www.bcci.tv/.\n",
    "\n",
    "You need to find following details:\n",
    "    A) Match title (I.e. 1st ODI)\n",
    "    B) Series\n",
    "    C) Place\n",
    "    D) Date\n",
    "    E) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')      # creating instance of the webdriver\n",
    "def scraper2(b):                                 # defining function with 'b' as a parameter\n",
    "    driver.get(b)                                # loading the website\n",
    "    driver.maximize_window()                     # maximizing browser window\n",
    "    time.sleep(5)                                # making the driver to wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    # clicking on \"INTERNATIONAL\" link on the webpage\n",
    "    international=driver.find_element_by_xpath('//a[contains(text(),\"INTERNATIONAL\")]').click()\n",
    "    time.sleep(5)              # making the driver to wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    # creating Dataframe to store all the scraped information\n",
    "    Fixture_df=pd.DataFrame([],columns=['Title Of The Match','Scheduled Place','Name Of Series',\n",
    "                                        'Scheduled Date','Scheduled Time'])\n",
    "    \n",
    "    # logic to scrape all the required information from the website\n",
    "    fixture_boxes=driver.find_elements_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div')\n",
    "    for box in range(1,len(fixture_boxes)+1):\n",
    "        \n",
    "        # scraping the title of the match\n",
    "        match_title=driver.find_element_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div[{div}]//child::div[@class=\"fixture-card-bottom\"]//span[1]'.format(div=str(box))).text.split('-')[0]\n",
    "        \n",
    "        # scraping the venue of the match\n",
    "        place=driver.find_element_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div[{div}]//child::div[@class=\"fixture-card-bottom\"]//span[2]'.format(div=str(box))).text.split(',')[0]\n",
    "        \n",
    "        # scraping the name of the series\n",
    "        series=driver.find_element_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div[{div}]//child::h5[2]'.format(div=str(box))).text\n",
    "        \n",
    "        # scraping the date of the match\n",
    "        date=driver.find_element_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div[{div}]//child::h5[2]//following-sibling::div/div[1]/h5'.format(div=str(box))).text\n",
    "        \n",
    "        # scraping the time of the match\n",
    "        match_time=driver.find_element_by_xpath('//div[@class=\"fixture-tab-inner row\"]/div[{div}]//child::h5[2]//following-sibling::div/div[2]/h5'.format(div=str(box))).text\n",
    "        \n",
    "        # Adding all the scraped information to a list\n",
    "        Values=[match_title,place,series,date,match_time]\n",
    "        \n",
    "        # Passing All the content of the list to the Dataframe\n",
    "        Fixture_df.loc[len(Fixture_df)]=Values\n",
    "    \n",
    "    print(Fixture_df)                # printing Dataframe\n",
    "    \n",
    "url=\"https://www.bcci.tv/\"           # URL of the website\n",
    "scraper2(url)                        # calling the function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165d8f8",
   "metadata": {},
   "source": [
    "## 3. Scrape the details of selenium exception from guru99.com.\n",
    "### Url = https://www.guru99.com/\n",
    "\n",
    "You need to find following details:\n",
    "    A) Name\n",
    "    B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa986be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')           # creating instance of webdriver\n",
    "def scraper3(c):                                      # defining a function with 'c' as a parameter to it\n",
    "    driver.get(c)                                     # loading the website\n",
    "    driver.maximize_window()                          # maximizing browser window\n",
    "    time.sleep(10)                                    # making the driver wait to for 10 seconds to get the website loaded properly\n",
    "    \n",
    "    # Finding and clicking on \"Selenium\" link\n",
    "    try:\n",
    "        selenium_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//li/a[@title=\"Selenium\"]'))).click()\n",
    "        time.sleep(10)           # making the driver wait for 10 seconds to get the page loaded properly\n",
    "    except Exception:\n",
    "        selenium_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a'))).click()\n",
    "        time.sleep(10)           # making the driver wait for 10 seconds to get the page loaded properly\n",
    "        \n",
    "    # Finding and clicking on \"Selenium Exception Handling (Common Exceptions List)\" link\n",
    "    try:\n",
    "        exception_handling_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//a[@title=\"Selenium Exception Handling (Common Exceptions List)\"]'))).click()\n",
    "        time.sleep(10)           # making the driver to wait for 10 seconds to get the page loaded properly\n",
    "    except Exception:\n",
    "        exception_handling_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]/a'))).click()\n",
    "        time.sleep(10)           # making the driver to wait for 10 seconds to get the page loaded properly\n",
    "    \n",
    "    # creating a Dataframe to store all the sraped information\n",
    "    Exception_df=pd.DataFrame([],columns=['Name Of Exception','Description Of Exception'])\n",
    "    \n",
    "    # Logic to scrape all the required information\n",
    "    exceptions_table=driver.find_elements(By.XPATH,'//table[@class=\"table table-striped\"]/tbody/tr')\n",
    "    for et in range(2,len(exceptions_table)+1):\n",
    "        \n",
    "        # Scraping Name of exception\n",
    "        exception_name=driver.find_element(By.XPATH,'//table[@class=\"table table-striped\"]/tbody/tr[{tr}]/td[1]'.format(tr=str(et))).text\n",
    "        \n",
    "        # Scraping Description of the exception\n",
    "        exception_desc=driver.find_element(By.XPATH,'//table[@class=\"table table-striped\"]/tbody/tr[{tr}]/td[2]'.format(tr=str(et))).text\n",
    "        \n",
    "        # Storing all the scraped information in a list\n",
    "        Values=[exception_name,exception_desc]\n",
    "        \n",
    "        # Passing all the content of the list to the Dataframe\n",
    "        Exception_df.loc[len(Exception_df)]=Values\n",
    "    \n",
    "    print(Exception_df)                             # Printing the Dataframe\n",
    "\n",
    "url='https://www.guru99.com/'                       # URL of the website\n",
    "scraper3(url)                                       # calling the function and passing the URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9e741",
   "metadata": {},
   "source": [
    "## 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "### Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)\n",
    "    D) GSDP(17-18)\n",
    "    E) Share(2017)\n",
    "    F) GDP($ billion)\n",
    "\n",
    "Note:- From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # creating instance of the webdriver\n",
    "def scraper4(d):                                  # defining a function with 'd' as a parameter to it\n",
    "    driver.get(d)                                 # loading the website\n",
    "    driver.maximize_window()                      # maximizing the browser window\n",
    "    time.sleep(10)                                # making the driver wait for 10 seconds to get the website loaded properly\n",
    "    \n",
    "    # Hover over \"Economy\" Dropdown and clicking on \"India\" in the dropdown list\n",
    "    economy_dropdown=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/button')\n",
    "    india_option=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/div/a[contains(text(),\"India\")]')\n",
    "    ActionChains(driver).move_to_element(economy_dropdown).move_to_element(india_option).click().perform()\n",
    "    \n",
    "    # Asking to close the ad which shows up when the \"India\" option is clicked\n",
    "    print(\"!!!!!! Please! Close The Ad, Within 10 seconds !!!!!!\")\n",
    "    time.sleep(10)         # Making driver wait for 5 seconds to give time for the person to close the ad \n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[0])    # switching to current browser tab after closing the Ad \n",
    "    \n",
    "    # Getting the URL of \"GDP Of Indian States\" link on the page and loading it in new tab\n",
    "    try:\n",
    "        gdp_of_indian_states_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[starts-with(text(),\"Indian states\")]/following-sibling::ul/li[1]/a')))\n",
    "        driver.execute_script('window.open(\" %s \", \"_blank\")' % gdp_of_indian_states_link.get_attribute('href'))\n",
    "        time.sleep(10)            # Making the driver wait for 10 seconds for page to be loaded properly\n",
    "    except Exception:\n",
    "        gdp_of_indian_states_link=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[starts-with(text(),\"Indian states\")]/following-sibling::ul/li[1]/a')))\n",
    "        driver.execute_script('window.open(\" %s \", \"_blank\")' % gdp_of_indian_states_link.get_attribute('href'))\n",
    "        time.sleep(10)            # Making the driver wait for 10 seconds for page to be loaded properly\n",
    "    try:\n",
    "        WebDriverWait(driver,20).until(EC.number_of_windows_to_be(2))\n",
    "    except Exception:\n",
    "        time.sleep(10)\n",
    "        pass\n",
    "    driver.switch_to.window(driver.window_handles[1])     # switching the driver to newly opened browser tab \n",
    "    time.sleep(5)\n",
    "        \n",
    "    # Creating a Dataframe to store all the scraped information\n",
    "    GSDP_Df = pd.DataFrame([], columns=['Rank Of State', 'Name Of State', 'GSDP (19-20)', 'GSDP (18-19)', 'Share (18-19)','GDP (in $BILLION)'])\n",
    "    \n",
    "    # Logic to Scrape all the required information\n",
    "    gsdp_table_rows=driver.find_elements(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr')\n",
    "    for i in range(1,len(gsdp_table_rows)+1):\n",
    "        \n",
    "        # Scraping \"Rank\" of the State\n",
    "        rank=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[1]'.format(tr=i)).text\n",
    "        \n",
    "        # Scraping \"Name\" of the State\n",
    "        state_name=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[2]'.format(tr=i)).text\n",
    "                \n",
    "        # Scraping GSDP IN 2019-2020 of the State\n",
    "        gsdp19_20=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[3]'.format(tr=i)).text\n",
    "                \n",
    "        # Scraping GSDP in 2018-2019 of the State\n",
    "        gsdp18_19=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[4]'.format(tr=i)).text\n",
    "                \n",
    "        # Scraping \"Share\" of the State\n",
    "        share18_19=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[5]'.format(tr=i)).text\n",
    "                \n",
    "        # Scraping GDP of the State\n",
    "        gdp_in_billiond=driver.find_element(By.XPATH,'//div[@id=\"table_id_wrapper\"]/table/tbody/tr[{tr}]/td[6]'.format(tr=i)).text\n",
    "        \n",
    "        # Storing all the Scraped information in a list\n",
    "        Values=[rank,state_name,gsdp19_20,gsdp18_19,share18_19,gdp_in_billiond]\n",
    "        \n",
    "        GSDP_Df.loc[len(GSDP_Df)]=Values                      # Passing all the content of the list to the Dataframe\n",
    "    \n",
    "    driver.close()                                        # closing the current tab\n",
    "    driver.switch_to.window(driver.window_handles[0])     # switching to previous tab\n",
    "\n",
    "    print(GSDP_Df.to_string(index=False))         # Printing Dataframe\n",
    "url='http://statisticstimes.com/'                 # URL of the website\n",
    "scraper4(url)                                     # calling function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a42568",
   "metadata": {},
   "source": [
    "## 5. Scrape the details of trending repositories on Github.com.\n",
    "### Url = https://github.com/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Repository title\n",
    "    B) Repository description\n",
    "    C) Contributors count\n",
    "    D) Language used\n",
    "    \n",
    "Note:- From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549bac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')      # creating instance of the webdriver\n",
    "def scraper5(e):                                 # Defining a function with 'e' as a parameter\n",
    "    driver.get(e)                                # Loading the website\n",
    "    driver.maximize_window()                     # Maximizing browser window\n",
    "    time.sleep(10)                               # Making driver wait for 10 seconds to get the page loaded properly\n",
    "    \n",
    "    # Moving to \"Explore\" option and clicking on it, then moving to \"Trending\" option in the dropdown list and clicking on it \n",
    "    try:\n",
    "        explore_option=WebDriverWait(driver,20).until(EC.element_to_be_clickable((By.XPATH,'//summary[contains(text(),\"Explore\")]')))\n",
    "    except Exception:\n",
    "        driver.refresh()\n",
    "        explore_option=WebDriverWait(driver,20).until(EC.element_to_be_clickable((By.XPATH,'//nav[@class=\"mt-0 px-3 px-lg-0 mb-5 mb-lg-0\"]/ul/li[4]/details/summary')))\n",
    "    try:\n",
    "        trending_option=driver.find_element(By.XPATH,'//a[contains(text(),\"Trending\")]')\n",
    "    except Exception:\n",
    "        trending_option=driver.find_element(By.XPATH,'//nav[@class=\"mt-0 px-3 px-lg-0 mb-5 mb-lg-0\"]/ul/li[4]/details/div//li[5]/a')\n",
    "    ActionChains(driver).move_to_element(explore_option).click().move_to_element(trending_option).click().perform()\n",
    "    time.sleep(5)        # Making the driver wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[0]) # Switching to the Loaded Page\n",
    "    \n",
    "    # Creating the Dataframe to store All The Scraped information \n",
    "    GitHub_df=pd.DataFrame([],columns=['Title Of Repository','Description Of Repository','No. Of Contributors','Languages Used'])\n",
    "    \n",
    "    # Logic to Scrape All the required information\n",
    "    repo_boxes=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h1/a')\n",
    "    for repo_box in repo_boxes:\n",
    "        \n",
    "        # Getting the URL of the Repository and loading it in a new tab\n",
    "        repo_link=driver.execute_script(\"window.open('%s', '_blank')\" % repo_box.get_attribute(('href')))\n",
    "        try:\n",
    "            WebDriverWait(driver,10).until(EC.number_of_windows_to_be(2))\n",
    "        except Exception:\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        \n",
    "        # Switching the driver to the new Tab where the repository is loaded\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        \n",
    "        # Scraping Title of the repository if Available\n",
    "        try:\n",
    "            repo_title=driver.find_element(By.XPATH,'//strong[@class=\"mr-2 flex-self-stretch\"]/a').text\n",
    "        except Exception:\n",
    "            repo_title='Information Not Available'\n",
    "        \n",
    "        # Scraping the Description of Repository if Available\n",
    "        try:\n",
    "            repo_desc=driver.find_element(By.XPATH,'//p[@class=\"f4 my-3\"]').text\n",
    "        except Exception:\n",
    "            repo_desc='Information Not Available'\n",
    "        \n",
    "        # Scraping Number Of Contributors in the Repository if Available\n",
    "        try:\n",
    "            no_of_contributors=driver.find_element(By.XPATH,'//ul[contains(@class,\"flex-wrap mb-n2\")]/preceding-sibling::h2/a').text.replace(' ','-')\n",
    "        except Exception:\n",
    "            try:\n",
    "                no_of_contributors=driver.find_element(By.XPATH,'//ul[contains(@class,\"list-style-none \")]/preceding-sibling::h2/a').text\n",
    "            except Exception:\n",
    "                no_of_contributors=\"Information Not Available\"\n",
    "        \n",
    "        # Scraping Programming Languages used in the Repository if Available\n",
    "        try:\n",
    "            lang_used= [lang.text for lang in driver.find_elements(By.XPATH,'//span[@class=\"color-fg-default text-bold mr-1\"]')]\n",
    "            if lang_used==[]:\n",
    "                raise Exception\n",
    "        except Exception:\n",
    "            lang_used.append('Information Not Available')\n",
    "        \n",
    "        # Storing All the Scraped Information in a list\n",
    "        Values=[repo_title,repo_desc,no_of_contributors,lang_used]\n",
    "        GitHub_df.loc[len(GitHub_df)]=Values         # Passing the content of the list to the Dataframe\n",
    "        driver.close()                               # Closing the Current Tab\n",
    "        driver.switch_to.window(driver.window_handles[0])  # Switching to the Previous Tab\n",
    "    \n",
    "    print(GitHub_df)                                       # Printing the Dataframe\n",
    "url='https://github.com/'                                  # URL of the Website\n",
    "scraper5(url)                                              # Calling the function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dd91a",
   "metadata": {},
   "source": [
    "## 6. Scrape the details of top 100 songs on billiboard.com.\n",
    "### Url = https://www.billboard.com/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Song name\n",
    "    B) Artist name\n",
    "    C) Last week rank\n",
    "    D) Peak rank\n",
    "    E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')          # Creating an instance of webdriver\n",
    "def scraper6(f):                                     # Defining a function with 'f' as a parameter of it\n",
    "    driver.get(f)                                    # Loading the Website\n",
    "    driver.maximize_window()                         # Maximizing browser window \n",
    "    time.sleep(5)                                    # Making the driver wait for 5 seconds to get the website loaded properly\n",
    "    \n",
    "    # Waiting for the \"Charts\" button to be prsent on the page and Clicking on it\n",
    "    try:\n",
    "        charts_button=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"main-menu // u-display-none@desktop-max lrv-u-flex-grow-1 u-width-100p@desktop-xl-max lrv-u-padding-t-025\"]/nav//li[1]/a')))\n",
    "        charts_button.click()\n",
    "        time.sleep(10)               # Making the driver wait for 5 seconds to get the page loaded properly\n",
    "    except Exception:\n",
    "        charts_button=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"main-menu // u-display-none@desktop-max lrv-u-flex-grow-1 u-width-100p@desktop-xl-max lrv-u-padding-t-025\"]/nav//li[1]/a')))\n",
    "        charts_button.click()\n",
    "        time.sleep(10)               # Making the driver wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    # Waiting for the \"Hot 100\" button to be present on the page and clicking on it\n",
    "    try:\n",
    "        hot_100_button=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//li[@class=\"o-nav__list-item \"][1]/a'))).click()\n",
    "        time.sleep(10)               # Making the driver wait for 5 seconds to get the page loaded properly\n",
    "    except Exception:\n",
    "        hot_100_button=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//li[@class=\"o-nav__list-item \"][1]/a'))).click()\n",
    "        time.sleep(10)               # Making the driver wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    # Creating a Dataframe to Store all the scraped information        \n",
    "    Info_df=pd.DataFrame([],columns=['Name Of Song','Name Of Artist',\"Last Week's Rank\",'Peak Rank','Weeks On Board'])\n",
    "    \n",
    "    # Logic to Scrape all the required information\n",
    "    try:\n",
    "        info_boxes=WebDriverWait(driver,20).until(EC.presence_of_all_elements_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]//child::li[@class=\"lrv-u-width-100p\"]//h3[@id=\"title-of-a-story\"]')))\n",
    "    except Exception as e:\n",
    "        time.sleep(2)\n",
    "        info_boxes=WebDriverWait(driver,20).until(EC.presence_of_all_elements_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]//child::li[@class=\"lrv-u-width-100p\"]//h3[@id=\"title-of-a-story\"]')))\n",
    "    for ib in range(1,len(info_boxes)+1):\n",
    "        \n",
    "        # Scraping  Name of the Song\n",
    "        try:\n",
    "            song=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li//child::h3'.format(div=str(ib))))).text\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "            song=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li//child::h3'.format(div=str(ib))))).text\n",
    "        \n",
    "        # Scraping Name of the Artist\n",
    "        try:\n",
    "            artist=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li//child::h3//following-sibling::span'.format(div=str(ib))))).text\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "            artist=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li//child::h3//following-sibling::span'.format(div=str(ib))))).text\n",
    "        \n",
    "        # Scraping Last Week's Rank Of the Song\n",
    "        try:\n",
    "            lw_rank=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span'.format(div=str(ib))))).text\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "            lw_rank=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span'.format(div=str(ib))))).text\n",
    "        \n",
    "        # Scraping Peak Rank Of the Song\n",
    "        try:\n",
    "            peak_rank=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span'.format(div=str(ib))))).text\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "            peak_rank=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span'.format(div=str(ib))))).text\n",
    "        \n",
    "        # Scraping \"Weeks On Board\" Value Of the Song \n",
    "        try:\n",
    "            weeks_on_board=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[6]/span'.format(div=str(ib))))).text\n",
    "        except Exception as e:\n",
    "            time.sleep(2)\n",
    "            weeks_on_board=WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"][{div}]//child::li[@class=\"lrv-u-width-100p\"]/ul/li[6]/span'.format(div=str(ib))))).text\n",
    "        \n",
    "        values = [song,artist,lw_rank,peak_rank,weeks_on_board]      # Storing all the scraped information in a list\n",
    "        Info_df.loc[len(Info_df)]=values                             # Passing all the content of the list to the Dataframe \n",
    "  \n",
    "    print(Info_df)                    # Printing Dataframe\n",
    "url='https://www.billboard.com/'      # URL of the website\n",
    "scraper6(url)                         # Calling Function and passing URL as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887640cc",
   "metadata": {},
   "source": [
    "## 7. Scrape the details of Data science recruiters from naukri.com.\n",
    "### Url = https://www.naukri.com/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Name\n",
    "    B) Designation\n",
    "    C) Company\n",
    "    D) Skills they hire for\n",
    "    E) Location\n",
    "    \n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')      # Creating the instance of the webdriver                    \n",
    "def scraper7(g):                                 # Defining a function with 'g' as a parameter of it\n",
    "    driver.get(g)                                # Loading the Website\n",
    "    driver.maximize_window()                     # Maximizing the Browser Window\n",
    "    time.sleep(3)                                # Making driver wait for 5 seconds to get the page loaded properly\n",
    "    \n",
    "    first_window=driver.window_handles[0]        # Creating a variable to store unique identifier for the first tab\n",
    "    \n",
    "    # Finding \"Jobs\" dropdown then moving to \"Data Science Jobs\" option and clicking on it\n",
    "    jobs_option=driver.find_element_by_xpath('/html/body/div/div[2]/div[1]/div/ul/li[1]')\n",
    "    ds_jobs_option=driver.find_element_by_xpath('//a[@title=\"Data Science jobs\"]')\n",
    "    ActionChains(driver).move_to_element(jobs_option).move_to_element(ds_jobs_option).click().perform()\n",
    "    try:\n",
    "        WebDriverWait(driver,20).until(EC.number_of_windows_to_be(2))\n",
    "    except Exception:\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "    second_window=driver.window_handles[1]         # Creating a variable to store unique identifier for the second tab\n",
    "    driver.switch_to.window(second_window)         # Switching to new window after clicking on \"Data Science Jobs\" Option\n",
    "    time.sleep(5)                                # Making the driver wait for 3 seconds for page to be loaded properly\n",
    "    \n",
    "    # Finding \"Recruiters\" dropdown then finding \"Browse All Recruiters\" options and clicking on it\n",
    "    recruiters_option=driver.find_element_by_xpath('/html/body/div[1]/div[1]/div/ul[1]/li[2]/a')\n",
    "    all_recruiters_option=driver.find_element_by_xpath('/html/body/div[1]/div[1]/div/ul[1]/li[2]/div/ul/li[1]/a')\n",
    "    ActionChains(driver).move_to_element(recruiters_option).move_to_element(all_recruiters_option).click().perform()\n",
    "    try:\n",
    "        WebDriverWait(driver,20).until(EC.number_of_windows_to_be(3))\n",
    "    except Exception:\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "    third_window=driver.window_handles[2]     # Creating a variable to store unique identifier for the third tab\n",
    "    driver.switch_to.window(third_window)     # Switching to new window after clicking on \"All Recruiters\" Option\n",
    "    time.sleep(5)                             # Making the driver to wait for 3 seconds for page to be loaded properly\n",
    "    \n",
    "    # Searching \"Data Science Recruiters\" in the Search field\n",
    "    search_field=driver.find_element_by_xpath('//input[@name=\"qp\"]').send_keys('Data Science Recruiters')\n",
    "    time.sleep(2)                             # Making driver to wait for 3 seconds for page to be loaded properly\n",
    "    \n",
    "    # Finding and clicking on \"Search\" button\n",
    "    search_button=driver.find_element_by_xpath('//button[@class=\"fl qsbSrch blueBtn\"]').click()\n",
    "    time.sleep(5)                             # Making driver to wait for 3 seconds for page to be loaded properly\n",
    "    \n",
    "    # Creating a dataframe to store all the scraped information\n",
    "    Recruiter_df=pd.DataFrame([],columns=[\"Recruiter's Name\",\"Recruiter's Designation\",'Name Of Company'\n",
    "                                          ,'Skills Required','Location Of Company'])\n",
    "    # Logic To Scrape the Required Information\n",
    "    recruiter_info_boxes=driver.find_elements_by_xpath('//div[@class=\"outerRecSec\"]')\n",
    "    for i in range(1,len(recruiter_info_boxes)+1):\n",
    "        \n",
    "        # Scraping Name,Designation,Company, Skills Looking For,Location related to Recruiter in Div[1]\n",
    "        name_f1=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[1]//child::p/a/span'.format(div=str(i))).text\n",
    "        designation_f1=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[1]//child::p/span'.format(div=str(i))).text\n",
    "        company_f1=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[1]//child::a[2]'.format(div=str(i))).text\n",
    "        skills_f1=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[1]//div[@class=\"hireSec highlightable\"]'.format(div=str(i))).text\n",
    "        try:\n",
    "            location_f1=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[1]//child::span[2]/small'.format(div=str(i))).text.strip()\n",
    "        except Exception as e:\n",
    "            location_f1=\"Location Not Available\"\n",
    "        \n",
    "        # Storing Scraped Information related to Div[1] in a list\n",
    "        values_f1=[name_f1,designation_f1,company_f1,skills_f1,location_f1]\n",
    "        \n",
    "        # Passing Content of list to the Dataframe\n",
    "        Recruiter_df.loc[len(Recruiter_df)]=values_f1\n",
    "        \n",
    "        # Scraping Name,Designation,Company, Skills Looking For,Location related to Recruiter in Div[2]\n",
    "        name_fr=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[2]//child::p/a/span'.format(div=str(i))).text\n",
    "        designation_fr=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[2]//child::p/span'.format(div=str(i))).text\n",
    "        company_fr=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[2]//child::a[2]'.format(div=str(i))).text\n",
    "        skills_fr=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[2]//div[@class=\"hireSec highlightable\"]'.format(div=str(i))).text\n",
    "        try:\n",
    "            location_fr=driver.find_element_by_xpath('//div[@class=\"outerRecSec\"]/parent::div/div[{div}]/div[2]//child::span[2]/small'.format(div=str(i))).text.strip()\n",
    "        except Exception as e:\n",
    "            location_fr=\"Location Not Available\"\n",
    "        \n",
    "        # Storing Scraped Information related to Div[1] in a list\n",
    "        values_fr=[name_fr,designation_fr,company_fr,skills_fr,location_fr]\n",
    "        \n",
    "        # Passing Content of list to the Dataframe\n",
    "        Recruiter_df.loc[len(Recruiter_df)]=values_fr\n",
    "    print(Recruiter_df)                     # Printing Dataframe\n",
    "url='https://www.naukri.com/'               # URL of the Website\n",
    "scraper7(url)                               # Calling the function and passing URL of the Website as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cd8c3",
   "metadata": {},
   "source": [
    "## 8. Scrape the details of Highest selling novels.\n",
    "### Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Book name\n",
    "    B) Author name\n",
    "    C) Volumes sold\n",
    "    D) Publisher\n",
    "    E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2779953",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # Creating instance of the webdriver\n",
    "def scraper8(h):                                  # Defining function with h as a parameter\n",
    "    driver.get(h)                                 # Loading website\n",
    "    driver.maximize_window()                      # Maximizing Browser Window\n",
    "    time.sleep(5)                                 # Making driver wait for 5 seconds for page to be loaded properly\n",
    "    \n",
    "    # Creating Dataframe to store Scraped information\n",
    "    Books_df=pd.DataFrame([],columns=['Name Of Book',\"Author's Name\",'Number Of Volumes Sold',\"Publisher's Name\",'Genre Of Book'])\n",
    "    \n",
    "    # Logic for Scraping required information\n",
    "    Info_Table=driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr')\n",
    "    for i in range(1,len(Info_Table)+1):\n",
    "        \n",
    "        # Scraping Name of the Book\n",
    "        book_name=driver.find_element_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr[{tr}]/td[2]'.format(tr=str(i))).text\n",
    "        \n",
    "        # Scraping Author's Name\n",
    "        author_name=driver.find_element_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr[{tr}]/td[3]'.format(tr=str(i))).text\n",
    "        \n",
    "        # Scraping Number of Volumes Sold\n",
    "        volums_sold=driver.find_element_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr[{tr}]/td[4]'.format(tr=str(i))).text.replace(',','')\n",
    "        \n",
    "        # Scraping Publisher's Name\n",
    "        publisher_name=driver.find_element_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr[{tr}]/td[5]'.format(tr=str(i))).text\n",
    "        \n",
    "        # Scraping Genre of the Book\n",
    "        genre=driver.find_element_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr[{tr}]/td[6]'.format(tr=str(i))).text\n",
    "        \n",
    "        Values=[book_name,author_name,volums_sold,publisher_name,genre]  # Storing All the scraped Information in a list\n",
    "        Books_df.loc[len(Books_df)]=Values                        # Passing All the content of the list to the Dataframe\n",
    "    \n",
    "    print(Books_df)                                               # Printing Dataframe\n",
    "# URL of the Webpage\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/'\n",
    "scraper8(url)                  # Calling function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e8ed8",
   "metadata": {},
   "source": [
    "## 9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "### Url = https://www.imdb.com/list/ls095964455/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Name\n",
    "    B) Year span\n",
    "    C) Genre\n",
    "    D) Run time\n",
    "    E) Ratings\n",
    "    F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5779bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # Creating Instance Of the webdriver\n",
    "def scraper9(i):                                  # Defining a function with 'i' as its parameter\n",
    "    driver.get(i)                                 # Loading the Webpage\n",
    "    driver.maximize_window()                      # Maximizing the Browser window\n",
    "    time.sleep(5)                                 # Making the driver wait for 5 seconds for page to be loaded properly\n",
    "    \n",
    "    # Creating a Dataframe to store all the scraped information\n",
    "    Info_df=pd.DataFrame([],columns=['Name Of TV Series','Span Of Time','Genres','Run Time','Ratings','Votes'])\n",
    "    \n",
    "    # Logic to Scrape the Required Information\n",
    "    info_containers=driver.find_elements_by_xpath('//div[@class=\"lister-item-content\"]')\n",
    "    for i in range(1,len(info_containers)+1):\n",
    "        \n",
    "        # Scraping Name Of The Movie\n",
    "        name=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/h3/a'.format(div=str(i))).text\n",
    "        \n",
    "        # Scraping Duration of the Movie\n",
    "        span_duration=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/h3/span[2]'.format(div=str(i))).text.replace(' ','Till Date')\n",
    "        \n",
    "        # Scraping Genre of the Movie\n",
    "        genres=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/p/span[@class=\"genre\"]'.format(div=str(i))).text\n",
    "        \n",
    "        # Scraping Runtime Of the Movie\n",
    "        runtime=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/p/span[@class=\"runtime\"]'.format(div=str(i))).text\n",
    "        \n",
    "        # Scraping Ratings of the Movie\n",
    "        rating=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/div[1]/div[1]/span[2]'.format(div=str(i))).text\n",
    "        \n",
    "        # Scraping Total Votes of the Movie\n",
    "        votes=driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[3]/div[1]/div/div[3]/div[3]/div[{div}]/div[2]/p[4]/span[2]'.format(div=str(i))).text\n",
    "        \n",
    "        Values=[name,span_duration,genres,runtime,rating,votes]      # Storing the Scraped information in a list\n",
    "        Info_df.loc[len(Info_df)]=Values                             # Passing the Content of the list to the Dataframe\n",
    "    \n",
    "    print(Info_df)                                                   # Printing Dataframe\n",
    "url='https://www.imdb.com/list/ls095964455/'                         # URL of the Webpage\n",
    "scraper9(url)                                                        # Calling function and passing URL as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6c989",
   "metadata": {},
   "source": [
    "## 10. Details of Datasets from UCI machine learning repositories.\n",
    "### Url = https://archive.ics.uci.edu/\n",
    "\n",
    "You have to find the following details:\n",
    "    A) Dataset name\n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type\n",
    "    E) No of instances\n",
    "    F) No of attribute\n",
    "    G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')           # creating instance of webdriver\n",
    "def scraper10(j):                                       # Defining a function with 'j' as a paramter \n",
    "    driver.get(j)                                       # Loading the Website\n",
    "    driver.maximize_window()                            # Maximizing Browser Window\n",
    "    time.sleep(5)                                       # Making the driver wait for 5 seconds for page to be loaded properly\n",
    "    \n",
    "    # Finding and clicking on \"view all data sets\" link on the webpage\n",
    "    view_all_data_sets = driver.find_element(By.XPATH,'//td/span[@class=\"whitetext\"][2]').click()\n",
    "    time.sleep(3)                                       # Making the driver to wait for 3 second for page to be loaded properly\n",
    "    \n",
    "    # Creating a Dataframe to store all the Scraped information\n",
    "    Dataset_df = pd.DataFrame([], columns=['Name Of DataSet', 'Type Of DataSet',\n",
    "                                           'Task', 'Type Of Attribute',\n",
    "                                           'No. Of Instances', 'No. Of Attributes', 'Year Of Creation'])\n",
    "    \n",
    "    # Logic to Scrape the required data from the webpage \n",
    "    try:\n",
    "        ds_table = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, '//table[@border=\"1\"]//tbody')))\n",
    "    except  Exception:\n",
    "        time.sleep(5)\n",
    "        ds_table= driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody')\n",
    "    for i in range(2, len(ds_table) + 1):\n",
    "        \n",
    "        # Scraping Name of the dataset if available\n",
    "        ds_name = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[1]//p'.format(tr=str(i))).text.strip()\n",
    "        if ds_name=='':\n",
    "            ds_name='-'\n",
    "        \n",
    "        # Scraping Type of dataset if available\n",
    "        ds_type = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[2]//p'.format(tr=str(i))).text.strip()\n",
    "        if ds_type=='':\n",
    "            ds_type='-'\n",
    "        \n",
    "        # Scraping Task related to dataset if available\n",
    "        ds_task = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[3]//p'.format(tr=str(i))).text.strip()\n",
    "        if ds_task=='':\n",
    "            ds_task='-'\n",
    "        \n",
    "        # Scraping Types of attributes of the dataset if available\n",
    "        attribute_type = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[4]//p'.format(tr=str(i))).text.strip()\n",
    "        if attribute_type=='':\n",
    "            attribute_type='-'\n",
    "        \n",
    "        # Scraping Number of instance of dataset if available\n",
    "        no_of_instances = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[5]//p'.format(tr=str(i))).text.strip()\n",
    "        if no_of_instances=='':\n",
    "            no_of_instances='-'\n",
    "        \n",
    "        # Scraping Number of attributes of dataset if available\n",
    "        no_of_attributes = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[6]//p'.format(tr=str(i))).text.strip()\n",
    "        if no_of_attributes=='':\n",
    "            no_of_attributes='-'\n",
    "        \n",
    "        # Scraping Year of Creation of dataset if available\n",
    "        year = driver.find_element(By.XPATH, '//table[@border=\"1\"]//tbody/tr[{tr}]/td[7]//p'.format(tr=str(i))).text.strip()\n",
    "        if year=='':\n",
    "            year='-'\n",
    "        \n",
    "        # Storing All the scraped information in a list\n",
    "        Values = [ds_name, ds_type, ds_task, attribute_type, no_of_instances, no_of_attributes,year]\n",
    "        Dataset_df.loc[len(Dataset_df)] = Values          # Passing All the content of the list to the dataframe\n",
    "    \n",
    "    print(Dataset_df)                                     # Printing Dataframe\n",
    "url = 'https://archive.ics.uci.edu/'                      # URL of the Website\n",
    "scraper10(url)                                            # Calling the function and passing URL as an argument to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d57c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
