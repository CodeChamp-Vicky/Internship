{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007a1d5",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "     • All questions are compulsory.\n",
    "     • In each of the questions you have to automate the process. You do not have to click on any button, click\n",
    "       any clickable element, enter keywords in search boxes manually. Each process has to be performed via\n",
    "       coding.\n",
    "     • Q1 and Q2 are connected questions i.e. after attempting Q1 proceed to Q2. Do not write whole code\n",
    "       from beginning for Q2.\n",
    "     • You may use any web scraping library and tools.\n",
    "     • The question can be attempted in various ways; the correctness of question depends on the output.\n",
    "     • If you encounter any Null values during scraping, you may replace it by hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a27fe",
   "metadata": {},
   "source": [
    "## Q1.\n",
    "          Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    " \n",
    "          The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’.\n",
    " \n",
    "          Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aec3b",
   "metadata": {},
   "source": [
    "## Q2.\n",
    "          In the above question, now scrape the following details of each product listed in first 3 pages of your\n",
    "          search results and save it in a data frame and csv.\n",
    " \n",
    "          In case if any product has less than 3 pages in search\n",
    "          results then scrape all the products available under that product name.\n",
    " \n",
    "          Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\",   \n",
    "          \"Availability\" and “Product URL”.\n",
    " \n",
    "         In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631893a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')   # Creating Instance Of WebDriver\n",
    "def scraper1_2(u,p):                            # Defining Function With Two Parameters(URL And Product Name)\n",
    "    driver.get(u)                               # Loading Webpage \n",
    "    time.sleep(10)                              # Assigning 10 seconds Of Sleep Time To Get The Page Loaded Properly\n",
    "    driver.maximize_window()                    # Maximizing Browser Window\n",
    "    time.sleep(5)\n",
    "    # Sending Input To The Search Field And Clicking On Search Button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys(p)\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "    time.sleep(10)                              # Assigning 10 seconds Of Sleep Time To Get The Page Loaded Properly\n",
    "    \n",
    "    # Creating A DataFrame With Required Columns To Store Scraped Information\n",
    "    Amazon_Product_Detail = pd.DataFrame([], columns=[\"Name of the Product\", \"Brand Name\",  \"Price\", \"Return/Exchange\",\n",
    "                                                     \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "    # Logic To Fetch All The Required Information From The Website\n",
    "    for page in range(3):\n",
    "        time.sleep(10)\n",
    "        items = driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "        for i in items:\n",
    "            values = []\n",
    "            i.click()\n",
    "            try:\n",
    "                WebDriverWait(driver, 30).until(EC.number_of_windows_to_be(2))\n",
    "            except selenium.common.exceptions.TimeoutException:\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            # Fetching Name Of Product If Available\n",
    "            try:\n",
    "                productName = driver.find_element(By.XPATH, '//div[@id=\"titleSection\"]').text\n",
    "                if productName:\n",
    "                    values.append(productName)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching Name Of Brand If AVailable\n",
    "            try:\n",
    "                brandName = driver.find_element(By.XPATH, '//*[@id=\"productOverview_feature_div\"]/div/table/tbody/tr[1]/td[2]/span').text\n",
    "                if brandName:\n",
    "                    values.append(brandName)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching Price Of Product If Available\n",
    "            try:\n",
    "                price = driver.find_element(By.XPATH, '//*[@id=\"corePrice_feature_div\"]/div/span').text.split('₹')[1].split('\\n')[0]\n",
    "                if price:\n",
    "                    values.append(price)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching If Item is Available For Return Or Exchange\n",
    "            try:\n",
    "                returnPolicy = driver.find_element(By.XPATH, '//div[@id=\"RETURNS_POLICY\"]').text\n",
    "                if returnPolicy:\n",
    "                    values.append(\"Available - \"+returnPolicy)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching Expected Delivery Date If Available\n",
    "            try:\n",
    "                delivery = driver.find_element(By.XPATH, '//div[@id=\"mir-layout-DELIVERY_BLOCK\"]//b').text\n",
    "                if delivery:\n",
    "                    values.append(delivery)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching Availability Of Product\n",
    "            try:\n",
    "                available = driver.find_element(By.XPATH, '//div[@id=\"availability\"]/span').text\n",
    "                if available:\n",
    "                    values.append(available)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "            # Fetching URL Of The Product Page\n",
    "            try:\n",
    "                productUrl = driver.current_url\n",
    "                if productUrl:\n",
    "                    values.append(productUrl)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"-\")\n",
    "                \n",
    "            # Closing The Product Tab\n",
    "            driver.close()\n",
    "            \n",
    "            # Switching to Previous Tab\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            \n",
    "            # Adding Scraped Information To The DataFrame\n",
    "            Amazon_Product_Detail.loc[len(Amazon_Product_Detail)] = values\n",
    "            \n",
    "        # Clicking On Next Button\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]').click()\n",
    "        except Exception:\n",
    "            break    \n",
    "    \n",
    "    # Printing DataFrame\n",
    "    print(Amazon_Product_Detail)\n",
    "    # Saving DataFrame Into A CSV File\n",
    "    Amazon_Product_Detail.to_csv(\"Amazon_Product.csv\", index=False)\n",
    "    \n",
    "# Taking Desired Product As Input From User\n",
    "product = input(\"Enter the Product Of Your Choice : \")\n",
    "url = \"https://www.amazon.in/\"             # URL Of Website\n",
    "scraper1_2(url, product)                   # Calling Function And Passing URL And User Input As Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2047d4b",
   "metadata": {},
   "source": [
    "## Q3.\n",
    "           Write a python program to access the search bar and search button on images.google.com and\n",
    "\n",
    "           scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b0b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")            # Creating Instance Of Webdriver\n",
    "def scraper3(a, c):                                      # Defining function with url and product name as parameters\n",
    "    driver.get(a)                                        # Loading URL Of Website\n",
    "    driver.maximize_window()                             # Maximizing Browser Window\n",
    "    time.sleep(5)                                        # Making driver sleep for 5 seconds\n",
    "\n",
    "    image_dict = {}                                      # Creating an empty Dictionary\n",
    "    os.makedirs('GoogleImages', exist_ok=True)           # Creating Folder \"GoogleImages\" In Current Working Directory To Store Downloaded Images\n",
    "                                                         # If Folder already exists It Will be used to store the downloaded Images  \n",
    "    # Logic to fetch images related to each keyword\n",
    "    for keyword in images:\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.send_keys(keyword)\n",
    "        search_field.send_keys(Keys.ENTER)\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        items = driver.find_elements(By.XPATH, '//div[@data-ri]/a[1]/div/img[contains(@src,\"http\")]')\n",
    "        for item in items:\n",
    "            try:\n",
    "                imageLink = item.get_attribute(\"src\")\n",
    "            except Exception as e:\n",
    "                print(\"Unable to scrap image name and image link --> \" + str(e))\n",
    "\n",
    "            if (imageLink is not None) and (imageLink[0:4] == 'http'):\n",
    "                if keyword in image_dict.keys():\n",
    "                    image_dict[keyword].append(imageLink)\n",
    "                else:\n",
    "                    image_dict[keyword] = [imageLink]\n",
    "\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.clear()\n",
    "    # Logic to write the downloaded images to the created folder and naming the images\n",
    "    for key, value in image_dict.items():\n",
    "        count = 1\n",
    "        for imgLink in value:\n",
    "            try:\n",
    "                if count > 10:\n",
    "                    break\n",
    "                response = requests.get(imgLink)\n",
    "                img = open('GoogleImages/' + key + '_{}.jpeg'.format(count), \"wb\")\n",
    "                img.write(response.content)\n",
    "                print(\"Downloaded Image {} of 10 of \".format(count) + key)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "\n",
    "url = \"https://images.google.com/\"                                     # URL Of Website\n",
    "images = ['Fruits', 'Cars', 'Machine Learning', 'Guitar', 'Cakes']     # List Of Items For Which Images are to be downloaded\n",
    "scraper3(url, images)                                                  # calling function and passing url and list of items as arguments\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2bf31",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "\n",
    "      Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "      www.flipkart.com and\n",
    "      Scrape following details for all the search results displayed on 1st page.\n",
    "      \n",
    "      Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary \n",
    "      Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. \n",
    "      \n",
    "      Incase if any of the details is missing then replace it by “- “.\n",
    "      \n",
    "      Save your results in a dataframe and CSV.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418055a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')           # Creating Instance Of Webdriver\n",
    "def scraper4(c,phone):                                # Defining The Function With URL And Smartphone Name As Parameters\n",
    "    driver.get(c)                                     # Loading Website\n",
    "    driver.maximize_window()                          # Maximizing The Chrome Browser Window\n",
    "    time.sleep(5)                                     # Making Driver To Wait For 5 Seconds\n",
    "    popup=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()       # Closing PopUp Window \n",
    "    # Finding Search Field Using XPATH And Passing User Input Into It\n",
    "    search_field=driver.find_element_by_class_name(\"_3704LK\").send_keys(phone) \n",
    "    # Finding Search Button Using XPATH And Clicking On it\n",
    "    search_button=driver.find_element_by_class_name(\"L0Z3Pu\").click()        \n",
    "    time.sleep(5)                                     # Making Driver To Wait For 5 Seconds \n",
    "    #Fetching the url_tags of all the smartphones\n",
    "    url_tags=driver.find_elements_by_xpath(('//div[@class=\"_2kHMtA\"]/a'))\n",
    "    urls=[]\n",
    "    # Creating DataFrame To Store The Outcome\n",
    "    SPhone_Df=pd.DataFrame([],columns=['Brand Name',\"SmartPhone's Name\",\"SmartPhone's Colour\",\n",
    "                                       \"SmartPhone's RAM\",\"SmartPhone's ROM\",'Display Size',\n",
    "                                       'Primary Camera','Secondary Camera',\"SmartPhone's Battery\",\n",
    "                                       \"SmartPhone's Price\",'URL Of The Product'])\n",
    "    # Fetching urls of all the smartphones\n",
    "    for u in url_tags:\n",
    "        urls.append(u.get_attribute('href'))\n",
    "    # Fetching All the Required Information \n",
    "    for url in urls:\n",
    "        # Loading The Urls Of Each Smartphone One By One\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')))\n",
    "        except Exception:\n",
    "            driver.refresh()\n",
    "            WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')))\n",
    "        # Clicking On Read More Button\n",
    "        read_more_button=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]').click()\n",
    "        time.sleep(2)\n",
    "        # fetching brand of smartphone\n",
    "        try:\n",
    "            brand_name=driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]').text.split(' ')[0]\n",
    "        except Exception:\n",
    "            brand_name='-'\n",
    "        # fetching name of smartphone\n",
    "        try:\n",
    "            sphone_name=driver.find_element_by_xpath('//td[text()=\"Model Name\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            sphone_name='-'\n",
    "        # fetching color of smartphone\n",
    "        try:\n",
    "            sphone_color=driver.find_element_by_xpath('//td[text()=\"Color\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            sphone_color='-'\n",
    "        # fetching RAM of smartphone\n",
    "        try:\n",
    "            sphone_ram=driver.find_element_by_xpath('//td[text()=\"RAM\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            sphone_ram='-'\n",
    "        # Fetching ROM of smartphone\n",
    "        try:\n",
    "            sphone_rom=driver.find_element_by_xpath('//td[text()=\"Internal Storage\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            sphone_rom='-'\n",
    "        #Fetching Display Size Of Smartphone\n",
    "        try:\n",
    "            display_size=driver.find_element_by_xpath('//td[text()=\"Display Size\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            display_size='-'\n",
    "        # Fetching Primary Camera Specifications\n",
    "        try:\n",
    "            primary_cam=driver.find_element_by_xpath('//td[text()=\"Primary Camera\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            primary_cam='-'\n",
    "        # Fetching Secondary Camera specifications        \n",
    "        try:\n",
    "            sec_cam=driver.find_element_by_xpath('//td[text()=\"Secondary Camera\"]/following-sibling::td//li').text\n",
    "        except NoSuchElementException as e:\n",
    "            try:\n",
    "                sec_cam=driver.find_element_by_xpath('//td[text()=\"Secondary Camera Features\"]/following-sibling::td//li').text\n",
    "            except NoSuchElementException as e:\n",
    "                sec_cam=\"-\"\n",
    "        # fetching battery capacity of smartphone\n",
    "        try:\n",
    "            sphone_battery=driver.find_element_by_xpath('//td[text()=\"Battery Capacity\"]/following-sibling::td//li').text\n",
    "        except Exception:\n",
    "            sphone_battery='-'\n",
    "        # fetching price of smartphone\n",
    "        try:\n",
    "            sphone_price=driver.find_element_by_xpath('//div[@class=\"_25b18c\"]/div').text\n",
    "        except Exception:\n",
    "            sphone_price='-'\n",
    "        # URL of the page on which all the information about the smartphone is present\n",
    "        product_url=url\n",
    "        # List to store all the information gathered above \n",
    "        Values=[brand_name,sphone_name,sphone_color,sphone_ram,sphone_rom,display_size,primary_cam,\n",
    "                sec_cam,sphone_battery,sphone_price,product_url]\n",
    "        # Passing Gathered information into DataFrame\n",
    "        SPhone_Df.loc[len(SPhone_Df)]=Values\n",
    "    # Printing Dataframe\n",
    "    print(SPhone_Df)                                                                            \n",
    "    # Saving DataFrame In CSV Format\n",
    "    SPhone_Df.to_csv('Products.csv',index=False)\n",
    "# Taking User Input For Smartphone Choice \n",
    "sphone=input(\"Enter the Smartphone You want to search for : \")\n",
    "url='https://www.flipkart.com/'        # URL of Website\n",
    "scraper4(url,sphone)                   # Calling Function And Passing URL And User Input As Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f58696",
   "metadata": {},
   "source": [
    "## Q5.\n",
    "         Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "         maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85549811",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # Creating Instance Of Webdriver\n",
    "def scraper5(d,c):                                # Defining Function\n",
    "    driver.get(d)                                 # loading Webpage\n",
    "    driver.maximize_window()                      # Maximizing Chrome Window\n",
    "    time.sleep(5)                                 # Making driver to wait for 5 Seconds\n",
    "    \n",
    "    search_field=driver.find_element_by_id('searchboxinput').send_keys(c)     # Finding Search Field And Sending Input\n",
    "    search_button=driver.find_element_by_id('searchbox-searchbutton').click() # Finding Search Button And Clicking On It\n",
    "    time.sleep(5)                                                             # Making driver to wait for 5 Seconds\n",
    "    \n",
    "    # Extracting City URL From Webpage\n",
    "    city_url=driver.find_element_by_xpath('//a[@id=\"gb_70\"]').get_attribute('href')\n",
    "    \n",
    "    # Using Regex Methods to Extract Longitude And Latitude Of Entered City\n",
    "    city_longitude=city_url.split('%2F%40')[1].split('%2C1')[0].split('%2C')[0]\n",
    "    city_latitude=city_url.split('%2F%40')[1].split('%2C1')[0].split('%2C')[1]\n",
    "    \n",
    "    # Printing Extracted Desired Outcomes\n",
    "    print(f\"Longitude Of {c.capitalize()} = {city_longitude}, Latitude Of {c.capitalize()} = {city_latitude}\")\n",
    "\n",
    "url='https://www.google.com/maps/@20.9880135,82.7525294,5z'   # URL Of The Google Maps Website\n",
    "city=input(\"Enter City Of Your Choice :\")                     # Taking User Input As City\n",
    "scraper5(url,city)                                            # Passing URL And City As Parameters To The Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd9cd7",
   "metadata": {},
   "source": [
    "## Q6.\n",
    "        Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21)\n",
    "        from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # creating instance of webdriver\n",
    "def scraper6(a):                                  # defining a function with url as a parameter\n",
    "    driver.get(a)                                 # loading webpage using url passed as an argument to the function\n",
    "    driver.maximize_window()                      # maximizing browser window\n",
    "    time.sleep(5)                                 # making driver wait for 5 seconds\n",
    "    # creating a dataframe to store all the required information\n",
    "    Investment_Details = pd.DataFrame([], columns=[\"Sr.No.\", \"Date\", \"Startup Name\", \"Industry/Vertical\", \"Sub-Vertical\",\n",
    "                                               \"City/Location\", \"Investor's Name\", \"Investment Type\", \"Amount (In USD)\"])\n",
    "    \n",
    "    # fetching funding-deals page url from homepage to gather the required information\n",
    "    funding_deals_url=driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "    driver.get(funding_deals_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # fetching month tags using xpath\n",
    "    month_names = driver.find_elements(By.XPATH, '//h2[contains(@class,\"tablepress-table-name\")]')\n",
    "    # fetching all the information related to funding deals of second quarter\n",
    "    for month in month_names:\n",
    "        if month.text in ['March, 2021', 'February, 2021', 'January, 2021']:\n",
    "            table_id = month.get_attribute(\"class\").split(' ')[1].split('-')[4]\n",
    "            xpath_string = '//table[@id=\"tablepress-' + table_id + '\"]//tbody/tr'\n",
    "            values = [i.text for i in driver.find_elements(By.XPATH, xpath_string+'/td')]\n",
    "            for i in range(0, len(values), 9):\n",
    "                Investment_Details.loc[len(Investment_Details)] = values[i:i + 9]\n",
    "    # dropping \"Sr.No.\" Columns from dataframe\n",
    "    Investment_Details = Investment_Details.drop('Sr.No.',axis=1)\n",
    "    # printing dataframe\n",
    "    print(Investment_Details)\n",
    "# URL of website\n",
    "url = \"https://trak.in\"\n",
    "scraper6(url)              # calling function with url as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f6af9",
   "metadata": {},
   "source": [
    "## Q7.\n",
    "         Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e66924",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")          # Creating Instance Of Webdriver\n",
    "def scraper7(a):                                       # creating function with url as a parameter\n",
    "    driver.get(a)                                      # Loading the website\n",
    "    driver.maximize_window()                           # maximizing browser window\n",
    "    time.sleep(5)                                      # making driver to sleep for 5 seconds\n",
    "    # creating a dataframe to store all the required information after scraping\n",
    "    Product_df = pd.DataFrame([], columns=[\"Laptop's Name\", \"Laptop's Price\", \"Laptop's OS\", \"Display Size(In Inches)\",\n",
    "                                           \"Processor's Name\", \"Laptop's HDD\",\n",
    "                                           \"Laptop's RAM\", \"Laptop's Weight(In Kgs)\", \"Laptop's Dimensions(In MM)\",\n",
    "                                           \"Laptop's GPU\"])\n",
    "    # clicking on \"Top10\" Link\n",
    "    Top10_link = driver.find_element(By.XPATH, '//a[text()=\"Top 10\"]').click()\n",
    "    time.sleep(2)\n",
    "    # clicking on \"Laptops\" option \n",
    "    laptops_button = driver.find_element(By.XPATH, '//div[@class=\"categoty_list\"]/button/img[@title=\"Laptops\"]')\n",
    "    ActionChains(driver).move_to_element(laptops_button).click().perform()\n",
    "    time.sleep(2)\n",
    "    # clicking on \"Best Gaming Laptops in India\" link on the page\n",
    "    driver.find_element(By.XPATH, '//a[text()=\"Best Gaming Laptops in India\"]').click()\n",
    "    \n",
    "    #Logic to click on \"Full specs\" option related to every product and then scrape the required information\n",
    "    items = driver.find_elements_by_xpath('//span[text()=\"Full specs\"]')\n",
    "    for item in items:\n",
    "        driver.execute_script(\"window.open('%s', '_blank')\" % item.get_attribute('data-href'))\n",
    "        try:\n",
    "            WebDriverWait(driver, 20)\n",
    "        except selenium.common.exceptions.TimeoutException:\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # fetching model name of laptop\n",
    "        try:\n",
    "            productName = driver.find_element_by_xpath('//td[text()=\"Model name\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            productName = '-'\n",
    "        \n",
    "        # fetching Operating System of laptop\n",
    "        try:\n",
    "            osName = driver.find_element_by_xpath('//td[text()=\"operating system (with version)\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            osName = '-'\n",
    "        \n",
    "        # fetching size of display of laptop\n",
    "        try:\n",
    "            displaySize = driver.find_element_by_xpath('//td[text()=\"display size (in inches)\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            displaySize = '-'\n",
    "        \n",
    "        # fetching name of processor of laptop\n",
    "        try:\n",
    "            processorName = driver.find_element_by_xpath('//td[text()=\"processor model name\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            processorName = '-'\n",
    "        \n",
    "        # fetching storage type and storage capacity of laptop\n",
    "        try:\n",
    "            hddType = driver.find_element_by_xpath('//td[text()=\"Storage drive type\"]/following-sibling::td[2]').text\n",
    "            hddCapacity = driver.find_element_by_xpath('//td[text()=\"Storage drive capacity\"]/following-sibling::td[2]').text\n",
    "            Storage = hddCapacity+\" \"+hddType\n",
    "        except Exception:\n",
    "            Storage = '-'\n",
    "        \n",
    "        # fecthing RAM type and capacity of laptop\n",
    "        try:\n",
    "            ramCapacity = driver.find_element_by_xpath('//td[text()=\"ram included (in gb)\"]/following-sibling::td[2]').text\n",
    "            ramType = driver.find_element_by_xpath('//td[text()=\"ram type\"]/following-sibling::td[2]').text\n",
    "            ram = ramCapacity +\" \"+ramType\n",
    "        except Exception:\n",
    "            ram = '-'\n",
    "        \n",
    "        # fetching weight of laptop\n",
    "        try:\n",
    "            weight = driver.find_element_by_xpath('//td[text()=\"laptop weight (in kgs)\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            weight = '-'\n",
    "        \n",
    "        # fetching dimensions of laptop\n",
    "        try:\n",
    "            dimensions = driver.find_element_by_xpath('//td[text()=\"laptop dimension (in mm)\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            dimensions = '-'\n",
    "        \n",
    "        # fetching Graphics Processor Name\n",
    "        try:\n",
    "            gpu = driver.find_element_by_xpath('//td[text()=\"graphics processor\"]/following-sibling::td[2]').text\n",
    "        except Exception:\n",
    "            gpu = '-'\n",
    "        \n",
    "        # fetching price of laptop\n",
    "        try:\n",
    "            price = driver.find_element_by_xpath('//span[@class=\"price last-child\"]').text\n",
    "        except Exception:\n",
    "            price = '-'\n",
    "        \n",
    "        # storing all the fetched values in a list\n",
    "        Values = [productName, price, osName, displaySize, processorName, Storage, ram, weight, dimensions, gpu]\n",
    "        # closing the tab\n",
    "        driver.close()\n",
    "        # switching to previous tab\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # passing values to Dataframe \n",
    "        Product_df.loc[len(Product_df)] = Values\n",
    "   \n",
    "    # printing dataframe\n",
    "    print(Product_df)\n",
    "\n",
    "url = \"https://www.digit.in/\"          # URL of website\n",
    "scraper7(url)                          # calling function and passing URL as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b80155",
   "metadata": {},
   "source": [
    "## Q8.\n",
    "        Write a python program to scrape the details for all billionaires from www.forbes.com.\n",
    "        Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")          # creatig instance of webdriver\n",
    "def scraper8(a):                                       # creating function with url as a parameter\n",
    "    driver.get(a)                                      # loading website\n",
    "    driver.maximize_window()                           # maximizing browser window\n",
    "    time.sleep(5)                                      # making driver sleep for 5 seconds\n",
    "    # creating a dataframe to store all the scraped information\n",
    "    Billionaires_df = pd.DataFrame([], columns=[\"Rank\", \"Name\", \"Net Worth\", \"Age\", \"Citizenship\", \"Source\", \"Industry\"])\n",
    "    \n",
    "    # logic to fetch the required information from website\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        time.sleep(5)\n",
    "        # logic for closing the popup if it occurs\n",
    "        if count == 2:\n",
    "            try:\n",
    "                WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[@class=\"tp-close tp-active\"]'))).click()\n",
    "            except Exception:\n",
    "                pass\n",
    "        billionaire_rows = [item.text for item in driver.find_elements(By.XPATH, '//div[@class=\"table-row-group__container\"]/div[@role=\"row\"]')]\n",
    "        # iterating table row content and splitting the values as per each of the columns\n",
    "        for item in billionaire_rows:\n",
    "            splitted_row_values = item.split('\\n')\n",
    "            if len(splitted_row_values) > 7:\n",
    "                splitted_row_values.remove('★')\n",
    "            splitted_row_values[0] = splitted_row_values[0].split('.')[0]\n",
    "            Billionaires_df.loc[len(Billionaires_df)] = splitted_row_values\n",
    "            \n",
    "        # clicking on \"NEXT\" button\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '//button[@class=\"pagination-btn pagination-btn--next \"]').click()\n",
    "        except Exception:\n",
    "            break\n",
    "    # printing dataframe\n",
    "    print(Billionaires_df)\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\"               # URL of website\n",
    "scraper8(url)                                              # calling function with url as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff55634",
   "metadata": {},
   "source": [
    "## Q9. \n",
    "          Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "          from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")      # creating instance of webdriver\n",
    "def scraper9(u):                                   # defining a function with url as a parameter\n",
    "    driver.get(u)                                  # loading the website\n",
    "    driver.maximize_window()                       # maximizing browser window\n",
    "    time.sleep(5)                                  # making the driver to sleep for 5 seconds\n",
    "    # creating a dataframe to store all the scraped information\n",
    "    Comment_Info=pd.DataFrame([], columns=['Comment', 'Time of Comment', 'Likes on Comment'])\n",
    "    # logic to check if the title of the video is available to fetch\n",
    "    try:\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"container\"]/h1/yt-formatted-string')))\n",
    "    except Exception:             \n",
    "        driver.refresh()\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"container\"]/h1/yt-formatted-string')))\n",
    "    \n",
    "    # Logic to scrape the required values\n",
    "    # driver will pause for 2 seconds initially after scrolling to load the comments\n",
    "    pause_time_while_scrolling = 2\n",
    "    # driver will scroll for 25 times\n",
    "    cycles = 25\n",
    "\n",
    "    html = driver.find_element(By.TAG_NAME, 'html')\n",
    "\n",
    "    try:\n",
    "        html.send_keys(Keys.PAGE_DOWN)\n",
    "    except Exception:\n",
    "        html.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "    time.sleep(pause_time_while_scrolling*3)\n",
    "    # fetching the title of the video\n",
    "    title = driver.find_element(By.XPATH, '//*[@id=\"container\"]/h1/yt-formatted-string').text.split('|')\n",
    "    print(\"Scrapping Details for - \" + title[0]+\" | \"+title[1])\n",
    "    # scrolling for each cycle and pausing for certain time\n",
    "    for i in range(cycles):\n",
    "        html.send_keys(Keys.END)\n",
    "        time.sleep(pause_time_while_scrolling)\n",
    "    # fetching all comment sections\n",
    "    comments_section= driver.find_elements(By.XPATH, '//div[@id=\"contents\"]/ytd-comment-thread-renderer')\n",
    "    # fetching all the required information from comments_section\n",
    "    for i in range(1, len(comments_section)+1):\n",
    "        # fetching comment text if available\n",
    "        try:\n",
    "            comments_text = driver.find_element(By.XPATH, '//div[@id=\"contents\"]/ytd-comment-thread-renderer[{i}]//yt-formatted-string[@id=\"content-text\"]'.format(i=i)).text.replace('\\n',' ')\n",
    "        except Exception:\n",
    "            comments_text = \"-\"\n",
    "        # fetching comment time if available\n",
    "        try:\n",
    "            comment_time = driver.find_element(By.XPATH, '//div[@id=\"contents\"]/ytd-comment-thread-renderer[{i}]//yt-formatted-string[contains(@class,\"published-time-text\")]'.format(i=i)).text\n",
    "        except Exception:\n",
    "            comment_time = \"-\"\n",
    "        # fetching no of likes on comment if available\n",
    "        try:\n",
    "            likes_count = driver.find_element(By.XPATH, '//div[@id=\"contents\"]/ytd-comment-thread-renderer[{i}]//span[@id=\"vote-count-middle\"]'.format(i=i)).text\n",
    "            if likes_count == \"\":\n",
    "                likes_count = 0\n",
    "        except Exception:\n",
    "            likes_count = \"-\"\n",
    "        # storing all the scraped values in a list\n",
    "        Values = [comments_text, comment_time, likes_count]\n",
    "        # adding all the values to the dataframe\n",
    "        Comment_Info.loc[len(Comment_Info)] = Values\n",
    "    # printing dataframe\n",
    "    print(Comment_Info)\n",
    "# URL of the youtube video\n",
    "vid_url=\"https://www.youtube.com/watch?v=zR5-HbFW6hc\"\n",
    "scraper9(vid_url)   # calling funtion with url as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e9da1",
   "metadata": {},
   "source": [
    "## Q10.\n",
    "        Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "        “London” location. \n",
    "        You have to scrape hostel name, distance from city centre, ratings, total reviews,\n",
    "        overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")    # creating instance of webdriver\n",
    "def scraper10(a):                                # defining a function with url of the website as a parameter\n",
    "    driver.get(a)                                # loading website\n",
    "    driver.maximize_window()                     # maximizing browser window\n",
    "    time.sleep(5)                                # making driver wait for 5 seconds after loading the website\n",
    "    \n",
    "    # creating a dataframe to store all the scraped information \n",
    "    Hostel_Details = pd.DataFrame([], columns=[\"Hostel Name\", \"Distance from City Centre\", \"Ratings\", \"Total Reviews\", \"Overall Review\",\n",
    "                                               \"Privates from price\", \"Dorms from price\", \"Facilities\", \"Property Description\"])\n",
    "    # logic to scrape all the required information\n",
    "    page = 0\n",
    "    div_count = 0\n",
    "    while True:\n",
    "        page += 1\n",
    "        time.sleep(10)\n",
    "        properties = driver.find_elements(By.XPATH, '//div[@class=\"message-container\"]/../div[@class=\"property-card\"]//a[@class=\"view-button\"]')\n",
    "        for i in range(0, len(properties)):\n",
    "            if page > 1:\n",
    "                div_count = 2\n",
    "            else:\n",
    "                div_count = 3\n",
    "            # fetching name of hostel if available\n",
    "            try:\n",
    "                hostelName = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//h2[contains(@class,\"title-6\")]'.format(no=i+div_count)).text\n",
    "            except Exception:\n",
    "                hostelName = \"-\"\n",
    "            # fetching distance of hostel from city centre if available\n",
    "            try:\n",
    "                distanceCityCentre = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[@class=\"property\"]//div[@class=\"title-row\"]//div[@class=\"subtitle body-3\"]/a/span[@class=\"description\"]'.format(no=i+div_count)).text.split('-')[1].split(' ')[0]\n",
    "            except Exception:\n",
    "                distanceCityCentre = \"-\"\n",
    "            # fetching ratings of hostel if available\n",
    "            try:\n",
    "                rating = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"score\")]'.format(no=i+div_count)).text\n",
    "            except Exception:\n",
    "                rating = \"-\"\n",
    "            # fetching total number of reviews if available\n",
    "            try:\n",
    "                totalReview = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"summary \")]//div[@class=\"reviews\"]'.format(no=i+div_count)).text.split('Total')[0]\n",
    "            except Exception:\n",
    "                totalReview = \"-\"\n",
    "            # fetching overall review if available\n",
    "            try:\n",
    "                overallReview = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"summary \")]//div[@class=\"keyword\"]'.format(no=i+div_count)).text\n",
    "            except Exception:\n",
    "                overallReview = \"-\"\n",
    "            # fetching price of private rooms if available\n",
    "            try:\n",
    "                privatesPrice = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"prices-col\")]//a[@class=\"prices\"]/div[1]/div'.format(no=i+div_count)).text.split(' ')[1]\n",
    "            except Exception:\n",
    "                try:\n",
    "                    privatesPrice = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"prices-col\")]//a[@class=\"prices\"]/div[1]/div'.format(no=i + div_count)).text\n",
    "                except Exception:\n",
    "                    privatesPrice = \"-\"\n",
    "            # fetching price of dorm if available\n",
    "            try:\n",
    "                dormPrice = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"prices-col\")]//a[@class=\"prices\"]/div[2]/div'.format(no=i+div_count)).text.split(' ')[1]\n",
    "            except Exception:\n",
    "                try:\n",
    "                    dormPrice = driver.find_element(By.XPATH, '//div[@class=\"message-container\"]/../div[{no}][@class=\"property-card\"]//div[contains(@class,\"prices-col\")]//a[@class=\"prices\"]/div[2]/div'.format(no=i + div_count)).text\n",
    "                except Exception:\n",
    "                    dormPrice = \"-\"\n",
    "            # clicking on property link\n",
    "            try:\n",
    "                properties[i].click()\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                ActionChains(driver).key_down(Keys.CONTROL).click(properties[i]).key_up(Keys.CONTROL).perform()\n",
    "                try:\n",
    "                    WebDriverWait(driver, 30).until(EC.number_of_windows_to_be(2))\n",
    "                except selenium.common.exceptions.TimeoutException:\n",
    "                    WebDriverWait(driver, 10).until(EC.number_of_windows_to_be(2))\n",
    "                    continue\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "            except Exception:\n",
    "                print(\"Unable to click 'View' Button\")\n",
    "            # fetching all the facilities if available\n",
    "            try:\n",
    "                facility = driver.find_elements(By.XPATH, '//div[@class=\"facilities-container\"]//ul[@class=\"facility-sections\"]//ul[@class=\"facilities\"]/li')\n",
    "                facilities = [f.text for f in facility]\n",
    "            except Exception:\n",
    "                facilities = \"-\"\n",
    "            # fetching description of property if available\n",
    "            try:\n",
    "                propertyDescription = driver.find_element(By.XPATH, '//div[@class=\"flex-80\"]//div[@class=\"description-container\"]/div/div[2]').text.replace('\\n','')\n",
    "            except Exception:\n",
    "                propertyDescription = \"-\"\n",
    "            # closing new tab and switching to previous tab\n",
    "            try:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "            except Exception as e:\n",
    "                print(\"Driver Session Expired - Unable to switch driver window\")\n",
    "            # passing scraped information to the dataframe\n",
    "            Hostel_Details.loc[len(Hostel_Details)] = [hostelName, distanceCityCentre, rating, totalReview, overallReview, privatesPrice, dormPrice, ', '.join(facilities), propertyDescription]\n",
    "        # clicking on \"NEXT\" button\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.XPATH, '//div[@class=\"pagination-item pagination-next\"]')\n",
    "            driver.execute_script('arguments[0].click()', next_btn)\n",
    "        except Exception as e:\n",
    "            break\n",
    "    # printing dataframe\n",
    "    print(Hostel_Details)\n",
    "    # converting dataframe into a CSV file\n",
    "    Hostel_Details.to_csv(\"Hostelworld.csv\", index=False)\n",
    "\n",
    "# getting \"City\" as an input from user\n",
    "city = input(\"Please Enter the City in which you are looking for hostels :\")\n",
    "url = \"https://www.hostelworld.com/s?q={}&type=city&id=3&guests=1\".format(city)    # URL of the website\n",
    "scraper10(url)                                                                     # calling function with url as an argument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
