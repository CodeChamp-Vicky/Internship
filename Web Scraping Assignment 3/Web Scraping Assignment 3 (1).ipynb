{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007a1d5",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "     • All questions are compulsory.\n",
    "     • In each of the questions you have to automate the process. You do not have to click on any button, click\n",
    "       any clickable element, enter keywords in search boxes manually. Each process has to be performed via\n",
    "       coding.\n",
    "     • Q1 and Q2 are connected questions i.e. after attempting Q1 proceed to Q2. Do not write whole code\n",
    "       from beginning for Q2.\n",
    "     • You may use any web scraping library and tools.\n",
    "     • The question can be attempted in various ways; the correctness of question depends on the output.\n",
    "     • If you encounter any Null values during scraping, you may replace it by hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a27fe",
   "metadata": {},
   "source": [
    "## Q1.\n",
    "          Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    " \n",
    "          The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’.\n",
    " \n",
    "          Then search for guitars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aec3b",
   "metadata": {},
   "source": [
    "## Q2.\n",
    "          In the above question, now scrape the following details of each product listed in first 3 pages of your\n",
    "          search results and save it in a data frame and csv.\n",
    " \n",
    "          In case if any product has less than 3 pages in search\n",
    "          results then scrape all the products available under that product name.\n",
    " \n",
    "          Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\",   \n",
    "          \"Availability\" and “Product URL”.\n",
    " \n",
    "         In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')   # Creating Instance Of WebDriver\n",
    "def scraper1(u,p):                              # Defining Function With Two Parameters(URL And Product Name)\n",
    "    driver.get(u)                               # Loading Webpage \n",
    "    time.sleep(10)                              # Assigning 10 seconds Of Sleep Time To Get The Page Loaded Properly\n",
    "    driver.maximize_window()                    # Maximizing Browser Window\n",
    "    time.sleep(5)\n",
    "    # Sending Input To The Search Field And Clicking On Search Button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys(p)\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "    time.sleep(10)                              # Assigning 10 seconds Of Sleep Time To Get The Page Loaded Properly\n",
    "    # Creating A DataFrame With Required Columns\n",
    "    Amazon_Product_Detail = pd.DataFrame([], columns=[\"Name of the Product\", \"Brand Name\",  \"Price\", \"Return/Exchange\",\n",
    "                                                     \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "    # Logic To Fetch All The Required Information From The Website\n",
    "    for page in range(3):\n",
    "        time.sleep(10)\n",
    "        items = driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "        for i in items:\n",
    "            values = []\n",
    "            i.click()\n",
    "            try:\n",
    "                WebDriverWait(driver, 30).until(EC.number_of_windows_to_be(2))\n",
    "            except selenium.common.exceptions.TimeoutException:\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            try:\n",
    "                productName = driver.find_element(By.XPATH, '//div[@id=\"titleSection\"]').text\n",
    "                if productName:\n",
    "                    values.append(productName)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                brandName = driver.find_element(By.XPATH, '//*[@id=\"productOverview_feature_div\"]/div/table/tbody/tr[1]/td[2]/span').text\n",
    "                if brandName:\n",
    "                    values.append(brandName)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                price = driver.find_element(By.XPATH, '//*[@id=\"corePrice_feature_div\"]/div/span').text.split('₹')[1].split('\\n')[0]\n",
    "                if price:\n",
    "                    values.append(price)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                returnPolicy = driver.find_element(By.XPATH, '//div[@id=\"RETURNS_POLICY\"]').text\n",
    "                if returnPolicy:\n",
    "                    values.append(\"Available - \"+returnPolicy)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                delivery = driver.find_element(By.XPATH, '//div[@id=\"mir-layout-DELIVERY_BLOCK\"]//b').text\n",
    "                if delivery:\n",
    "                    values.append(delivery)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                available = driver.find_element(By.XPATH, '//div[@id=\"availability\"]').text\n",
    "                if available:\n",
    "                    values.append(\"Yes\")\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "            try:\n",
    "                productUrl = driver.current_url\n",
    "                if productUrl:\n",
    "                    values.append(productUrl)\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                values.append(\"#\")\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            Amazon_Product_Detail.loc[len(Amazon_Product_Detail)] = values\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]').click()\n",
    "        except Exception:\n",
    "            break    \n",
    "    # Saving DataFrame Into A CSV File\n",
    "    Amazon_Product_Detail.to_csv(\"Amazon_Product.csv\", index=False)\n",
    "    # Printing DataFrame\n",
    "    print(Amazon_Product_Detail)\n",
    "# Taking Desired Product As Input From User\n",
    "product = input(\"Enter the Product Of Your Choice : \")\n",
    "url = \"https://www.amazon.in/\"\n",
    "scraper1(url, product)                   # Calling Function And Passing URL And User Input As Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2047d4b",
   "metadata": {},
   "source": [
    "## Q3.\n",
    "           Write a python program to access the search bar and search button on images.google.com and\n",
    "\n",
    "           scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')        # Creatng Instance Of WebDriver\n",
    "def scraper3(a, c):                                # Defining Function With Two Parameters(URL And List Of Items) \n",
    "    driver.get(a)                                  # Loading Website\n",
    "    driver.maximize_window()                       # Maximizing Browser Window\n",
    "    time.sleep(5)\n",
    "    image_dict = {}\n",
    "\n",
    "    # Logic To Scrape Required Information\n",
    "    for keyword in images:\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.send_keys(keyword)\n",
    "        search_field.send_keys(Keys.ENTER)\n",
    "        time.sleep(5)\n",
    "\n",
    "        for i in range(10):\n",
    "            if keyword in image_dict.keys():\n",
    "                image_dict[keyword].append(driver.find_element(By.XPATH, '//div[@data-ri=\"{}\"]/a[1]/div/img'.format(str(i))).get_attribute(\"src\"))\n",
    "            else:\n",
    "                image_dict[keyword] = [driver.find_element(By.XPATH, '//div[@data-ri=\"{}\"]/a[1]/div/img'.format(str(i))).get_attribute(\"src\")]\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.clear()\n",
    "    Images_df = pd.DataFrame(image_dict)\n",
    "    # Printing DataFrame\n",
    "    print(Images_df)\n",
    "# URL Of Website\n",
    "url = \"https://images.google.com/\"\n",
    "images = ['Fruits', 'Cars', 'Machine Learning', 'Guitar', 'Cakes']       # List Of Given Items\n",
    "scraper3(url, images)               # Calling Function And Passing URL And List Of Items As Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91beb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "def scraper3(a, c):\n",
    "    driver.get(a)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    image_dict = {}\n",
    "\n",
    "    for keyword in images:\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.send_keys(keyword)\n",
    "        search_field.send_keys(Keys.ENTER)\n",
    "        time.sleep(5)\n",
    "        for i in range(10):\n",
    "            if keyword in image_dict.keys():\n",
    "                image_dict[keyword].append(driver.find_element(By.XPATH, '//div[@data-ri=\"{}\"]/a[1]/div/img'.format(str(i))).get_attribute(\"src\"))\n",
    "            else:\n",
    "                image_dict[keyword] = [driver.find_element(By.XPATH, '//div[@data-ri=\"{}\"]/a[1]/div/img'.format(str(i))).get_attribute(\"src\")]\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@title=\"Search\"]')\n",
    "        search_field.clear()\n",
    "\n",
    "    Images_df = pd.DataFrame(image_dict)\n",
    "    print(Images_df)\n",
    "\n",
    "url = \"https://images.google.com/\"\n",
    "images = ['Fruits', 'Cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "scraper3(url, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2bf31",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "\n",
    "      Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "      www.flipkart.com and\n",
    "      Scrape following details for all the search results displayed on 1st page.\n",
    "      \n",
    "      Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary \n",
    "      Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. \n",
    "      \n",
    "      Incase if any of the details is missing then replace it by “- “.\n",
    "      \n",
    "      Save your results in a dataframe and CSV.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')           # Creating Instance Of Webdriver\n",
    "\n",
    "\n",
    "def scraper4(c,phone):                                # Defining The Function With URL And Smartphone Name As Arguments\n",
    "    driver.get(c)                                     # Loading Webpage\n",
    "    driver.maximize_window()                          # Maximising The Chrome Browser Window\n",
    "    time.sleep(5)                                     # Making Driver To Wait For 5 Seconds\n",
    "    popup=driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()       # Closing PopUp Window \n",
    "    # Finding Search Field Using XPATH And Passing User Input Into It\n",
    "    search_field=driver.find_element_by_class_name(\"_3704LK\").send_keys(phone) \n",
    "    # Finding Search Button Using XPATH And Clicking On it\n",
    "    search_button=driver.find_element_by_class_name(\"L0Z3Pu\").click()        \n",
    "    time.sleep(5)                                     # Making Driver To Wait For 5 Seconds\n",
    "    # Creating Empty Lists To Store Desired Values In Them    \n",
    "    brand_names=[]\n",
    "    sphone_names=[]\n",
    "    colours=[]\n",
    "    ram=[]\n",
    "    rom=[]\n",
    "    displays=[]\n",
    "    batteries=[]\n",
    "    prices=[]\n",
    "    product_urls=[]\n",
    "    \n",
    "    # Finding Web Elements Related To each Desired Output To Extract Desired Information From Them \n",
    "    brand_tags=driver.find_elements_by_xpath('//div[@class=\"_1AtVbE col-12-12\"]//child::div[@class=\"_4rR01T\"]')\n",
    "    memory_tags=driver.find_elements_by_xpath('//div[@class=\"col col-7-12\"]//li[@class=\"rgWa7D\"][1]')\n",
    "    display_tags=driver.find_elements_by_xpath('//ul[@class=\"_1xgFaf\"]/li[2]')\n",
    "    battery_tags=driver.find_elements_by_xpath('//ul[@class=\"_1xgFaf\"]/li[4]')\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"_30jeq3 _1_WHN1\"]')\n",
    "    url_tags=driver.find_elements_by_xpath(('//a[@class=\"_1fQZEK\"]'))\n",
    "    \n",
    "    \n",
    "    for b in brand_tags:                   \n",
    "        brand_names.append(b.text.split(' ')[0])\n",
    "        sphone_names.append(b.text.split('(')[0].strip())\n",
    "        colours.append((b.text.split('(')[-1]).split(',')[0])\n",
    "    for m in memory_tags:\n",
    "        ram.append(m.text.split('|')[0].strip())\n",
    "        rom.append(m.text.split('|')[1].strip())\n",
    "    for d in display_tags:\n",
    "        displays.append(d.text)\n",
    "    for bt in battery_tags:\n",
    "        batteries.append(bt.text)\n",
    "    for p in price_tags:\n",
    "        prices.append(p.text)\n",
    "    for u in url_tags:\n",
    "        product_urls.append(u.get_attribute('href'))\n",
    "    # Creating A Dictionary To Store Required Information   \n",
    "    info_dict={\"Brand Of Device\":brand_names,\"Name Of Device\":sphone_names,\n",
    "               'Colour Of Device':colours,'RAM Of Device':ram,'ROM Of Device':rom,'Display Size Of Device':displays,\n",
    "               'Battery Capacity Of Device':batteries,'Price Of Device':prices,'URL Of Device Page':product_urls}\n",
    "    # Creating DataFrame\n",
    "    df=pd.DataFrame(info_dict)\n",
    "    df.index=range(1,len(df)+1)\n",
    "    # Saving DataFrame In CSV Format\n",
    "    df.to_csv('Products.csv')\n",
    "    print(df)                                                                            \n",
    "# Taking User Input For Smartphone Choice \n",
    "sphone=input(\"Enter the Smartphone You want to search for : \")\n",
    "url='https://www.flipkart.com/'  \n",
    "scraper4(url,sphone)                   # Calling Function And Passing URL And User Input As Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f58696",
   "metadata": {},
   "source": [
    "## Q5.\n",
    "         Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "         maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4648d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')       # Creating Instance Of Webdriver\n",
    "def scraper4(d,c):                                # Defining Function\n",
    "    driver.get(d)                                 # loading Webpage\n",
    "    driver.maximize_window()                      # Maximizing Chrome Window\n",
    "    time.sleep(5)                                 # Making driver to wait for 5 Seconds\n",
    "    \n",
    "    search_field=driver.find_element_by_id('searchboxinput').send_keys(c)     # Finding Search Field And Sending Input\n",
    "    search_button=driver.find_element_by_id('searchbox-searchbutton').click() # Finding Search Button And Clicking On It\n",
    "    time.sleep(5)                                                             # Making driver to wait for 5 Seconds\n",
    "    \n",
    "    # Extracting City URL From Webpage\n",
    "    city_url=driver.find_element_by_xpath('//a[@id=\"gb_70\"]').get_attribute('href')\n",
    "    \n",
    "    # Using Regex Methods to Extract Longitude And Latitude Of Entered City\n",
    "    city_longitude=city_url.split('%2F%40')[1].split('%2C1')[0].split('%2C')[0]\n",
    "    city_latitude=city_url.split('%2F%40')[1].split('%2C1')[0].split('%2C')[1]\n",
    "    \n",
    "    # Printing Extracted Desired Outcomes\n",
    "    print(f\"Longitude Of {c.capitalize()} = {city_longitude}, Latitude Of {c.capitalize()} = {city_latitude}\")\n",
    "\n",
    "url='https://www.google.com/maps/@20.9880135,82.7525294,5z'   # URL Of The Google Maps Website\n",
    "city=input(\"Enter City Of Your Choice :\")                     # Taking User Input As City\n",
    "scraper4(url,city)                                            # Passing URL And City As Parameters To The Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd9cd7",
   "metadata": {},
   "source": [
    "## Q6.\n",
    "        Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21)\n",
    "        from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper6(a):\n",
    "    driver.get(a)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    Investment_Details = pd.DataFrame([], columns=[\"Sr.No.\", \"Date\", \"Startup Name\", \"Industry/Vertical\", \"Sub-Vertical\",\n",
    "                                               \"City/Location\", \"Investor's Name\", \"Investment Type\", \"Amount (In USD)\"])\n",
    "\n",
    "    month_names = driver.find_elements(By.XPATH, '//h2[contains(@class,\"tablepress-table-name\")]')\n",
    "    for month in month_names:\n",
    "        if month.text in ['March, 2021', 'February, 2021', 'January, 2021']:\n",
    "            table_id = month.get_attribute(\"class\").split(' ')[1].split('-')[4]\n",
    "            xpath_string = '//table[@id=\"tablepress-' + table_id + '\"]//tbody/tr'\n",
    "            values = [i.text for i in driver.find_elements(By.XPATH, xpath_string+'/td')]\n",
    "            for i in range(0, len(values), 9):\n",
    "                Investment_Details.loc[len(Investment_Details)] = values[i:i + 9]\n",
    "\n",
    "    Investment_Details = Investment_Details[[\"Date\", \"Startup Name\", \"Industry/Vertical\", \"Sub-Vertical\",\n",
    "                                               \"City/Location\", \"Investor's Name\", \"Investment Type\", \"Amount (In USD)\"]]\n",
    "    print(Investment_Details)\n",
    "\n",
    "url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "scraper6(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f6af9",
   "metadata": {},
   "source": [
    "## Q7.\n",
    "         Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper7(a, key):\n",
    "    driver.get(a)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    Product_df = pd.DataFrame([], columns=[\"Laptop Name\", \"Laptop Image Link\", \"Description\", \"Price\"])\n",
    "\n",
    "    search_button = driver.find_element(By.XPATH, '//div[@class=\"search\"]')\n",
    "    search_button.click()\n",
    "    search_field = driver.find_element(By.XPATH, '//input[@id=\"globalPageSearchText\"]')\n",
    "    search_field.send_keys(key)\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    driver.find_element(By.XPATH, '//input[@id=\"content_products\"]').click()\n",
    "    for page in range(3):\n",
    "        items = driver.find_elements(By.XPATH, '//div[@class=\"searchPage\"]/a')\n",
    "        for item in items:\n",
    "            ActionChains(driver).key_down(Keys.CONTROL).click(item).key_up(Keys.CONTROL).perform()\n",
    "            try:\n",
    "                WebDriverWait(driver, 20)\n",
    "            except selenium.common.exceptions.TimeoutException:\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            try:\n",
    "                productName = driver.find_element(By.XPATH, '//div[@class=\"heading-wraper\"]/h1').text\n",
    "            except Exception:\n",
    "                productName = \"#\"\n",
    "            try:\n",
    "                productDescription = driver.find_element(By.XPATH, '//div[@class=\"para_container\"]').text\n",
    "            except Exception:\n",
    "                productDescription = \"#\"\n",
    "            try:\n",
    "                productPrice = driver.find_element(By.XPATH, '//div[@class=\"price\"]/h2/strong').text\n",
    "            except Exception:\n",
    "                productPrice = \"#\"\n",
    "            try:\n",
    "                productImagelink = driver.find_element(By.XPATH, '//div[@class=\"slideshow-item\"]/img').get_attribute(\"src\")\n",
    "            except Exception:\n",
    "                productImagelink = \"#\"\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            Product_df.loc[len(Product_df)] = [productName, productImagelink, productDescription, productPrice]\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//a[@title=\"Next Page\"]').click()\n",
    "        except Exception:\n",
    "            break\n",
    "    print(Product_df)\n",
    "\n",
    "url = \"https://www.digit.in/\"\n",
    "keyword = \"Best Gaming Laptop\"\n",
    "scraper7(url, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b80155",
   "metadata": {},
   "source": [
    "## Q8.\n",
    "        Write a python program to scrape the details for all billionaires from www.forbes.com.\n",
    "        Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ba365",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "def scraper8(a):\n",
    "    driver.get(a)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    Billionaires_df = pd.DataFrame([], columns=[\"Rank\", \"Name\", \"Net Worth\", \"Age\", \"Citizenship\", \"Source\", \"Industry\"])\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        time.sleep(5)\n",
    "        if count == 2:\n",
    "            WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[@class=\"tp-close tp-active\"]'))).click()\n",
    "\n",
    "        billionaire_rows = [item.text for item in driver.find_elements(By.XPATH, '//div[@class=\"table-row-group__container\"]/div[@role=\"row\"]')]\n",
    "\n",
    "        for item in billionaire_rows:\n",
    "            splitted_row_values = item.split('\\n')\n",
    "            if len(splitted_row_values) > 7:\n",
    "                splitted_row_values.remove('★')\n",
    "            Billionaires_df.loc[len(Billionaires_df)] = splitted_row_values\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '//button[@class=\"pagination-btn pagination-btn--next \"]').click()\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "    print(Billionaires_df)\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "scraper8(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff55634",
   "metadata": {},
   "source": [
    "## Q9. \n",
    "          Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "          from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "def scraper9(u):\n",
    "    driver.get(u)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "    comments=[]\n",
    "    upvotes=[]\n",
    "    time_of_comment=[]\n",
    "    count=0\n",
    "\n",
    "    while count<=500:\n",
    "        WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "        comment_box=WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.XPATH, '//div[@id=\"main\"]')))\n",
    "        for c in comment_box:\n",
    "            comments.append(c.text)\n",
    "            upvotes.append(cu.text)\n",
    "            time_of_comment.append(ct.text)\n",
    "            count+=1\n",
    "    print(comments)\n",
    "#     comment_info={'Comments':comments,'Upvotes On Comment':upvotes,'Time Of Comment':time_of_comment}\n",
    "#     df=pd.DataFrame(comment_info)\n",
    "#     print(df)\n",
    "vid_url=\"https://www.youtube.com/watch?v=zR5-HbFW6hc\"\n",
    "scraper9(vid_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e9da1",
   "metadata": {},
   "source": [
    "## Q10.\n",
    "        Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "        “London” location. \n",
    "        You have to scrape hostel name, distance from city centre, ratings, total reviews,\n",
    "        overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e55152",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "def scraper10(a, c):\n",
    "    driver.get(a)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(5)\n",
    "\n",
    "    Hostel_Details = pd.DataFrame([], columns=[\"Hostel Name\", \"Distance from City Centre\", \"Ratings\", \"Total Reviews\", \"Overall Review\",\n",
    "                                               \"Privates from price\", \"Dorms from price\", \"Facilities\", \"Property Description\"])\n",
    "    try:\n",
    "        search_field = driver.find_element(By.XPATH, '//input[@id=\"location-text-input-field\"]')\n",
    "        search_field.send_keys(c)\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_xpath('//ul[@id=\"predicted-search-results\"]/li[2]').click()\n",
    "    except Exception as e:\n",
    "        print(\"Exception raised:\", e)\n",
    "\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]').click()\n",
    "    except Exception:\n",
    "        exit(-1)\n",
    "\n",
    "    time.sleep(10)\n",
    "    items = driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "#     for i in items:\n",
    "\n",
    "\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "city = input(\"Please Enter the City in which you are looking for hostels :\")\n",
    "scraper10(url, city)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
