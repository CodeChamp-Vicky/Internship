{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa27b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\vikas\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "# Installed bs4 and requests module\n",
    "\n",
    "!pip install bs4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b740",
   "metadata": {},
   "source": [
    "# Question 1. Write a python program to display all the header tags from https://en.wikipedia.org/wiki/Main_Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b8c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "All the header tags on the URL are :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1> \n",
      "\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2> \n",
      "\n",
      "\n",
      "<h2>Navigation menu</h2> \n",
      "\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3> \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library and requests  \n",
    "\n",
    "from bs4 import BeautifulSoup          \n",
    "import requests\n",
    "\n",
    "def header_func(y):                                        # function definition\n",
    "    \n",
    "    response = requests.get(url)                           # requesting response from the Web Page\n",
    "    \n",
    "    print(response, \"\\n\")                                  # for printing response from the Web Page\n",
    "    \n",
    "    soup=BeautifulSoup(response.content)                   # creating a BeautifulSoup object for fetching required information through it\n",
    "    \n",
    "    headers=soup.find_all(['h1','h2','h3','h4','h5','h6']) # fetching information related to all the header tags on the webpage and storing it in a container headers\n",
    "    \n",
    "    print(\"All the header tags on the URL are :\\n\")        # printing the message\n",
    "    \n",
    "    for h in headers:\n",
    "        print(h,\"\\n\\n\")                                    # printing the content stored in headers(container_name)\n",
    "\n",
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"              # URL of webpage\n",
    "all_headers = header_func(url)                             # passing URL of web page as a parameter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db99beb",
   "metadata": {},
   "source": [
    "# Question 2 :- Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b07a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Movie_Names   Movie Ratings   Year Of Release \n",
      "1               The Shawshank Redemption             9.2            (1994)\n",
      "2                          The Godfather             9.2            (1972)\n",
      "3                        The Dark Knight             9.0            (2008)\n",
      "4                 The Godfather: Part II             9.0            (1974)\n",
      "5                           12 Angry Men             9.0            (1957)\n",
      "..                                   ...             ...               ...\n",
      "96     M - Eine Stadt sucht einen Mörder             8.3            (1931)\n",
      "97                    North by Northwest             8.3            (1959)\n",
      "98                                Jagten             8.3            (2012)\n",
      "99                               Vertigo             8.3            (1958)\n",
      "100  Le fabuleux destin d'Amélie Poulain             8.3            (2001)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library as an alias bs, requests and pandas library as an alias pd \n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "  \n",
    "def top_rated_100_movies(x):                           # function definition\n",
    "    \n",
    "    src_code = requests.get(imdb_url)                  # requesting response from webpage\n",
    "    html_text=src_code.content                         # storing html content of webpage in a container html_text\n",
    "    soup=bs(html_text,\"html.parser\")                   # creating an object of BeautifulSoup to parse through the content of webpage\n",
    "\n",
    "    \n",
    "    names=[]                                           # empty list for storing names of movies\n",
    "    count1=0                                           # counter variable\n",
    "    movies=soup.find_all('td',class_=\"titleColumn\")    # fetching names of movies on the webpage and storing them in a container, movies \n",
    "    for movie in movies:                               # appending the names of the movies to the empty list names\n",
    "        count1+=1\n",
    "        if count1<=100:\n",
    "            movie=movie.a.text\n",
    "            names.append(movie)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    release=[]                                                           # empty list for storing year of release of all the movies\n",
    "    count2=0                                                             # counter variable\n",
    "    movi_release=soup.find_all('span', class_='secondaryInfo')           # fetching year of release of the movies and storing them in movi_release container\n",
    "    for mov in movi_release:                                             # appending the fecthed information from container movi_release to the empty list realease\n",
    "        count2+=1\n",
    "        if count2<=100:\n",
    "            mov=mov.text\n",
    "            release.append(mov)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "   \n",
    "    ratings=[]                                                           # empty list for storing ratings of the movies\n",
    "    count3=0                                                             # counter variable\n",
    "    movi_rating=soup.find_all('td',class_=\"ratingColumn imdbRating\")     # fetching ratings of the movies and storing them in a container movi_rating  \n",
    "    for movi in movi_rating:                                             # appending the fetched information from movi_rating container to the epty list ratings\n",
    "        count3+=1\n",
    "        if count3<=100:\n",
    "            movi=movi.text.strip()\n",
    "            ratings.append(movi)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # dictionary to store all the scraped information\n",
    "    movie_dict={' Movie_Names ':names,' Movie Ratings ':ratings,' Year Of Release ':release}  \n",
    "\n",
    "    df=pd.DataFrame(movie_dict)     # creation of dataframe df\n",
    "    df.index=range(1,len(df)+1)     # assigning indices to the dataframe\n",
    "    print(df)                       # printing the dataframe\n",
    "    \n",
    "\n",
    "imdb_url=\"https://www.imdb.com/chart/top?sort=ir,desc&mode=simple&page=1\"  # Url of the webpage\n",
    "top_rated_100_movies(imdb_url)    # url passed as a parameter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc01cb1",
   "metadata": {},
   "source": [
    "# Question 3 :- Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05d7015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Movie_Names   Movie_Ratings   Year_Of_Release \n",
      "1                       Jai Bhim             8.4            (2021)\n",
      "2                     Anbe Sivam             8.4            (2003)\n",
      "3              Pariyerum Perumal             8.4            (2018)\n",
      "4                        Golmaal             8.4            (1979)\n",
      "5                        Nayakan             8.4            (1987)\n",
      "..                           ...             ...               ...\n",
      "96    The Legend of Bhagat Singh             8.0            (2002)\n",
      "97                        Sholay             8.0            (1975)\n",
      "98                        Angoor             8.0            (1982)\n",
      "99   Baahubali 2: The Conclusion             8.0            (2017)\n",
      "100      Maheshinte Prathikaaram             8.0            (2016)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library as bs, requests library, and pandas library as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_rated_100_indian_movies(y):    # function definition\n",
    "    \n",
    "    response=requests.get(url)         # requesting response from the webpage url\n",
    "    html_code=response.content         # storing html content of webpage in a container \n",
    "    soup=bs(html_code,'html.parser')   # creating a BeautifulSoup Object to parse through the content of the webpage\n",
    "\n",
    "    names=[]                                          # empty list to store names of movies\n",
    "    count4=0                                          # counter variable\n",
    "    name=soup.find_all('td',class_=\"titleColumn\")     # fetching and storing the info related to names of movies in a container, name\n",
    "    for n in name:                                    # appending all the required names to the empty list created above as names\n",
    "        count4+=1\n",
    "        if count4<=100:\n",
    "            n=n.a.text\n",
    "            names.append(n)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    rating=[]                                                      # created an empty list to store ratings of movies\n",
    "    count5=0                                                       # counter variable\n",
    "    rates=soup.find_all('td',class_=\"ratingColumn imdbRating\")     # fetching and storing the information related to ratings of the movies in a container, rates\n",
    "    for r in rates:                                                # appending all the required ratings to the empty list, rating, created above\n",
    "        count5+=1\n",
    "        if count5<=100:\n",
    "            r=r.text.strip()\n",
    "            rating.append(r)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    release_year=[]                                                # empty list to store year of release of all the movies\n",
    "    count6=0                                                       # counter variable\n",
    "    year=soup.find_all('span',class_=\"secondaryInfo\")              # fetching and storing the information related to year of release in a container, year\n",
    "    for y in year:                                                 # appending all the years of release related to each movie to the empty list, release_year\n",
    "        count6+=1\n",
    "        if count6<=100:\n",
    "            y=y.text\n",
    "            release_year.append(y)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # creating dictionary to store all the scraped information \n",
    "    info_dict={' Movie_Names ':names,' Movie_Ratings ':rating,' Year_Of_Release ':release_year}   \n",
    "    df1=pd.DataFrame(info_dict)                                     # creating dataframe and passing dictionary as a parameter\n",
    "    df1.index=range(1,len(df1)+1)                                   # assigning indices to the dataframe\n",
    "    print(df1)                                                      # printing the dataframe\n",
    "                                                                                                                                                    \n",
    "\n",
    "url=\"https://www.imdb.com/india/top-rated-indian-movies/\"         # URL of web page\n",
    "top_rated_100_indian_movies(url)                                  # url passed as a parameter when function is called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd6bc",
   "metadata": {},
   "source": [
    "# Question 4 :- Write a python program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3e11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Product Names Product Prices  \\\n",
      "1        Elite Alluring Women Handbags           ₹300   \n",
      "2        Gorgeous Fancy Women Handbags          ₹3035   \n",
      "3        Gorgeous Fancy Women Handbags           ₹434   \n",
      "4       Voguish Stylish Women Handbags           ₹567   \n",
      "5          Elite Classy Women Handbags           ₹824   \n",
      "6   Elegant Fashionable Women Handbags           ₹303   \n",
      "7   Classic Fashionable Women Handbags           ₹656   \n",
      "8      Gorgeous Stylish Women Handbags           ₹625   \n",
      "9       Trendy Alluring Women Handbags          ₹1099   \n",
      "10                            handbags           ₹268   \n",
      "11   Trendy Fashionable Women Handbags          ₹1146   \n",
      "12         Trendy Fancy Women Handbags          ₹4949   \n",
      "13    Elegant Versatile Women Handbags           ₹615   \n",
      "14        Elegant Fancy Women Handbags           ₹761   \n",
      "15   Trendy Fashionable Women Handbags           ₹339   \n",
      "16      Trendy Women's Leather Handbag           ₹645   \n",
      "17     Classic Alluring Women Handbags           ₹145   \n",
      "18      Classic Stylish Women Handbags           ₹145   \n",
      "19  Classic Fashionable Women Handbags           ₹772   \n",
      "20     Elegant Alluring Women Handbags           ₹434   \n",
      "\n",
      "            Discounts Offered  \n",
      "1   ₹50 discount on 1st order  \n",
      "2   ₹50 discount on 1st order  \n",
      "3   ₹50 discount on 1st order  \n",
      "4   ₹50 discount on 1st order  \n",
      "5   ₹50 discount on 1st order  \n",
      "6   ₹50 discount on 1st order  \n",
      "7   ₹50 discount on 1st order  \n",
      "8   ₹50 discount on 1st order  \n",
      "9   ₹50 discount on 1st order  \n",
      "10  ₹47 discount on 1st order  \n",
      "11  ₹50 discount on 1st order  \n",
      "12  ₹50 discount on 1st order  \n",
      "13  ₹50 discount on 1st order  \n",
      "14  ₹50 discount on 1st order  \n",
      "15  ₹50 discount on 1st order  \n",
      "16  ₹50 discount on 1st order  \n",
      "17  ₹25 discount on 1st order  \n",
      "18  ₹25 discount on 1st order  \n",
      "19  ₹50 discount on 1st order  \n",
      "20  ₹50 discount on 1st order  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup and pandas libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def meesho_info(z):                         # Function definition \n",
    "    response_code=requests.get(meesho_url)  # requesting response from the webpage \n",
    "    html_content=response_code.content      # storing html content of the webpage in a container\n",
    "    soup=bs(html_content,'html.parser')     # creating a BeautifulSoup object to parse the html content with html parser\n",
    "    \n",
    "    prod_name=[]                            # empty list to store product names\n",
    "    prod_price=[]                           # empty list to store product prices\n",
    "    prod_discount=[]                        # empty list to store discounts offered\n",
    "    \n",
    "    # fetching the information from the webpage and storing it in a container, name \n",
    "    name=soup.find_all('p',class_=\"Text__StyledText-sc-oo0kvp-0 cPgaBh NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw\")\n",
    "    for n in name:                          # appending name of the each prdoduct in empty list prod_name \n",
    "        n=n.text.strip()\n",
    "        prod_name.append(n)\n",
    "    \n",
    "    price=soup.find_all('h5')               # fetching and storing the price related information from the webpage in a container, price\n",
    "    for p in price:                         # appending price of each product in empty prod_price\n",
    "        p=p.text\n",
    "        prod_price.append(p)\n",
    "    \n",
    "    # fetching and storing the information related to discounts offered on each product from the webpage in a container, discount\n",
    "    discount=soup.find_all('p',class_=\"Text__StyledText-sc-oo0kvp-0 iDRzyZ NewProductCard__DiscountTextParagraph-sc-j0e7tu-16 dppwvY NewProductCard__DiscountTextParagraph-sc-j0e7tu-16 dppwvY\")\n",
    "    for d in discount:                      # appending discount on each product in empty list prod_discount\n",
    "        d=d.text\n",
    "        prod_discount.append(d)\n",
    "    \n",
    "    # created a dictionary to store scraped information\n",
    "    Meesho_dict={'Product Names':prod_name, 'Product Prices':prod_price, 'Discounts Offered':prod_discount}\n",
    "    df2=pd.DataFrame(Meesho_dict)     # creating a dataframe and passing the dictionary in it\n",
    "    df2.index=range(1,len(df2)+1)     # assigning indices to the dataframe created\n",
    "    print(df2)                        # printing DataFrame \n",
    "    \n",
    "    \n",
    "meesho_url=\"https://meesho.com/bags-ladies/pl/p7vbp\"    # url of webpage \n",
    "meesho_info(meesho_url)                                 # passing the url as a paramter while calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9e274",
   "metadata": {},
   "source": [
    "# Question 5 :- Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e97abc",
   "metadata": {},
   "source": [
    "# 5 a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6744263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Names of the Teams Total Matches Total Points Total Ratings\n",
      "1         New Zealand            18        2,185           121\n",
      "2             England            32        3,793           119\n",
      "3           Australia            30        3,430           114\n",
      "4               India            38        4,162           110\n",
      "5        South Africa            31        3,167           102\n",
      "6            Pakistan            29        2,757            95\n",
      "7          Bangladesh            36        3,350            93\n",
      "8           Sri Lanka            35        2,835            81\n",
      "9         West Indies            36        2,788            77\n",
      "10        Afghanistan            23        1,562            68\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup and pandas libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_10_odi_men(a):                    # function definition\n",
    "     \n",
    "    resp_code=requests.get(men_url)       # requesting a response from the web page\n",
    "    html_cont=resp_code.content           # storing html content of webpage in a container\n",
    "    soup=bs(html_cont, 'html.parser')     # creating a BeautifulSoup object to parse thorugh the html content from the web page through html parser\n",
    "    \n",
    "    team_names=[]                                           # empty list to store names of teams\n",
    "    count7=0                                                # counter variable\n",
    "    teams=soup.find_all('span', class_='u-hide-phablet')    # fetching and storing information related to names in a container, team\n",
    "    for t in teams:                                         # logic to append name of each team into empty list team_names\n",
    "        count7+=1\n",
    "        while(count7<=10):\n",
    "            (team_names).append(t.text)\n",
    "            break\n",
    "    \n",
    "    team_matches=[]                                                             # empty list to store total matches played\n",
    "    first_match=soup.find('td', class_=\"rankings-block__banner--matches\").text  # fetching and storing information related to total matches of first team in first_match\n",
    "    team_matches.append(first_match)                                            # appending total matches of first team in empty list team_matches\n",
    "    rest_matches=soup.find_all('td', class_=\"table-body__cell u-center-text\")   # fetching and storing total matches of each of remaining teams into rest_matches\n",
    "    for i in range(len(rest_matches)):                                          # logic to append total matches of each of the remaining teams in the list team_matches  \n",
    "        while(i%2==0 and i<18):\n",
    "            team_matches.append(rest_matches[i].text)\n",
    "            break\n",
    "\n",
    "            \n",
    "    team_points=[]                                                                  # empty list to store total points of the teams\n",
    "    first_point=soup.find('td', class_=\"rankings-block__banner--points\").text       # fetching and storing information related to total points of first team in a container, first_point\n",
    "    team_points.append(first_point)                                                 # logic to append total points of first team into empty list team_points\n",
    "    for i in range(len(rest_matches)):                                              # logic to append total points of rest of the teams into list team_points\n",
    "        while(i%2!=0 and i<18):\n",
    "            team_points.append(rest_matches[i].text)\n",
    "            break\n",
    "\n",
    "    team_ratings=[]                                                                # empty list to store ratings of all the teams\n",
    "    count8=0                                                                       # counter variable\n",
    "    \n",
    "    # fetching and storing information related to rating of first team in a container, first_rating\n",
    "    first_rating=soup.find('td',class_=\"rankings-block__banner--rating u-text-right\").text.strip()\n",
    "    team_ratings.append(first_rating)                                                #logic to append ratings of the first team to empty list, team_ratings\n",
    "    rest_ratings=soup.find_all('td', class_=\"table-body__cell u-text-right rating\")  # fetching and storing information related to ratings of rest of the teams in a container, rest_ratings\n",
    "    for rr in rest_ratings:                                                          # logic to append ratings of each of the remaining teams in the list team ratings\n",
    "        count8+=1\n",
    "        while(count8<=9):\n",
    "            team_ratings.append(rr.text)\n",
    "            break\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    odi_dict={\"Names of the Teams\" : team_names, \"Total Matches\":team_matches, \"Total Points\": team_points, \"Total Ratings\":team_ratings}\n",
    "    df3=pd.DataFrame(odi_dict)                       # creating a dataframe and passing dictionary into it\n",
    "    df3.index=range(1,len(df3)+1)                    # assigning indices to the dataframe\n",
    "    print(df3)                                       # printing dataframe\n",
    "\n",
    "\n",
    "men_url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"    # URL of the webpage\n",
    "top_10_odi_men(men_url)                                                  # passing the url as a paramter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664f961",
   "metadata": {},
   "source": [
    "# 5 b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60bb382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name Of The Player Name Of The Team Player Rating\n",
      "1              Babar Azam              PAK           872\n",
      "2             Virat Kohli              IND           811\n",
      "3             Ross Taylor               NZ           794\n",
      "4            Rohit Sharma              IND           791\n",
      "5         Quinton de Kock               SA           789\n",
      "6          Jonny Bairstow              ENG           775\n",
      "7             Aaron Finch              AUS           771\n",
      "8   Rassie van der Dussen               SA           769\n",
      "9            David Warner              AUS           758\n",
      "10            Imam-ul-Haq              PAK           746\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_batsmen_info(b):                      # function definition\n",
    "    respo_code=requests.get(batsmen_url)      # requesting response from the web page server\n",
    "    html_txt=respo_code.content               # fetching html content of the webpage\n",
    "    soup=bs(html_txt,'html.parser')           # creating object of Beautiful Soup to parse through the html content of the webpage\n",
    "    \n",
    "    player_names=[]                           # empty list to store names of the players\n",
    "    count9=0                                  # counter variable\n",
    "    first_name=soup.find('div', class_=\"rankings-block__banner--name\").text  # fetching name of first player from the web page\n",
    "    player_names.append(first_name)                                          # appending name of first player to the empty list, player_names\n",
    "    rest_names=soup.find_all('td',class_=\"table-body__cell name\")            # fetching and storing names of rest of the players \n",
    "    for rn in rest_names:                                                    # appending names of rest of the players to the list player_names\n",
    "        count9+=1\n",
    "        while(count9<=9):\n",
    "            rn=rn.a.text\n",
    "            player_names.append(rn)\n",
    "            break\n",
    "    \n",
    "    team_names=[]                                                            # empty list to store name of the team of each player\n",
    "    count10=0                                                                # counter variable\n",
    "    first_team=soup.find('div', class_=\"rankings-block__banner--nationality\").text.strip().split()[0]   #fetching name of team of first player from web page\n",
    "    team_names.append(first_team)                                                                       # appending name of team of first player to the empty list team_names\n",
    "    rest_teams=soup.find_all('span', class_=\"table-body__logo-text\")                                    # fetching names of teams of rest of the players from webpage\n",
    "    for rt in rest_teams:                                                                               # appending names of the teams of rest of the players in the list team_names\n",
    "        count10+=1\n",
    "        while(count10<=9):\n",
    "            rt=rt.text\n",
    "            team_names.append(rt)\n",
    "            break\n",
    "        \n",
    "    \n",
    "    player_rating=[]                                                                # empty list to store ratings of each player\n",
    "    count11=0                                                                       # counter variable\n",
    "    first_rating=soup.find('div', class_=\"rankings-block__banner--rating\").text     # fetching ratings of first player from webpage\n",
    "    player_rating.append(first_rating)                                              # appending ratings of first player to the empty list player_rating\n",
    "    rest_ratings=soup.find_all('td', class_=\"table-body__cell u-text-right rating\") # fetching ratings of rest of the players from webpage\n",
    "    for rr in rest_ratings:                                                         # appending ratings of rest of the players to the list player_rating\n",
    "        count11+=1\n",
    "        while(count11<=9):\n",
    "            rr=rr.text\n",
    "            player_rating.append(rr)\n",
    "            break\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data from the webpage\n",
    "    player_dict={\"Name Of The Player\":player_names,\"Name Of The Team\":team_names,\"Player Rating\":player_rating}\n",
    "    \n",
    "    df4=pd.DataFrame(player_dict)     # creating a dataframe and passing dictionary in it\n",
    "    df4.index=range(1,len(df4)+1)     # assigning indices to the dataframe\n",
    "    print(df4)                        # printing dataframe\n",
    "    \n",
    "    \n",
    "batsmen_url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"    # url of the webpage\n",
    "top_batsmen_info(batsmen_url)                                                  # passing url as a paramter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ad1cd",
   "metadata": {},
   "source": [
    "# 5 c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b56eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bowler's Name Bowler's Team Bowler's Ratings\n",
      "1        Trent Boult            NZ              733\n",
      "2     Josh Hazlewood           AUS              705\n",
      "3       Chris Woakes           ENG              700\n",
      "4         Matt Henry            NZ              687\n",
      "5   Mujeeb Ur Rahman           AFG              681\n",
      "6     Jasprit Bumrah           IND              679\n",
      "7       Mehedi Hasan           BAN              661\n",
      "8    Shakib Al Hasan           BAN              657\n",
      "9         Adam Zampa           AUS              650\n",
      "10       Rashid Khan           AFG              650\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_10_bowlers_odi(c):                                   # function definition\n",
    "    source_code=requests.get(bowler_url)                     # requesting response from the webpage server\n",
    "    html_code_text=source_code.content                       # fetching html content from the webpage\n",
    "    soup=bs(html_code_text,\"html.parser\")                    # creating a BeautifulSoup object to parse thorugh the html content of the webpage\n",
    "    \n",
    "    bowler_names=[]                                                              # empty list to store names of bowlers\n",
    "    count12=0                                                                    # counter variable\n",
    "    first_bowler=soup.find('div',class_=\"rankings-block__banner--name-large\")    # fetching name of first bowler from the webpage\n",
    "    bowler_names.append(first_bowler.text)                                       # appending name of first bowler to the empty list bowler_names\n",
    "    rest_bowlers=soup.find_all('td',class_=\"table-body__cell rankings-table__name name\")  # fetching names of rest of the bowlers from the webpage\n",
    "    for rb in rest_bowlers:                                                               # appending names of rest of the bowlers to the list bowler_names\n",
    "        count12+=1\n",
    "        while(count12<=9):\n",
    "            rb=rb.a.text\n",
    "            bowler_names.append(rb)\n",
    "            break\n",
    "    \n",
    "    bowler_teams=[]                                                            # empty list to store name of the team of each bowler\n",
    "    count13=0                                                                  # counter variable\n",
    "    first_team=soup.find('div',class_=\"rankings-block__banner--nationality\").text.strip()  # fetching team of first bowler from the webpage\n",
    "    bowler_teams.append(first_team)                                                        # appending team of first bowler to the empty list, bowler_teams\n",
    "    other_teams=soup.find_all('span', class_=\"table-body__logo-text\")                      # fetching teams of rest of the bowlers from the webpage\n",
    "    for ot in other_teams:                                                                 # appending teams of rest of the bowlers to the list, bowler_teams\n",
    "        count13+=1\n",
    "        while(count13<=9):\n",
    "            ot=ot.text\n",
    "            bowler_teams.append(ot)\n",
    "            break\n",
    "    \n",
    "    bowler_ratings=[]                                                            # empty list to store ratings of each bowler\n",
    "    count14=0                                                                    # counter variable\n",
    "    bowler_rating_1=soup.find('div',class_=\"rankings-block__banner--rating\")     # fetching ratings of first bowler from the webpage\n",
    "    bowler_ratings.append(bowler_rating_1.text)                                  # appending ratings of the first bowler to the empty list, bowler_ratings\n",
    "    rest_bowler_ratings=soup.find_all('td',class_=\"table-body__cell rating\")     # fetching ratikngs of rest of the bowlers from the webpage\n",
    "    for rbr in rest_bowler_ratings:                                              # appending ratings of the rest of the bowlers to the list bowler_ratings\n",
    "        count14+=1\n",
    "        while(count14<=9):\n",
    "            rbr=rbr.text\n",
    "            bowler_ratings.append(rbr)\n",
    "            break\n",
    "    # creating a dictionary to store all the scraped information in it\n",
    "    bowler_dict={\"Bowler's Name\": bowler_names, \"Bowler's Team\": bowler_teams, \"Bowler's Ratings\": bowler_ratings}\n",
    "    df5=pd.DataFrame(bowler_dict)              # creating a dataframe and passing dictonary in it   \n",
    "    df5.index=range(1,len(df5)+1)              # assigning indices to the dataframe\n",
    "    print(df5)                                 # printing the dataframe\n",
    "    \n",
    "bowler_url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"    # url of the webpage\n",
    "top_10_bowlers_odi(bowler_url)                                                        # passing url as the parameter when the function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a084aef",
   "metadata": {},
   "source": [
    "# Question 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3830a2",
   "metadata": {},
   "source": [
    "# 6 a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ddcb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Of The Team   Total Matches   Total Points   Total Ratings\n",
      "1         Australia              28           4663             167\n",
      "2      South Africa              28           3504             125\n",
      "3           England              29           3425             118\n",
      "4             India              29           2890             100\n",
      "5       New Zealand              31           3018              97\n",
      "6       West Indies              28           2478              89\n",
      "7        Bangladesh              12            935              78\n",
      "8          Pakistan              26           1753              67\n",
      "9           Ireland               5            240              48\n",
      "10        Sri Lanka               5            233              47\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library \n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def women_odi_teams(w):                                        # function definition\n",
    "    source_text=requests.get(women_team_url).content           # requesting html content from the web page\n",
    "    soup=bs(source_text,\"html.parser\")                         # creating a BeautifulSoup object to parse through the html content of the webpage\n",
    "    \n",
    "    women_teams=[]                                             # empty list to store names of the teams\n",
    "    counter=0                                                  # counter variable\n",
    "    \n",
    "    rest_women_teams=soup.find_all('span', class_=\"u-hide-phablet\")               # fetching names of all the teams from webpage\n",
    "    for rwt in rest_women_teams:                                                  # appending names of all the teams to the empty list women_teams\n",
    "        counter+=1\n",
    "        while(counter<=10):\n",
    "            women_teams.append(rwt.text)\n",
    "            break\n",
    "            \n",
    "            \n",
    "    women_matches=[]                                                               # empty list to store total matches played by each team\n",
    "        \n",
    "    first_women_match=soup.find('td', class_=\"rankings-block__banner--matches\")    # fetching total matches of first team from the webpage\n",
    "    women_matches.append(first_women_match.text)                                   # appending total matches of first team to the empty list,women_matches\n",
    "    rest_women_matches=soup.find_all('td',class_=\"table-body__cell u-center-text\") # fetching total matches of rest of the teams from webpage\n",
    "    for i in range(len(rest_women_matches)):                                       # appending total matches of rest of the teams to the list,women_matches\n",
    "        while(i%2==0 and i<18):\n",
    "                    women_matches.append(rest_women_matches[i].text)\n",
    "                    break\n",
    "    \n",
    "    women_points=[]                                                                 # emplty list to store total points of each team\n",
    "    \n",
    "    first_women_points=soup.find('td',class_=\"rankings-block__banner--points\")      # fetching total points of first team from webpage\n",
    "    women_points.append(first_women_points.text.replace(',',''))                    # appending total points of first team to the empty list, women_points\n",
    "    rest_women_points=soup.find_all('td',class_=\"table-body__cell u-center-text\")   # fetching total points of rest of the teams from the webpage\n",
    "    for i in range(len(rest_women_points)):                                         # appending total points of rest of the teams to the list, women_points\n",
    "        while(i%2!=0 and i<18):\n",
    "                    women_points.append(rest_women_points[i].text.replace(',',''))\n",
    "                    break\n",
    "        \n",
    "    \n",
    "    women_ratings=[]                                                                # empty list to store ratings of each team\n",
    "    counter1=0                                                                      # counter variable\n",
    "    \n",
    "    first_women_rating=soup.find('td', class_=\"rankings-block__banner--rating u-text-right\") # fetching ratings of the first team fromt the webpage\n",
    "    women_ratings.append(first_women_rating.text.strip())                                    # appending ratings of the first team tot he empty list women_ratings\n",
    "    rest_women_ratings=soup.find_all('td',class_=\"table-body__cell u-text-right rating\")     # fetching ratings of rest of the teams from the webpage\n",
    "    for rwr in rest_women_ratings:                                                           # appending ratings of rest of the teams to the list, women_ratings\n",
    "        counter1+=1\n",
    "        while(counter1<10):\n",
    "            women_ratings.append(rwr.text)\n",
    "            break\n",
    "    \n",
    "    # creating a dictionary to store all the scraped information in it\n",
    "    women_team_dict={\"Name Of The Team\":women_teams,\"  Total Matches\":women_matches,\"  Total Points\":women_points,\"  Total Ratings\":women_ratings}\n",
    "    df6=pd.DataFrame(women_team_dict)                          # creating a dataframe and passing dictionary in it\n",
    "    df6.index=range(1,len(df6)+1)                              # assigning indices to the dataframe\n",
    "    print(df6)                                                 # printing dataframe\n",
    "    \n",
    "    \n",
    "women_team_url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\" # url of the webpage\n",
    "women_odi_teams(women_team_url)                                                # passing url of the webpage as a parameter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c811798",
   "metadata": {},
   "source": [
    "# 6 b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c341e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Player's Name   Player's Team   Player's Rating\n",
      "1     Laura Wolvaardt              SA               740\n",
      "2         Beth Mooney             AUS               726\n",
      "3         Meg Lanning             AUS               718\n",
      "4      Natalie Sciver             ENG               705\n",
      "5        Alyssa Healy             AUS               703\n",
      "6         Mithali Raj             IND               686\n",
      "7      Rachael Haynes             AUS               684\n",
      "8      Tammy Beaumont             ENG               682\n",
      "9   Amy Satterthwaite              NZ               681\n",
      "10    Smriti Mandhana             IND               669\n"
     ]
    }
   ],
   "source": [
    "# importing request, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_women_batsmen_info(wb):                                   # function definition\n",
    "    source=requests.get(women_batsmen_url)                        # requesting response code from the webpage server\n",
    "    html_content=source.content                                   # storing html content from the webpage in a container  \n",
    "    soup=bs(html_content,'html.parser')                           # creating BeautifulSoup object to parse through the html content of the webpage\n",
    "    \n",
    "    women_batsmen_names=[]                                        # emplty list to store name of each batsman\n",
    "    counter2=0                                                    # counter variable\n",
    "    first_women_name=soup.find('div', class_=\"rankings-block__banner--name\").text    # fetching name of first batsman\n",
    "    women_batsmen_names.append(first_women_name)                                     # appending name of the first batsan to the list women_batsmen_names\n",
    "    rest_of_names=soup.find_all('td',class_=\"table-body__cell name\")                 # fetching names of the rest 9 players\n",
    "    for rn in rest_of_names:                                                         # appending names of rest 9 players to list women_batsmen_names\n",
    "        counter2+=1\n",
    "        while(counter2<=9):\n",
    "            rn=rn.a.text\n",
    "            women_batsmen_names.append(rn)\n",
    "            break\n",
    "     \n",
    "    women_team_names=[]                                                        # empty list to store name of the team of each batsman\n",
    "    counter3=0                                                                 # counter variable\n",
    "    one_team=soup.find('div', class_=\"rankings-block__banner--nationality\").text.strip().split()[0]    # fetching team related to first batsman\n",
    "    women_team_names.append(one_team)                                                                  # appending team of first batsman to the list women_team_names\n",
    "    rest_of_team=soup.find_all('span', class_=\"table-body__logo-text\")                                 # fetching name of the team related to rest 9 batsmen\n",
    "    for rt in rest_of_team:                                                                            # appending names of the teams related to rest 9 batsmen\n",
    "        counter3+=1\n",
    "        while(counter3<=9):\n",
    "            rt=rt.text.strip().split()[0]\n",
    "            women_team_names.append(rt)\n",
    "            break\n",
    "        \n",
    "    \n",
    "\n",
    "    women_batsmen_rating=[]                                                               # empty list to store rating of each batsman\n",
    "    counter4=0                                                                            # counter variable\n",
    "    first_rating=soup.find('div', class_=\"rankings-block__banner--rating\").text           # fetching rating of first batsman\n",
    "    women_batsmen_rating.append(first_rating)                                             # appending rating of the first batsman to list women_batsmen_ratings\n",
    "    rest_of_ratings=soup.find_all('td', class_=\"table-body__cell u-text-right rating\")    # fetching ratings of the rest 9 batsmen\n",
    "    for rr in rest_of_ratings:                                                            # appending ratings of the rest 9 batsmen to list women_batsmen_ratings\n",
    "        counter4+=1\n",
    "        while(counter4<=9):\n",
    "            rr=rr.text\n",
    "            women_batsmen_rating.append(rr)\n",
    "            break\n",
    "    \n",
    "    # creating a dictionary to store all the scraped information in it\n",
    "    women_batsmen_dict={\"Player's Name\":women_batsmen_names,\"  Player's Team\":women_team_names,\"  Player's Rating\":women_batsmen_rating}\n",
    "    \n",
    "    df7=pd.DataFrame(women_batsmen_dict)               # creating a dataframe and passing dictionary in it\n",
    "    df7.index=range(1,len(df7)+1)                      # assigning indices to the dataframe\n",
    "    print(df7)                                         # printing dataframe\n",
    "    \n",
    "    \n",
    "women_batsmen_url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"     # url of the webpage\n",
    "top_women_batsmen_info(women_batsmen_url)                                               # passing url as a paramter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba04ce",
   "metadata": {},
   "source": [
    "# 6 c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "472592cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AllRounder's Name   AllRounder's Team   AllRounder's Ratings\n",
      "1       Ellyse Perry                 AUS                    404\n",
      "2     Natalie Sciver                 ENG                    376\n",
      "3     Marizanne Kapp                  SA                    359\n",
      "4    Hayley Matthews                  WI                    340\n",
      "5        Amelia Kerr                  NZ                    335\n",
      "6   Ashleigh Gardner                 AUS                    278\n",
      "7      Deepti Sharma                 IND                    249\n",
      "8      Jess Jonassen                 AUS                    246\n",
      "9    Katherine Brunt                 ENG                    239\n",
      "10    Jhulan Goswami                 IND                    217\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def women_all_rounder_info(wa):                                          # function definition\n",
    "    src_code=requests.get(allrounder_url).content                        # requesting content from the webpage\n",
    "    soup=bs(src_code,'html.parser')                              # creating a BeautifulSoup object to parse through the content of the webpage\n",
    "    \n",
    "    allrounder_names=[]                                                                      # empty list to store names of each allrounder\n",
    "    counter5=0                                                                               # counter variable\n",
    "    first_ar_name=soup.find('div',class_=\"rankings-block__banner--name-large\")               # fetching name of first allrounder\n",
    "    allrounder_names.append(first_ar_name.text)                                              # appending name of the first allrounder to list allrounder_names\n",
    "    rest_ar_names=soup.find_all('td', class_=\"table-body__cell rankings-table__name name\")   # fetching names of rest of the 9 allrounders\n",
    "    for ran in rest_ar_names:                                                                # appending names of rest of the 9 allrounders to the list allrounder_names\n",
    "        counter5+=1\n",
    "        while(counter5<=9):\n",
    "            allrounder_names.append(ran.a.text)\n",
    "            break\n",
    "    \n",
    "    allrounder_teams=[]                                                                         # empty list to store team of each allrounder\n",
    "    counter6=0                                                                                  # counter variable\n",
    "    first_ar_team=soup.find('div',class_=\"rankings-block__banner--nationality\").text.strip()    # fetching team of first allrounder\n",
    "    allrounder_teams.append(first_ar_team)                                                      # appending team of first allrounder\n",
    "    rest_ar_teams=soup.find_all('span',class_=\"table-body__logo-text\")                          # fetching teams related to rest 9 of the allrounders\n",
    "    for rat in rest_ar_teams:                                                                   # appending teams related to rest 9 of the allrounders to the list allrounder_teams\n",
    "        counter6+=1\n",
    "        while(counter6<=9):\n",
    "            allrounder_teams.append(rat.text)\n",
    "            break\n",
    "    \n",
    "    allrounder_ratings=[]                                                      # empty list to store ratings of each allrounder\n",
    "    counter7=0                                                                 # counter variable\n",
    "    first_ar_rating=soup.find('div',class_=\"rankings-block__banner--rating\")   # fetching rating related to the first allrounder\n",
    "    allrounder_ratings.append(first_ar_rating.text)                            # appending rating related to first allrounder to the list all_rounder_ratings\n",
    "    rest_ar_ratings=soup.find_all('td',class_=\"table-body__cell rating\")       # fetching ratings related to rest of the allrounders\n",
    "    for rar in rest_ar_ratings:                                                # appending ratings related to rest 9 of the allrounders to the list allrounder_ratings\n",
    "        counter7+=1\n",
    "        while(counter7<=9):\n",
    "            allrounder_ratings.append(rar.text)\n",
    "            break\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    allrounder_dict={\"AllRounder's Name\":allrounder_names,\"  AllRounder's Team\":allrounder_teams,\"  AllRounder's Ratings\":allrounder_ratings}\n",
    "    df8=pd.DataFrame(allrounder_dict)                                          # creating a dataframe and passing dictionary in it\n",
    "    df8.index=range(1,len(df8)+1)                                              # assigning indices to the dataframe\n",
    "    print(df8)                                                                 # printing dataframe\n",
    "    \n",
    "allrounder_url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"   # url of the webpage\n",
    "women_all_rounder_info(allrounder_url)                                                         # passing url of webpage as a parameter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ffee6",
   "metadata": {},
   "source": [
    "# Question 7 :- Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content, and the code for the video from the link for the youtube video from the post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2b15064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Headings On Website    Date Of Publish  \\\n",
      "1   Python Tutorial: Zip Files – Creating and Extr...   November 19 2019   \n",
      "2   Python Data Science Tutorial: Analyzing the 20...    October 17 2019   \n",
      "3   Python Multiprocessing Tutorial: Run Code in P...  September 21 2019   \n",
      "4   Python Threading Tutorial: Run Code Concurrent...  September 12 2019   \n",
      "5                                 Update (2019-09-03)   September 3 2019   \n",
      "6   Python Quick Tip: The Difference Between “==” ...      August 6 2019   \n",
      "7   Python Tutorial: Calling External Commands Usi...       July 24 2019   \n",
      "8   Visual Studio Code (Windows) – Setting up a Py...         May 1 2019   \n",
      "9   Visual Studio Code (Mac) – Setting up a Python...         May 1 2019   \n",
      "10  Clarifying the Issues with Mutable Default Arg...      April 24 2019   \n",
      "\n",
      "                                 Theoritical Contents  \\\n",
      "1   In this video, we will be learning how to crea...   \n",
      "2   In this Python Programming video, we will be l...   \n",
      "3   In this Python Programming video, we will be l...   \n",
      "4   In this Python Programming video, we will be l...   \n",
      "5   Hey everyone. I wanted to give you an update o...   \n",
      "6   In this Python Programming Tutorial, we will b...   \n",
      "7   In this Python Programming Tutorial, we will b...   \n",
      "8   In this Python Programming Tutorial, we will b...   \n",
      "9   In this Python Programming Tutorial, we will b...   \n",
      "10  In this Python Programming Tutorial, we will b...   \n",
      "\n",
      "                                Video Links  \n",
      "1   https://youtube.com/watch?v=z0gguhEmWiY  \n",
      "2   https://youtube.com/watch?v=_P7X8tMplsw  \n",
      "3   https://youtube.com/watch?v=fKl2JW_qrso  \n",
      "4   https://youtube.com/watch?v=IEEhzQoKtQU  \n",
      "5   https://youtube.com/watch?v=mO_dS3rXDIs  \n",
      "6   https://youtube.com/watch?v=2Fp1N6dof0Y  \n",
      "7   https://youtube.com/watch?v=-nh9rCzPJ20  \n",
      "8   https://youtube.com/watch?v=06I63_p-2A4  \n",
      "9   https://youtube.com/watch?v=_JGmemuINww  \n",
      "10                 Video Link Not Available  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Website_content(wc):                                             # function definition\n",
    "    src_code=requests.get(web_url).content                           # requesting content from the webpage and storing it in a variable src_code\n",
    "    soup=bs(src_code,'html.parser')                               # creating a BeautifulSoup object to parse through the webpage content\n",
    "    \n",
    "    web_headings=[]                                                  # empty list to store headings on the webpage\n",
    "    headings=soup.find_all('a',class_=\"entry-title-link\")            # fetching data from the webpage\n",
    "    for h in headings:                                               # logic to append desired data to the list web_headings\n",
    "        web_headings.append(h.text.strip())\n",
    "\n",
    "    web_dates=[]                                                      # empty list to store dates on which the respective article was published\n",
    "    dates=soup.find_all('time',class_=\"entry-time\")                   # fetching dates related to each article from the webpage\n",
    "    for d in dates:                                                   # logic to append dates related to each article to the list web_dates\n",
    "        web_dates.append(d.text.replace(',',''))\n",
    "\n",
    "    web_contents=[]                                                    # empty list to store theoritical content from the webpage\n",
    "    contents=soup.find_all('div',class_=\"entry-content\")               # fetching content related to each heading from the webpage\n",
    "    for c in contents:                                                 # logic to append content related to each heading to the list web_contents\n",
    "        web_contents.append(c.p.text)\n",
    "\n",
    "    \n",
    "    web_video_links=[]                                                 # empty list to store youtube video links on webpage \n",
    "    video_links=soup.find_all('iframe',class_=\"youtube-player\")        # fetching content related to video from the webpage\n",
    "    for vl in video_links:                                             \n",
    "        video_ids=vl['src'].split('/')[4].split('?')[0]                    # logic to fetch id of video from the web page\n",
    "        yt_video_links=\"https://youtube.com/watch?v={}\".format(video_ids)   # creating a video link using id of each video\n",
    "        web_video_links.append(yt_video_links)                              # logic to append video links to the list web_video_links\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data from the webpage      \n",
    "    website_info_dict={\"Headings On Website\":web_headings, \"Date Of Publish\":web_dates, \"Theoritical Contents\":web_contents, \"Video Links\":web_video_links}\n",
    "    final_info_dict=dict([ (k,pd.Series(v)) for k,v in website_info_dict.items() ]) # creating a new dictionary from website_info_dict\n",
    "    df9=pd.DataFrame(final_info_dict)                                          # creating a dataframe and passing dictionary in it\n",
    "    df9[\"Video Links\"]=df9[\"Video Links\"].fillna(\"Video Link Not Available\")   # logic to append a message \"video link not available\" into the dataframe where there is no value in \"Video Links\"\n",
    "    df9.index=range(1,len(df9)+1)                                              # assigning indices to the dataframe\n",
    "    print(df9)                                                                 # print dataframe\n",
    "\n",
    "web_url=\"https://coreyms.com/\"         # url of the webpage\n",
    "Website_content(web_url)               # passing url as a parameter when the function is called \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc5f67",
   "metadata": {},
   "source": [
    "# Question 8 :- Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a303bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Name Of The House  \\\n",
      "1   2 BHK Flat  For Sale  In Kumar Ashraya Apartme...   \n",
      "2   3 BHK Apartment  For Sale  In Brigade Gateway ...   \n",
      "3   2 BHK Apartment  For Sale  In Tarang Parkway A...   \n",
      "4   4+ BHK In Independent House  For Sale  In Raja...   \n",
      "5   4+ BHK In Independent House  For Sale  In Jaya...   \n",
      "6   2 BHK Apartment  For Sale  In Blueberry Apartm...   \n",
      "7   3 BHK Apartment  For Sale  In Mayitri Enclave ...   \n",
      "8   2 BHK Flat  For Sale  In Jains Prakruti, Jayan...   \n",
      "9   4 BHK In Independent House  For Sale  In Jayan...   \n",
      "10  3 BHK Flat  For Sale  In Benaka Apartments In ...   \n",
      "11  4+ BHK In Independent House  For Sale  In Prak...   \n",
      "12  2 BHK Apartment  For Sale  In Jains Prakriti I...   \n",
      "13  2 BHK Flat  For Sale  In Jayanagar Residency I...   \n",
      "14  4 BHK Flat  For Sale  In Rrbc Piccassso In Jay...   \n",
      "15  4 BHK Flat  For Sale  In Phoenix  One Bangalor...   \n",
      "16  4+ BHK In Independent House  For Sale  In Laks...   \n",
      "17              3 BHK Flat  For Sale  In Rajaji Nagar   \n",
      "18  3 BHK Flat  For Sale  In  Sgrr Pallavi Pristin...   \n",
      "19  3 BHK Flat  For Sale  In Hal 2nd Stage,indiran...   \n",
      "20  3 BHK Apartment  For Sale  In Ashiana Gardens ...   \n",
      "21  4 BHK Flat  For Sale  In Golfridge In Indira N...   \n",
      "22  4+ BHK In Independent House  For Sale  In Raja...   \n",
      "23  4 BHK In Independent House  For Sale  In Jayan...   \n",
      "24  2 BHK Flat  For Sale  In Aishwaryam Gokula In ...   \n",
      "25  3 BHK Apartment  For Sale  In Admiralty Manor,...   \n",
      "\n",
      "                                Location of the House Area of the House  \\\n",
      "1    Kumar Ashraya Apartments 194, 9th Cross Rd 2n...          900 sqft   \n",
      "2   Brigade Gateway, Dr Rajkumar Road, Rajaji Naga...        1,640 sqft   \n",
      "3   Tarang Parkway Apartment, 2nd Main Rd, Shivana...        1,200 sqft   \n",
      "4   Independent House,  6th Cross road,9th main ro...        1,320 sqft   \n",
      "5   Independent House, TMC Layout behind rajalaxmi...        2,800 sqft   \n",
      "6   Blueberry Apartment, 13th E Main Rd, Channakes...        1,074 sqft   \n",
      "7   Mayitri Enclave, Mayitri Enclave, 39th C, 5T B...        1,450 sqft   \n",
      "8   Kanakapura Road, Jayanagar, Bangalore, Karnata...        1,301 sqft   \n",
      "9   Independent House, SBI Branch Jayanagar 9th bl...        2,600 sqft   \n",
      "10  871, 5th Cross Rd, Indira Nagar 1st Stage, Sta...        1,331 sqft   \n",
      "11  Independent House, 6th C cross 3rd main rd nea...        2,200 sqft   \n",
      "12  Jains Prakriti,  Kanakapura Rd, 7th Block West...        1,298 sqft   \n",
      "13  Jayanagar Residency, K V Layout, LIC Colony, J...        1,200 sqft   \n",
      "14  Municipal No.152, 18th main road, beside Sai B...        4,859 sqft   \n",
      "15  Dr Rajkumar Rd, opposite Sheraton Hotel, Rajaj...        2,901 sqft   \n",
      "16        Independent House, CMH Rd , Near B P Bakery        2,250 sqft   \n",
      "17  Independent House, 242, 27th cross, 2nd Block,...        1,275 sqft   \n",
      "18  11th Main Road,36 Cross, 4th T Block, Near ,Po...        1,340 sqft   \n",
      "19  Standalone building, 16th F main, Hal 2nd stag...        2,200 sqft   \n",
      "20  Ashiana Gardens Apartments, 4, Sri Rama Temple...        1,780 sqft   \n",
      "21                             embassy golf link road        2,750 sqft   \n",
      "22  Independent House, 6th Cross Rd, 2nd Block  ne...        2,240 sqft   \n",
      "23  Independent House, 47th Cross Rd, 8th Block, 1...        3,700 sqft   \n",
      "24            1st main govindraja naagar blore 560040        1,209 sqft   \n",
      "25  Admiralty Manor, Indiranagar, Admiralty Manor,...        1,960 sqft   \n",
      "\n",
      "    EMI of the House Price of the House  \n",
      "1      ₹37,827/Month           ₹66 Lacs  \n",
      "2    ₹1.5 Lacs/Month       ₹2.62 Crores  \n",
      "3      ₹57,314/Month           ₹1 Crore  \n",
      "4      ₹63,045/Month        ₹1.1 Crores  \n",
      "5   ₹2.21 Lacs/Month       ₹3.85 Crores  \n",
      "6      ₹87,691/Month       ₹1.53 Crores  \n",
      "7      ₹44,705/Month           ₹78 Lacs  \n",
      "8      ₹85,971/Month        ₹1.5 Crores  \n",
      "9   ₹1.55 Lacs/Month        ₹2.7 Crores  \n",
      "10     ₹48,717/Month           ₹85 Lacs  \n",
      "11     ₹77,374/Month       ₹1.35 Crores  \n",
      "12  ₹1.15 Lacs/Month          ₹2 Crores  \n",
      "13     ₹57,314/Month           ₹1 Crore  \n",
      "14  ₹3.91 Lacs/Month       ₹6.83 Crores  \n",
      "15  ₹2.72 Lacs/Month       ₹4.75 Crores  \n",
      "16     ₹63,045/Month        ₹1.1 Crores  \n",
      "17     ₹77,374/Month       ₹1.35 Crores  \n",
      "18     ₹74,508/Month        ₹1.3 Crores  \n",
      "19  ₹2.12 Lacs/Month        ₹3.7 Crores  \n",
      "20     ₹80,240/Month        ₹1.4 Crores  \n",
      "21  ₹1.63 Lacs/Month       ₹2.85 Crores  \n",
      "22     ₹83,679/Month       ₹1.46 Crores  \n",
      "23  ₹5.44 Lacs/Month        ₹9.5 Crores  \n",
      "24     ₹44,705/Month           ₹78 Lacs  \n",
      "25     ₹91,703/Month        ₹1.6 Crores  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def House_details(hd):                               # function definition\n",
    "    src_code=requests.get(passed_url).content        # requesting html content of the webpage and storing in a variable,src_code\n",
    "    soup=bs(src_code,\"html.parser\")                  # creating object of BeautifulSoup to parse through the src_code\n",
    "    articles=soup.find_all('article')\n",
    "    h_loc=[]                                         # empty list to store location of the houses\n",
    "    h_name=[]                                        # empty list to store name of the houses\n",
    "    h_area=[]                                        # empty list to store area of houses\n",
    "    h_emi=[]                                         # empty list to store value of EMI of the houses\n",
    "    h_price=[]                                       # empty list to store prices of the houses\n",
    "    \n",
    "    # logics to append the required info in the respective empty container lists above\n",
    "    for a in articles:      \n",
    "        h_name.append(a.find('h2', class_='heading-6 flex items-center font-semi-bold m-0').text)\n",
    "        h_area.append(a.find('div',class_=\"font-semi-bold heading-6\").text)\n",
    "        h_loc.append(a.find('div', class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\").text.replace('\\xa0',','))\n",
    "        h_emi.append(a.find('div',class_=\"font-semi-bold heading-6\",id=\"roomType\").text)\n",
    "        h_price.append(a.find('div',class_=\"flex flex-col w-33pe items-center bo tp:w-half po:w-full border-r-0\",id=\"minDeposit\").span.text)\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    House_dict={\"Name Of The House\":h_name, \"Location of the House\":h_loc, \"Area of the House\": h_area, \"EMI of the House\":h_emi, \"Price of the House\":h_price}\n",
    "    df12=pd.DataFrame(House_dict)                 # Creating a dataframe and passing dictionary into it\n",
    "    df12.index=range(1,len(df12)+1)               # assigning indices to the dataframe\n",
    "    print(df12)                                   # print(dataframe)\n",
    "\n",
    "                                                  \n",
    "# Url of the web page\n",
    "passed_url=\"https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIifSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIn0seyJsYXQiOjEyLjk3ODM2OTIsImxvbiI6NzcuNjQwODM1NiwicGxhY2VJZCI6IkNoSUprUU4zR0tRV3Jqc1JOaEJRSnJoR0Q3VSIsInBsYWNlTmFtZSI6IkluZGlyYW5hZ2FyIn1d&radius=2.0&city=bangalore&locality=Rajajinagar,&locality=Jayanagar,&locality=Indiranagar\"\n",
    "House_details(passed_url)                         # passing url of the web page as a parameter when the function is called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9524",
   "metadata": {},
   "source": [
    "# Question 9 :- Write a python program to scrape mentioned details from https://www.dineout.co.in/delhi-restaurants/buffet-special  : i) Restaurant name, ii) Cuisine, iii) Location, iv) Ratings, v) Image URL\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d5aed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name Of Restaurant  \\\n",
      "1                    Castle Barbeque   \n",
      "2                    Jungle Jamboree   \n",
      "3                    Castle Barbeque   \n",
      "4                         Cafe Knosh   \n",
      "5               The Barbeque Company   \n",
      "6                        India Grill   \n",
      "7                     Delhi Barbeque   \n",
      "8   The Monarch - Bar Be Que Village   \n",
      "9                         World Cafe   \n",
      "10                 Indian Grill Room   \n",
      "11                   Mad 4 Bar B Que   \n",
      "12                       Barbeque 29   \n",
      "13                        Glasshouse   \n",
      "\n",
      "                                  Cuisine Names  \\\n",
      "1                         North Indian, Chinese   \n",
      "2                  North Indian, Asian, Italian   \n",
      "3                         Chinese, North Indian   \n",
      "4                          Italian, Continental   \n",
      "5                         North Indian, Chinese   \n",
      "6                         North Indian, Italian   \n",
      "7                                  North Indian   \n",
      "8                         North Indian, Chinese   \n",
      "9            North Indian, Chinese, Continental   \n",
      "10                        North Indian, Mughlai   \n",
      "11                                 North Indian   \n",
      "12   North Indian, Mughlai, Desserts, Beverages   \n",
      "13        European, Italian, Asian, Continental   \n",
      "\n",
      "                           Location Of the Restaurant Ratings  \\\n",
      "1                      Connaught Place  Central Delhi     3.5   \n",
      "2              3CS Mall Lajpat Nagar - 3  South Delhi     3.9   \n",
      "3              Pacific Mall Tagore Garden  West Delhi     3.9   \n",
      "4   The Leela Ambience Convention Hotel Shahdara  ...     4.3   \n",
      "5                  Gardens Galleria Sector 38A  Noida       4   \n",
      "6                Hilton Garden Inn Saket  South Delhi     3.9   \n",
      "7      Taurus Sarovar Portico Mahipalpur  South Delhi     3.7   \n",
      "8   Indirapuram Habitat Centre Indirapuram  Ghaziabad     3.9   \n",
      "9    Vibe by The Lalit Traveller Sector 35  Faridabad     4.2   \n",
      "10   Suncity Business Tower Golf Course Road  Gurgaon     4.3   \n",
      "11                               Sector 29  Faridabad     3.6   \n",
      "12                                     NIT  Faridabad     4.2   \n",
      "13  DoubleTree By Hilton Gurugram Baani Square Sec...       4   \n",
      "\n",
      "                                       URLs of Images  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "# importing requests library, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def restaurant_info(ri):                                 # function definition\n",
    "    src_code=requests.get(rest_url).content              # storing webpage content on a variable, src_code\n",
    "    soup=bs(src_code,\"html.parser\")                      # creating object of BeautifulSoup to parse through the webpage content\n",
    "    \n",
    "    restaurant_names=[]                                            # empty list to store names of the restaurants\n",
    "    rest_names=soup.find_all('div',class_=\"restnt-info cursor\")    # fetching data from webpage\n",
    "    for rn in rest_names:                                          # appending desired data in the list restaurant_names\n",
    "        restaurant_names.append(rn.a.text)\n",
    "        \n",
    "    locations=[]                                                   # empty list to store locations of the restaurants\n",
    "    loc=soup.find_all('div',class_=\"restnt-loc ellipsis\")          # fetching data from webpage\n",
    "    for l in loc:                                                  # appending desired data in the list locations\n",
    "        l=l.text.replace(',',' ')\n",
    "        locations.append(l)\n",
    "        \n",
    "    ratings=[]                                                      # empty list to store ratings of the restaurants\n",
    "    r=soup.find_all('div',class_=\"restnt-rating rating-4\")          # fetching data from webpage\n",
    "    for i in r:                                                     # appending desired data in the list ratings\n",
    "        i=i.text\n",
    "        ratings.append(i)\n",
    "        \n",
    "    image_urls=[]                                                   # empty list to store image urls of the restaurants\n",
    "    images=soup.find_all('img',class_=\"no-img\")                     # fetching data from webpage\n",
    "    for j in images:                                                # appending desired data in the list image_urls\n",
    "        image_urls.append(j[\"data-src\"])\n",
    "    \n",
    "    cuisines=[]                                                        # empty list to store cuisines of the restaurants\n",
    "    c_names=soup.find_all('span',class_=\"double-line-ellipsis\")        # fetching data from webpage\n",
    "    for cn in c_names:                                                 # appending desired data in the list cuisines\n",
    "            cn=cn.text.split('|')[1]\n",
    "            cuisines.append(cn)\n",
    "    \n",
    "    \n",
    "    # creating a dictionary to store all the scraped data \n",
    "    restaurant_info_dict={\"Name Of Restaurant\": restaurant_names, \"Cuisine Names\": cuisines, \"Location Of the Restaurant\": locations, \"Ratings\": ratings, \"URLs of Images\": image_urls}\n",
    "    df10=pd.DataFrame(restaurant_info_dict)        # creating a dataframe and passing dictionary into it\n",
    "    df10.index=range(1,len(df10)+1)                # assigning the indices to the dataframe\n",
    "    print(df10)                                    # printing the dataframe\n",
    "    \n",
    "                                                                        \n",
    "rest_url=\"https://www.dineout.co.in/delhi-restaurants/buffet-special\"   # url of the webpage\n",
    "restaurant_info(rest_url)                                            # passing the url as a paramter when the function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed744d3e",
   "metadata": {},
   "source": [
    "# Question 10 :- Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-printed-t-shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7a533e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Name Of The Product Prices Offered  \\\n",
      "1     Women's Green Color Block Oversized Fit T-shirt          ₹ 558   \n",
      "2         Women's Red Graphic Print Oversized T-shirt          ₹ 629   \n",
      "3                    Women's Black Typography T-shirt          ₹ 575   \n",
      "4                     Women's Blue Typography T-shirt          ₹ 527   \n",
      "5                       Women's Green Striped T-shirt          ₹ 527   \n",
      "6       Women's Yellow Floral Print Oversized T-shirt          ₹ 527   \n",
      "7      Women's Pink Typographic Oversized Fit T Shirt          ₹ 531   \n",
      "8                    Women's Black Typography T-shirt          ₹ 527   \n",
      "9   Women's Blue Freedom Spirit Typography Back Pr...          ₹ 571   \n",
      "10      Women's Red Typographic Oversized Fit T Shirt          ₹ 531   \n",
      "\n",
      "                                           Image URLs  \n",
      "1   https://images.bewakoof.com/t320/difference-of...  \n",
      "2   https://images.bewakoof.com/t320/women-s-red-g...  \n",
      "3   https://images.bewakoof.com/t320/dillinger-bla...  \n",
      "4   https://images.bewakoof.com/t320/dillinger-blu...  \n",
      "5   https://images.bewakoof.com/t320/dillinger-bla...  \n",
      "6   https://images.bewakoof.com/t320/dillinger-wom...  \n",
      "7   https://images.bewakoof.com/t320/dillinger-wom...  \n",
      "8   https://images.bewakoof.com/t320/dillinger-bla...  \n",
      "9   https://images.bewakoof.com/t320/women-s-blue-...  \n",
      "10  https://images.bewakoof.com/t320/dillinger-wom...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def women_product_details(wpd):\n",
    "    \n",
    "    src_code=requests.get(product_url).content\n",
    "    soup=bs(src_code,\"html.parser\")\n",
    "    \n",
    "    product_names=[]   # empty list to store names of the products\n",
    "    prod_prices=[]     # empty list to store prices of the products\n",
    "    image_urls=[]      # empty list to store urls of the images\n",
    "    \n",
    "    # logic to store data from the webpage in prod_name\n",
    "    prod_name=soup.find('div',class_=\"productGrid\").find_all('div',class_='plp-product-card')\n",
    "    \n",
    "    # logics to fetch and append respective data from prod_name to empty lists product_names, prod_prices and image_urls\n",
    "    for p in prod_name:\n",
    "            product_names.append(p.find('div',class_=\"productCardDetail\").h3.text)\n",
    "            prod_prices.append(p.find('span',class_=\"discountedPriceText\").text)\n",
    "            image_urls.append(p.find('div',class_=\"productCardImg\").img[\"src\"])\n",
    "    \n",
    "    # dictionary to store all the scraped data\n",
    "    Prod_dict={\"Name Of The Product\": product_names, \"Prices Offered\":prod_prices, \"Image URLs\":image_urls}\n",
    "    df11=pd.DataFrame(Prod_dict)                                          # creating a dataframe and passing dictionary in it\n",
    "    df11.index=range(1,len(df11)+1)                                       # assigning indices to the dataframe\n",
    "    print(df11)                                                           # printing dataframe\n",
    "  \n",
    "    \n",
    "    \n",
    "product_url=\"https://www.bewakoof.com/women-printed-t-shirts\"             # url of the webpage\n",
    "women_product_details(product_url)                                        # passing url of the webpage as a parameter when function is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a368b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
