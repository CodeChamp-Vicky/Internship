{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "644b63da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\vikas\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\vikas\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "# Installed bs4 and requests module\n",
    "\n",
    "!pip install bs4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07b740",
   "metadata": {},
   "source": [
    "# Question 1. Write a python program to display all the header tags from https://en.wikipedia.org/wiki/Main_Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15e48408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> \n",
      "\n",
      "All the header tags on the URL are :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1> \n",
      "\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2> \n",
      "\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2> \n",
      "\n",
      "\n",
      "<h2>Navigation menu</h2> \n",
      "\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3> \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library and requests  \n",
    "\n",
    "from bs4 import BeautifulSoup          \n",
    "import requests\n",
    "\n",
    "def header_func(y):                                         # function definition\n",
    "    \n",
    "    response = requests.get(y)                              # requesting response from the Web Page\n",
    "    \n",
    "    print(response, \"\\n\")                                   # for printing response from the Web Page\n",
    "    \n",
    "    soup=BeautifulSoup(response.content)                    # creating a BeautifulSoup object for fetching required information through it\n",
    "    \n",
    "    headers=soup.find_all(['h1','h2','h3','h4','h5','h6'])  # fetching information related to all the header tags on the webpage and storing it in a container headers\n",
    "    \n",
    "    print(\"All the header tags on the URL are :\\n\")         # printing the message\n",
    "    \n",
    "    for h in headers:\n",
    "        print(h,\"\\n\\n\")                                     # printing the content stored in headers(container_name)\n",
    "\n",
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"               # URL of webpage\n",
    "header_func(url)                                            # passing URL of web page as a parameter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db99beb",
   "metadata": {},
   "source": [
    "# Question 2 :- Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3014bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Movie_Names   Movie Ratings   Year Of Release \n",
      "1               The Shawshank Redemption             9.2            (1994)\n",
      "2                          The Godfather             9.2            (1972)\n",
      "3                        The Dark Knight             9.0            (2008)\n",
      "4                 The Godfather: Part II             9.0            (1974)\n",
      "5                           12 Angry Men             9.0            (1957)\n",
      "..                                   ...             ...               ...\n",
      "96     M - Eine Stadt sucht einen Mörder             8.3            (1931)\n",
      "97                    North by Northwest             8.3            (1959)\n",
      "98                                Jagten             8.3            (2012)\n",
      "99                               Vertigo             8.3            (1958)\n",
      "100  Le fabuleux destin d'Amélie Poulain             8.3            (2001)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library as an alias bs, requests and pandas library as an alias pd \n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "  \n",
    "def top_rated_movies(x):                           # function definition\n",
    "    \n",
    "    response = requests.get(x)                         # requesting response from webpage\n",
    "    html_content=response.content                      # storing content of the webpage in a container html_content\n",
    "    soup=bs(html_content,\"html.parser\")                # creating an object of BeautifulSoup to parse through the content of webpage\n",
    "    \n",
    "    names=[]                                           # empty list for storing names of movies\n",
    "    releases=[]                                        # empty list for storing year of release of all the movies\n",
    "    ratings=[]                                         # empty list for storing ratings of the movies\n",
    "    \n",
    "    container=soup.find('tbody', class_=\"lister-list\")    # fetching content of parent containers of the required information\n",
    "    items=container.find_all('tr')                        \n",
    "    for i in items[:100]:\n",
    "        names.append(i.find('td', class_=\"titleColumn\").a.text)                       # appending names of movies to the empty list\n",
    "        releases.append(i.find('span', class_='secondaryInfo').text)                  # appending year of release of movies to the empty list\n",
    "        ratings.append(i.find('td',class_=\"ratingColumn imdbRating\").text.strip())    # appending ratings of movies to the empty list\n",
    "       \n",
    "    # dictionary to store all the scraped information\n",
    "    movie_dict={' Movie_Names ':names,' Movie Ratings ':ratings,' Year Of Release ':releases}  \n",
    "\n",
    "    df=pd.DataFrame(movie_dict)     # creation of dataframe df\n",
    "    df.index=range(1,len(df)+1)     # assigning indices to the dataframe\n",
    "    print(df)                       # printing the dataframe\n",
    "    \n",
    "\n",
    "imdb_url=\"https://www.imdb.com/chart/top?sort=ir,desc&mode=simple&page=1\"  # Url of the webpage\n",
    "top_rated_movies(imdb_url)    # url passed as a parameter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc01cb1",
   "metadata": {},
   "source": [
    "# Question 3 :- Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7930bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Movie_Names   Movie_Ratings   Year_Of_Release \n",
      "1                       Jai Bhim             8.4            (2021)\n",
      "2                     Anbe Sivam             8.4            (2003)\n",
      "3              Pariyerum Perumal             8.4            (2018)\n",
      "4                        Golmaal             8.4            (1979)\n",
      "5                        Nayakan             8.4            (1987)\n",
      "..                           ...             ...               ...\n",
      "96    The Legend of Bhagat Singh             8.0            (2002)\n",
      "97                        Sholay             8.0            (1975)\n",
      "98                        Angoor             8.0            (1982)\n",
      "99   Baahubali 2: The Conclusion             8.0            (2017)\n",
      "100      Maheshinte Prathikaaram             8.0            (2016)\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing BeautifulSoup library as bs, requests library, and pandas library as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def top_rated_indian_movies(y):           # function definition\n",
    "    \n",
    "    response=requests.get(y)              # requesting response from the webpage url\n",
    "    html_content=response.content         # storing html content of webpage in a container \n",
    "    soup=bs(html_content,'html.parser')   # creating a BeautifulSoup Object to parse through the content of the webpage\n",
    "    \n",
    "    names=[]                                                        # empty list to store names of movies\n",
    "    ratings=[]                                                      # created an empty list to store ratings of movies\n",
    "    release_years=[]                                                # empty list to store year of release of all the movies\n",
    "    \n",
    "    container=soup.find('tbody', class_=\"lister-list\")              # fetching content of parent containers of the required information\n",
    "    items=container.find_all('tr')\n",
    "    \n",
    "    for i in items[:100]:\n",
    "            names.append(i.find('td',class_=\"titleColumn\").a.text)                      # appending names of movies to the empty list\n",
    "            ratings.append(i.find('td',class_=\"ratingColumn imdbRating\").text.strip())  # appending ratings of movies to the empty list\n",
    "            release_years.append(i.find('span',class_=\"secondaryInfo\").text)            # appending year of release of movies to the empty list\n",
    "         \n",
    "    \n",
    "    # creating dictionary to store all the scraped information \n",
    "    info_dict={' Movie_Names ':names,' Movie_Ratings ':ratings,' Year_Of_Release ':release_years}   \n",
    "    df1=pd.DataFrame(info_dict)                                     # creating dataframe and passing dictionary as a parameter\n",
    "    df1.index=range(1,len(df1)+1)                                   # assigning indices to the dataframe\n",
    "    print(df1)                                                      # printing the dataframe\n",
    "                                                                                                                                                    \n",
    "\n",
    "indian_url=\"https://www.imdb.com/india/top-rated-indian-movies/\"         # URL of web page\n",
    "top_rated_indian_movies(indian_url)                                  # url passed as a parameter when function is called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd6bc",
   "metadata": {},
   "source": [
    "# Question 4 :- Write a python program to scrape product name, price and discounts from https://meesho.com/bags-ladies/pl/p7vbp ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25f45521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Product Names Product Prices  \\\n",
      "1           Gorgeous Classy Women Handbags           ₹238   \n",
      "2             Trendy Classy Women Handbags           ₹278   \n",
      "3         Classic Versatile Women Handbags           ₹922   \n",
      "4            Elegant Classy Women Handbags           ₹762   \n",
      "5        Trendy Fashionable Women Handbags           ₹389   \n",
      "6               Elite Fancy Women Handbags           ₹329   \n",
      "7          Ravishing Classy Women Handbags           ₹673   \n",
      "8         Voguish Versatile Women Handbags           ₹674   \n",
      "9         Trendy Attractive Women Handbags           ₹625   \n",
      "10          Elite Versatile Women Handbags           ₹733   \n",
      "11        Graceful Alluring Women Handbags           ₹285   \n",
      "12     Graceful Fashionable Women Handbags           ₹380   \n",
      "13           Graceful Fancy Women Handbags           ₹673   \n",
      "14       Classic Attractive Women Handbags           ₹645   \n",
      "15  Elegant Classic Women's Hangbags Vol 1           ₹447   \n",
      "16       Ravishing Alluring Women Handbags           ₹387   \n",
      "17         Elite Attractive Women Handbags           ₹212   \n",
      "18         Elegant Alluring Women Handbags           ₹932   \n",
      "19        Graceful Alluring Women Handbags           ₹310   \n",
      "20     Graceful Fashionable Women Handbags           ₹826   \n",
      "\n",
      "            Discounts Offered  \n",
      "1   ₹41 discount on 1st order  \n",
      "2   ₹49 discount on 1st order  \n",
      "3   ₹50 discount on 1st order  \n",
      "4   ₹50 discount on 1st order  \n",
      "5   ₹50 discount on 1st order  \n",
      "6   ₹50 discount on 1st order  \n",
      "7   ₹50 discount on 1st order  \n",
      "8   ₹50 discount on 1st order  \n",
      "9   ₹50 discount on 1st order  \n",
      "10  ₹50 discount on 1st order  \n",
      "11  ₹50 discount on 1st order  \n",
      "12  ₹50 discount on 1st order  \n",
      "13  ₹50 discount on 1st order  \n",
      "14  ₹50 discount on 1st order  \n",
      "15  ₹50 discount on 1st order  \n",
      "16  ₹50 discount on 1st order  \n",
      "17  ₹37 discount on 1st order  \n",
      "18  ₹50 discount on 1st order  \n",
      "19  ₹50 discount on 1st order  \n",
      "20  ₹50 discount on 1st order  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup and pandas libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def meesho_info(mi):                            # Function definition \n",
    "    response=requests.get(mi)                   # requesting response from the webpage \n",
    "    html_content=response.content               # fetching and storing content of the webpage in a container\n",
    "    soup=bs(html_content,'html.parser')         # creating a BeautifulSoup object to parse the html content with html parser\n",
    "   \n",
    "    articles=soup.find_all('div', class_=\"Card__BaseCard-sc-b3n78k-0 lfjhF NewProductCard__CardStyled-sc-j0e7tu-0 hzPSVW NewProductCard__CardStyled-sc-j0e7tu-0 hzPSVW\")\n",
    "    \n",
    "    prod_names=[]                            # empty list to store product names\n",
    "    prod_prices=[]                           # empty list to store product prices\n",
    "    prod_discounts=[]                        # empty list to store discounts offered\n",
    "    for a in articles:\n",
    "        prod_names.append(a.find('p',class_=\"Text__StyledText-sc-oo0kvp-0 cPgaBh NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw NewProductCard__ProductTitle_Desktop-sc-j0e7tu-4 hofZGw\").text.strip())\n",
    "        prod_prices.append(a.find('h5').text)\n",
    "        prod_discounts.append(a.find('p',class_=\"Text__StyledText-sc-oo0kvp-0 iDRzyZ NewProductCard__DiscountTextParagraph-sc-j0e7tu-16 dppwvY NewProductCard__DiscountTextParagraph-sc-j0e7tu-16 dppwvY\").text)\n",
    "    \n",
    "    # created a dictionary to store all the scraped information\n",
    "    Meesho_dict={'Product Names':prod_names, 'Product Prices':prod_prices, 'Discounts Offered':prod_discounts}\n",
    "    df2=pd.DataFrame(Meesho_dict)     # creating a dataframe and passing the dictionary in it\n",
    "    df2.index=range(1,len(df2)+1)     # assigning indices to the dataframe created\n",
    "    print(df2)                        # printing DataFrame \n",
    "    \n",
    "    \n",
    "bags_url=\"https://meesho.com/bags-ladies/pl/p7vbp\"    # url of webpage \n",
    "meesho_info(bags_url)                                 # passing the url as a paramter while calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9e274",
   "metadata": {},
   "source": [
    "# Question 5 :- Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e97abc",
   "metadata": {},
   "source": [
    "# 5 a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65c68863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Names of the Teams Total Matches Total Points Total Ratings\n",
      "1         New Zealand            19        2,316           122\n",
      "2             England            32        3,793           119\n",
      "3           Australia            31        3,475           112\n",
      "4               India            38        4,162           110\n",
      "5        South Africa            31        3,167           102\n",
      "6            Pakistan            30        2,921            97\n",
      "7          Bangladesh            36        3,350            93\n",
      "8           Sri Lanka            35        2,835            81\n",
      "9         West Indies            36        2,788            77\n",
      "10        Afghanistan            23        1,562            68\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup and pandas libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_odi_men_teams(a):                    # function definition\n",
    "     \n",
    "    response=requests.get(a)              # requesting a response from the web page\n",
    "    html_content=response.content            # storing html content of webpage in a container\n",
    "    soup=bs(html_content, 'html.parser')     # creating a BeautifulSoup object to parse thorugh the html content from the web page through html parser\n",
    "    \n",
    "    team_names=[]                                           # empty list to store names of teams\n",
    "    team_matches=[]                                         # empty list to store total matches played\n",
    "    team_points=[]                                          # empty list to store total points of the teams\n",
    "    team_ratings=[]                                         # empty list to store ratings of all the teams\n",
    "    parent=soup.find('tbody')\n",
    "    contents=parent.find_all('tr')                          # fetching parent containers of the required content\n",
    "    for c in contents[:10]:\n",
    "        team_names.append(c.find_all('td')[1].find_all('span')[1].text)   # appending names of teams\n",
    "        team_matches.append(c.find_all('td')[2].text)                     # appending total matches of teams\n",
    "        team_points.append(c.find_all('td')[3].text)                      # appending total points of teams\n",
    "        team_ratings.append(c.find_all('td')[4].text.strip())             # appending total ratings of the team\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    odi_dict={\"Names of the Teams\" : team_names, \"Total Matches\":team_matches, \"Total Points\": team_points, \"Total Ratings\":team_ratings}\n",
    "    df3=pd.DataFrame(odi_dict)                       # creating a dataframe and passing dictionary into it\n",
    "    df3.index=range(1,len(df3)+1)                    # assigning indices to the dataframe\n",
    "    print(df3)                                       # printing dataframe\n",
    "\n",
    "\n",
    "men_url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"    # URL of the webpage\n",
    "top_odi_men_teams(men_url)                                                  # passing the url as a paramter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664f961",
   "metadata": {},
   "source": [
    "# 5 b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfb59a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name Of The Player Name Of The Team Player Rating\n",
      "1              Babar Azam              PAK           872\n",
      "2             Virat Kohli              IND           811\n",
      "3             Ross Taylor               NZ           794\n",
      "4            Rohit Sharma              IND           791\n",
      "5         Quinton de Kock               SA           789\n",
      "6          Jonny Bairstow              ENG           775\n",
      "7             Aaron Finch              AUS           771\n",
      "8   Rassie van der Dussen               SA           769\n",
      "9            David Warner              AUS           758\n",
      "10            Imam-ul-Haq              PAK           746\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_batsmen_info(bat):                         # function definition\n",
    "    response=requests.get(bat)                     # requesting response from the web page server\n",
    "    html_content=response.content                  # fetching html content of the webpage\n",
    "    soup=bs(html_content,'html.parser')            # creating object of Beautiful Soup to parse through the html content of the webpage\n",
    "    \n",
    "    batting_container=soup.find_all('div',class_=\"rankings-block__container\")[0]     # parent container for batting related information\n",
    "    first_player_info=batting_container.find('div',class_=\"rankings-block__banner--player-info\")  # fetching parent class where first player's information is stored\n",
    "    player_names=[first_player_info.find('div',class_=\"rankings-block__banner--name\").text]     # appending name of the player\n",
    "    team_names=[first_player_info.find('div',class_=\"rankings-block__banner--nationality\").text.split()[0]] # appending team of the player\n",
    "    player_ratings=[first_player_info.find('div',class_=\"rankings-block__banner--rating\").text]             # appending ratings of the player\n",
    "    \n",
    "    other_players_info=batting_container.find('tbody').find_all('tr')   # fetching parent container for other player's information\n",
    "    for op in other_players_info:       \n",
    "        player_names.append(op.find_all('td')[1].text.strip())     # appending names of players\n",
    "        team_names.append(op.find_all('td')[2].text.strip())       # appending names of the teams\n",
    "        player_ratings.append(op.find_all('td')[3].text)           # appending ratings of the players\n",
    "    \n",
    "\n",
    "# creating a dictionary to store all the scraped data from the webpage\n",
    "    player_dict={\"Name Of The Player\":player_names,\"Name Of The Team\":team_names,\"Player Rating\":player_ratings}\n",
    "    \n",
    "    df4=pd.DataFrame(player_dict)     # creating a dataframe and passing dictionary in it\n",
    "    df4.index=range(1,len(df4)+1)     # assigning indices to the dataframe\n",
    "    print(df4)                        # printing dataframe\n",
    "    \n",
    "    \n",
    "batsmen_url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"    # url of the webpage\n",
    "top_batsmen_info(batsmen_url)                                                  # passing url as a paramter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ad1cd",
   "metadata": {},
   "source": [
    "# 5 c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c796f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Of The Player Name Of The Team Player Rating\n",
      "1         Trent Boult               NZ           733\n",
      "2      Josh Hazlewood              AUS           705\n",
      "3        Chris Woakes              ENG           700\n",
      "4          Matt Henry               NZ           687\n",
      "5    Mujeeb Ur Rahman              AFG           681\n",
      "6      Jasprit Bumrah              IND           679\n",
      "7        Mehedi Hasan              BAN           661\n",
      "8     Shakib Al Hasan              BAN           657\n",
      "9          Adam Zampa              AUS           650\n",
      "10        Rashid Khan              AFG           650\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_bowlers_odi(ball):                         # function definition\n",
    "    response=requests.get(ball)                    # requesting response from the web page server\n",
    "    html_content=response.content                  # fetching html content of the webpage\n",
    "    soup=bs(html_content,'html.parser')            # creating object of Beautiful Soup to parse through the html content of the webpage\n",
    "    \n",
    "    bowling_container=soup.find_all('div',class_=\"rankings-block__container\")[1]                 # parent container for bowling related information\n",
    "    first_player_info=bowling_container.find('div',class_=\"rankings-block__banner--player-info\") # fetching parent class where first player's information is stored\n",
    "    player_names=[first_player_info.find('div',class_=\"rankings-block__banner--name\").text]      # appending name of the player\n",
    "    team_names=[first_player_info.find('div',class_=\"rankings-block__banner--nationality\").text.split()[0]] # appending team of first player\n",
    "    player_ratings=[first_player_info.find('div',class_=\"rankings-block__banner--rating\").text]             # appending ratings of first player\n",
    "    \n",
    "    other_players_info=bowling_container.find('tbody').find_all('tr')  # fetching parent container for other player's information\n",
    "    for op in other_players_info:       \n",
    "        player_names.append(op.find_all('td')[1].text.strip())            # appending name of the players\n",
    "        team_names.append(op.find_all('td')[2].text.strip())              # appending team of other players\n",
    "        player_ratings.append(op.find_all('td')[3].text)                  # appending ratings of other players\n",
    "    \n",
    "\n",
    "    # creating a dictionary to store all the scraped data from the webpage\n",
    "    player_dict={\"Name Of The Player\":player_names,\"Name Of The Team\":team_names,\"Player Rating\":player_ratings}\n",
    "    \n",
    "    df4=pd.DataFrame(player_dict)     # creating a dataframe and passing dictionary in it\n",
    "    df4.index=range(1,len(df4)+1)     # assigning indices to the dataframe\n",
    "    print(df4)                        # printing dataframe\n",
    "    \n",
    "    \n",
    "bowler_url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"    # url of the webpage\n",
    "top_bowlers_odi(bowler_url)                                                  # passing url as a paramter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a084aef",
   "metadata": {},
   "source": [
    "# Question 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3830a2",
   "metadata": {},
   "source": [
    "# 6 a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acbbaf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Names of the Teams Total Matches Total Points Total Ratings\n",
      "1           Australia            28        4,663           167\n",
      "2        South Africa            28        3,504           125\n",
      "3             England            29        3,425           118\n",
      "4               India            29        2,890           100\n",
      "5         New Zealand            31        3,018            97\n",
      "6         West Indies            28        2,478            89\n",
      "7          Bangladesh            12          935            78\n",
      "8            Pakistan            26        1,753            67\n",
      "9             Ireland             5          240            48\n",
      "10          Sri Lanka             5          233            47\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup and pandas libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def women_odi_teams(w):                     # function definition\n",
    "     \n",
    "    response=requests.get(w)                # requesting a response from the web page\n",
    "    html_content=response.content           # storing html content of webpage in a container\n",
    "    soup=bs(html_content,'html.parser')     # creating a BeautifulSoup object to parse thorugh the html content from the web page through html parser\n",
    "    \n",
    "    team_names=[]                                           # empty list to store names of teams\n",
    "    team_matches=[]                                         # empty list to store total matches played\n",
    "    team_points=[]                                          # empty list to store total points of the teams\n",
    "    team_ratings=[]                                         # empty list to store ratings of all the teams\n",
    "    parent=soup.find('tbody')\n",
    "    contents=parent.find_all('tr')            # fetching parent tags where whole information is stored\n",
    "    for c in contents[:10]:\n",
    "        team_names.append(c.find_all('td')[1].find_all('span')[1].text)   # appending names of teams\n",
    "        team_matches.append(c.find_all('td')[2].text)                     # appending total matches of teams\n",
    "        team_points.append(c.find_all('td')[3].text)                      # appending total points of the teams\n",
    "        team_ratings.append(c.find_all('td')[4].text.strip())             # appending total ratings of the teams\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    odi_dict={\"Names of the Teams\" : team_names, \"Total Matches\":team_matches, \"Total Points\": team_points, \"Total Ratings\":team_ratings}\n",
    "    df3=pd.DataFrame(odi_dict)                       # creating a dataframe and passing dictionary into it\n",
    "    df3.index=range(1,len(df3)+1)                    # assigning indices to the dataframe\n",
    "    print(df3)                                       # printing dataframe\n",
    "\n",
    "\n",
    "women_team_url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"    # URL of the webpage\n",
    "women_odi_teams(women_team_url)                                                 # passing the url as a paramter when function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c811798",
   "metadata": {},
   "source": [
    "# 6 b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a279f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Of The Player Name Of The Team Player Rating\n",
      "1     Laura Wolvaardt               SA           740\n",
      "2         Beth Mooney              AUS           726\n",
      "3         Meg Lanning              AUS           718\n",
      "4      Natalie Sciver              ENG           705\n",
      "5        Alyssa Healy              AUS           703\n",
      "6         Mithali Raj              IND           686\n",
      "7      Rachael Haynes              AUS           684\n",
      "8      Tammy Beaumont              ENG           682\n",
      "9   Amy Satterthwaite               NZ           681\n",
      "10    Smriti Mandhana              IND           669\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_women_batsmen_info(w_bat):                          # function definition\n",
    "    response=requests.get(w_bat)                    # requesting response from the web page server\n",
    "    html_content=response.content                   # fetching html content of the webpage\n",
    "    soup=bs(html_content,'html.parser')               # creating object of Beautiful Soup to parse through the html content of the webpage\n",
    "    \n",
    "    batting_container=soup.find_all('div',class_=\"rankings-block__container\")[0]                            # fetching parent container where batting related information is stored\n",
    "    first_player_info=batting_container.find('div',class_=\"rankings-block__banner--player-info\")            # fetching container where information about first player is stored\n",
    "    player_names=[first_player_info.find('div',class_=\"rankings-block__banner--name\").text]                 # appending name of first player\n",
    "    team_names=[first_player_info.find('div',class_=\"rankings-block__banner--nationality\").text.split()[0]] # appending team of first player\n",
    "    player_ratings=[first_player_info.find('div',class_=\"rankings-block__banner--rating\").text]             # appending ratings of first player\n",
    "    \n",
    "    other_players_info=batting_container.find('tbody').find_all('tr')       # fetching parent container where information of other players is stored\n",
    "    for op in other_players_info:       \n",
    "        player_names.append(op.find_all('td')[1].text.strip())              # appending names of other players\n",
    "        team_names.append(op.find_all('td')[2].text.strip())                # appending teams of other players\n",
    "        player_ratings.append(op.find_all('td')[3].text)                    # appending ratings of other players\n",
    "    \n",
    "\n",
    "    # creating a dictionary to store all the scraped data from the webpage\n",
    "    player_dict={\"Name Of The Player\":player_names,\"Name Of The Team\":team_names,\"Player Rating\":player_ratings}\n",
    "    \n",
    "    df4=pd.DataFrame(player_dict)     # creating a dataframe and passing dictionary in it\n",
    "    df4.index=range(1,len(df4)+1)     # assigning indices to the dataframe\n",
    "    print(df4)                        # printing dataframe\n",
    "    \n",
    "    \n",
    "women_batsmen_url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"  # url of the webpage\n",
    "top_women_batsmen_info(women_batsmen_url)                                                  # passing url as a paramter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba04ce",
   "metadata": {},
   "source": [
    "# 6 c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64dbf0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name Of The Player Name Of The Team Player Rating\n",
      "1        Ellyse Perry              AUS           404\n",
      "2      Natalie Sciver              ENG           376\n",
      "3      Marizanne Kapp               SA           359\n",
      "4     Hayley Matthews               WI           340\n",
      "5         Amelia Kerr               NZ           335\n",
      "6    Ashleigh Gardner              AUS           278\n",
      "7       Deepti Sharma              IND           249\n",
      "8       Jess Jonassen              AUS           246\n",
      "9     Katherine Brunt              ENG           239\n",
      "10     Jhulan Goswami              IND           217\n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def top_women_allrounder_odi(ar):                           # function definition\n",
    "    response=requests.get(ar)                               # requesting response from the web page server\n",
    "    html_content=response.content                           # fetching html content of the webpage\n",
    "    soup=bs(html_content,'html.parser')                     # creating object of Beautiful Soup to parse through the html content of the webpage\n",
    "    \n",
    "    ar_container=soup.find_all('div',class_=\"rankings-block__container\")[2]                            # fetching container where allrounders related information is stored\n",
    "    first_player_info=ar_container.find('div',class_=\"rankings-block__banner--player-info\")            # fetching information related to first player\n",
    "    \n",
    "    ar_player_names=[first_player_info.find('div',class_=\"rankings-block__banner--name\").text]                 # appending name of first player\n",
    "    ar_team_names=[first_player_info.find('div',class_=\"rankings-block__banner--nationality\").text.split()[0]] # appending team of first player\n",
    "    ar_player_ratings=[first_player_info.find('div',class_=\"rankings-block__banner--rating\").text]             # appending ratings of first player\n",
    "    \n",
    "    other_players_info=ar_container.find('tbody').find_all('tr')         # fetching container where information related to other players is stored\n",
    "    for op in other_players_info:       \n",
    "        ar_player_names.append(op.find_all('td')[1].text.strip())        # appending names of other players\n",
    "        ar_team_names.append(op.find_all('td')[2].text.strip())          # appending teams of other players\n",
    "        ar_player_ratings.append(op.find_all('td')[3].text)              # appending ratings of other players\n",
    "    \n",
    "\n",
    "    # creating a dictionary to store all the scraped data from the webpage\n",
    "    player_dict={\"Name Of The Player\":ar_player_names,\"Name Of The Team\":ar_team_names,\"Player Rating\":ar_player_ratings}\n",
    "    \n",
    "    df4=pd.DataFrame(player_dict)     # creating a dataframe and passing dictionary in it\n",
    "    df4.index=range(1,len(df4)+1)     # assigning indices to the dataframe\n",
    "    print(df4)                        # printing dataframe\n",
    "    \n",
    "    \n",
    "women_allrounder_url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"    # url of the webpage\n",
    "top_women_allrounder_odi(women_allrounder_url)                                            # passing url as a paramter when calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ffee6",
   "metadata": {},
   "source": [
    "# Question 7 :- Write a python program to scrape details of all the posts from coreyms.com. Scrape the heading, date, content, and the code for the video from the link for the youtube video from the post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd90dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Headings On Website    Date Of Publish  \\\n",
      "1   Python Tutorial: Zip Files – Creating and Extr...   November 19 2019   \n",
      "2   Python Data Science Tutorial: Analyzing the 20...    October 17 2019   \n",
      "3   Python Multiprocessing Tutorial: Run Code in P...  September 21 2019   \n",
      "4   Python Threading Tutorial: Run Code Concurrent...  September 12 2019   \n",
      "5                                 Update (2019-09-03)   September 3 2019   \n",
      "6   Python Quick Tip: The Difference Between “==” ...      August 6 2019   \n",
      "7   Python Tutorial: Calling External Commands Usi...       July 24 2019   \n",
      "8   Visual Studio Code (Windows) – Setting up a Py...         May 1 2019   \n",
      "9   Visual Studio Code (Mac) – Setting up a Python...         May 1 2019   \n",
      "10  Clarifying the Issues with Mutable Default Arg...      April 24 2019   \n",
      "\n",
      "                                 Theoritical Contents  \\\n",
      "1   In this video, we will be learning how to crea...   \n",
      "2   In this Python Programming video, we will be l...   \n",
      "3   In this Python Programming video, we will be l...   \n",
      "4   In this Python Programming video, we will be l...   \n",
      "5   Hey everyone. I wanted to give you an update o...   \n",
      "6   In this Python Programming Tutorial, we will b...   \n",
      "7   In this Python Programming Tutorial, we will b...   \n",
      "8   In this Python Programming Tutorial, we will b...   \n",
      "9   In this Python Programming Tutorial, we will b...   \n",
      "10  In this Python Programming Tutorial, we will b...   \n",
      "\n",
      "                                Video Links  \n",
      "1   https://youtube.com/watch?v=z0gguhEmWiY  \n",
      "2   https://youtube.com/watch?v=_P7X8tMplsw  \n",
      "3   https://youtube.com/watch?v=fKl2JW_qrso  \n",
      "4   https://youtube.com/watch?v=IEEhzQoKtQU  \n",
      "5                  Video Link Not Available  \n",
      "6   https://youtube.com/watch?v=mO_dS3rXDIs  \n",
      "7   https://youtube.com/watch?v=2Fp1N6dof0Y  \n",
      "8   https://youtube.com/watch?v=-nh9rCzPJ20  \n",
      "9   https://youtube.com/watch?v=06I63_p-2A4  \n",
      "10  https://youtube.com/watch?v=_JGmemuINww  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Website_content(wc):                                   # function definition\n",
    "    response=requests.get(wc)\n",
    "    html_content=response.content                          # requesting content from the webpage and storing it in a variable src_code\n",
    "    soup=bs(html_content,'html.parser')                    # creating a BeautifulSoup object to parse through the webpage content\n",
    "    \n",
    "    articles=soup.find_all('article')                             # fetching article tags from the webpage\n",
    "    web_headings=[]                                               # empty list to store headings on the webpage\n",
    "    web_dates=[]                                                  # empty list to store dates on which the respective article was published\n",
    "    web_contents=[]                                               # empty list to store theoritical content from the webpage\n",
    "    web_video_links=[]                                            # empty list to store all the video links on the webpage\n",
    "    \n",
    "    for article in articles:\n",
    "                                               \n",
    "        web_headings.append(article.find('a',class_=\"entry-title-link\").text.strip())     # appending headings of all the articles to web_headings  \n",
    "                                                                \n",
    "        web_dates.append(article.find('time',class_=\"entry-time\").text.replace(',',''))   # appending dates of publish of all the articles to web_dates  \n",
    "                                                           \n",
    "        web_contents.append(article.find('div',class_=\"entry-content\").p.text)  # appending theoritical content of each article to the list web_contents           \n",
    "        \n",
    "          \n",
    "        if article.find('iframe'):                                              # appending video links of all the articles to web_video_links\n",
    "            video_links=article.find('iframe',class_=\"youtube-player\")['src']\n",
    "            video_ids=video_links.split('/')[4].split('?')[0]                    \n",
    "            yt_video_links=\"https://youtube.com/watch?v={}\".format(video_ids) \n",
    "            web_video_links.append(yt_video_links)   \n",
    "        else:\n",
    "            web_video_links.append(\"Video Link Not Available\")     # appending this message where video link is not available\n",
    "    \n",
    "    # creating dictionary to store all the scraped data in it  \n",
    "    website_info_dict={\"Headings On Website\":web_headings, \"Date Of Publish\":web_dates, \"Theoritical Contents\":web_contents, \"Video Links\":web_video_links}\n",
    "    df9=pd.DataFrame(website_info_dict)                  # creating a dataframe and passing dictionary in it                                    \n",
    "    df9.index=range(1,len(df9)+1)                        # assigning indices to the dataframe                     \n",
    "    print(df9)                                           # printing dataframe                     \n",
    "\n",
    "web_url=\"https://coreyms.com/\"         # url of the webpage\n",
    "Website_content(web_url)               # passing url as a parameter when the function is called \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc5f67",
   "metadata": {},
   "source": [
    "# Question 8 :- Write a python program to scrape house details from mentioned URL. It should include house title, location, area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar, Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "263f733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Name Of The House  \\\n",
      "1   4+ BHK In Independent House  For Sale  In Raja...   \n",
      "2   2 BHK Apartment  For Sale  In Tarang Parkway A...   \n",
      "3   2 BHK Flat  For Sale  In Kumar Ashraya Apartme...   \n",
      "4   3 BHK Apartment  For Sale  In Ashiana Gardens ...   \n",
      "5   3 BHK Apartment  For Sale  In Brigade Gateway ...   \n",
      "6   2 BHK Apartment  For Sale  In Blueberry Apartm...   \n",
      "7   3 BHK Apartment  For Sale  In Mayitri Enclave ...   \n",
      "8   4+ BHK In Independent House  For Sale  In Prak...   \n",
      "9   2 BHK Flat  For Sale  In Jains Prakruti, Jayan...   \n",
      "10  4 BHK In Independent House  For Sale  In Jayan...   \n",
      "11  3 BHK Flat  For Sale  In Benaka Apartments In ...   \n",
      "12  2 BHK Apartment  For Sale  In Jains Prakriti I...   \n",
      "13  2 BHK Flat  For Sale  In Jayanagar Residency I...   \n",
      "14  4 BHK Flat  For Sale  In Rrbc Piccassso In Jay...   \n",
      "15  4 BHK Flat  For Sale  In Phoenix  One Bangalor...   \n",
      "16  4+ BHK In Independent House  For Sale  In Laks...   \n",
      "17              3 BHK Flat  For Sale  In Rajaji Nagar   \n",
      "18  3 BHK Flat  For Sale  In  Sgrr Pallavi Pristin...   \n",
      "19  3 BHK Flat  For Sale  In Hal 2nd Stage,indiran...   \n",
      "20  4 BHK Flat  For Sale  In Golfridge In Indira N...   \n",
      "21  4+ BHK In Independent House  For Sale  In Raja...   \n",
      "22  4 BHK In Independent House  For Sale  In Jayan...   \n",
      "23  2 BHK Flat  For Sale  In Aishwaryam Gokula In ...   \n",
      "24  3 BHK Apartment  For Sale  In Admiralty Manor,...   \n",
      "25  2 BHK Flat  For Sale  In Aishwaryam Ranga In G...   \n",
      "\n",
      "                                Location of the House Area of the House  \\\n",
      "1   Independent House,  6th Cross road,9th main ro...        1,320 sqft   \n",
      "2   Tarang Parkway Apartment, 2nd Main Rd, Shivana...        1,200 sqft   \n",
      "3    Kumar Ashraya Apartments 194, 9th Cross Rd 2n...          900 sqft   \n",
      "4   Ashiana Gardens Apartments, 4, Sri Rama Temple...        1,780 sqft   \n",
      "5   Brigade Gateway, Dr Rajkumar Road, Rajaji Naga...        1,640 sqft   \n",
      "6   Blueberry Apartment, 13th E Main Rd, Channakes...        1,074 sqft   \n",
      "7   Mayitri Enclave, Mayitri Enclave, 39th C, 5T B...        1,450 sqft   \n",
      "8   Independent House, 6th C cross 3rd main rd nea...        2,200 sqft   \n",
      "9   Kanakapura Road, Jayanagar, Bangalore, Karnata...        1,301 sqft   \n",
      "10  Independent House, SBI Branch Jayanagar 9th bl...        2,600 sqft   \n",
      "11  871, 5th Cross Rd, Indira Nagar 1st Stage, Sta...        1,331 sqft   \n",
      "12  Jains Prakriti,  Kanakapura Rd, 7th Block West...        1,298 sqft   \n",
      "13  Jayanagar Residency, K V Layout, LIC Colony, J...        1,200 sqft   \n",
      "14  Municipal No.152, 18th main road, beside Sai B...        4,859 sqft   \n",
      "15  Dr Rajkumar Rd, opposite Sheraton Hotel, Rajaj...        2,901 sqft   \n",
      "16        Independent House, CMH Rd , Near B P Bakery        2,250 sqft   \n",
      "17  Independent House, 242, 27th cross, 2nd Block,...        1,275 sqft   \n",
      "18  11th Main Road,36 Cross, 4th T Block, Near ,Po...        1,340 sqft   \n",
      "19  Standalone building, 16th F main, Hal 2nd stag...        2,200 sqft   \n",
      "20                             embassy golf link road        2,750 sqft   \n",
      "21  Independent House, 6th Cross Rd, 2nd Block  ne...        2,240 sqft   \n",
      "22  Independent House, 47th Cross Rd, 8th Block, 1...        3,700 sqft   \n",
      "23            1st main govindraja naagar blore 560040        1,209 sqft   \n",
      "24  Admiralty Manor, Indiranagar, Admiralty Manor,...        1,960 sqft   \n",
      "25                        Near gayatrinagarpostoffice        1,150 sqft   \n",
      "\n",
      "    EMI of the House Price of the House  \n",
      "1      ₹63,045/Month        ₹1.1 Crores  \n",
      "2      ₹57,314/Month           ₹1 Crore  \n",
      "3      ₹37,827/Month           ₹66 Lacs  \n",
      "4      ₹80,240/Month        ₹1.4 Crores  \n",
      "5    ₹1.5 Lacs/Month       ₹2.62 Crores  \n",
      "6      ₹87,691/Month       ₹1.53 Crores  \n",
      "7      ₹44,705/Month           ₹78 Lacs  \n",
      "8      ₹77,374/Month       ₹1.35 Crores  \n",
      "9      ₹85,971/Month        ₹1.5 Crores  \n",
      "10  ₹1.55 Lacs/Month        ₹2.7 Crores  \n",
      "11     ₹48,717/Month           ₹85 Lacs  \n",
      "12  ₹1.15 Lacs/Month          ₹2 Crores  \n",
      "13     ₹57,314/Month           ₹1 Crore  \n",
      "14  ₹3.91 Lacs/Month       ₹6.83 Crores  \n",
      "15  ₹2.72 Lacs/Month       ₹4.75 Crores  \n",
      "16     ₹63,045/Month        ₹1.1 Crores  \n",
      "17     ₹77,374/Month       ₹1.35 Crores  \n",
      "18     ₹74,508/Month        ₹1.3 Crores  \n",
      "19  ₹2.12 Lacs/Month        ₹3.7 Crores  \n",
      "20  ₹1.63 Lacs/Month       ₹2.85 Crores  \n",
      "21     ₹83,679/Month       ₹1.46 Crores  \n",
      "22  ₹5.44 Lacs/Month        ₹9.5 Crores  \n",
      "23     ₹44,705/Month           ₹78 Lacs  \n",
      "24     ₹91,703/Month        ₹1.6 Crores  \n",
      "25     ₹51,583/Month           ₹90 Lacs  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def House_details(hd):                               # function definition\n",
    "    response=requests.get(hd)                        # requesting response from the webpage server\n",
    "    html_content=response.content                    # fetching html content of the webpage\n",
    "    soup=bs(html_content,\"html.parser\")              # creating object of BeautifulSoup to parse through the src_code\n",
    "    articles=soup.find_all('article')                # fetching all the article tags from webpage\n",
    "    \n",
    "    h_loc=[]                                         # empty list to store location of the houses\n",
    "    h_name=[]                                        # empty list to store name of the houses\n",
    "    h_area=[]                                        # empty list to store area of houses\n",
    "    h_emi=[]                                         # empty list to store value of EMI of the houses\n",
    "    h_price=[]                                       # empty list to store prices of the houses\n",
    "    \n",
    "    # logics to append the required info in the respective empty container lists above\n",
    "    for a in articles:      \n",
    "        h_name.append(a.find('h2', class_='heading-6 flex items-center font-semi-bold m-0').text)\n",
    "        h_area.append(a.find('div',class_=\"font-semi-bold heading-6\").text)\n",
    "        h_loc.append(a.find('div', class_=\"mt-0.5p overflow-hidden overflow-ellipsis whitespace-nowrap max-w-70 text-gray-light leading-4 po:mb-0 po:max-w-95\").text.replace('\\xa0',','))\n",
    "        h_emi.append(a.find('div',class_=\"font-semi-bold heading-6\",id=\"roomType\").text)\n",
    "        h_price.append(a.find('div',class_=\"flex flex-col w-33pe items-center bo tp:w-half po:w-full border-r-0\",id=\"minDeposit\").span.text)\n",
    "    \n",
    "    # creating a dictionary to store all the scraped data\n",
    "    House_dict={\"Name Of The House\":h_name, \"Location of the House\":h_loc, \"Area of the House\": h_area, \"EMI of the House\":h_emi, \"Price of the House\":h_price}\n",
    "    df12=pd.DataFrame(House_dict)                 # Creating a dataframe and passing dictionary into it\n",
    "    df12.index=range(1,len(df12)+1)               # assigning indices to the dataframe\n",
    "    print(df12)                                   # print(dataframe)\n",
    "\n",
    "                                                  \n",
    "# Url of the web page\n",
    "house_url=\"https://www.nobroker.in/property/sale/bangalore/multiple?searchParam=W3sibGF0IjoxMi45OTgxNzMyLCJsb24iOjc3LjU1MzA0NDU5OTk5OTk5LCJwbGFjZUlkIjoiQ2hJSnhmVzREUE05cmpzUktzTlRHLTVwX1FRIiwicGxhY2VOYW1lIjoiUmFqYWppbmFnYXIifSx7ImxhdCI6MTIuOTMwNzczNSwibG9uIjo3Ny41ODM4MzAyLCJwbGFjZUlkIjoiQ2hJSjJkZGxaNWdWcmpzUmgxQk9BYWYtb3JzIiwicGxhY2VOYW1lIjoiSmF5YW5hZ2FyIn0seyJsYXQiOjEyLjk3ODM2OTIsImxvbiI6NzcuNjQwODM1NiwicGxhY2VJZCI6IkNoSUprUU4zR0tRV3Jqc1JOaEJRSnJoR0Q3VSIsInBsYWNlTmFtZSI6IkluZGlyYW5hZ2FyIn1d&radius=2.0&city=bangalore&locality=Rajajinagar,&locality=Jayanagar,&locality=Indiranagar\"\n",
    "House_details(house_url)                         # passing url of the web page as a parameter when the function is called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9524",
   "metadata": {},
   "source": [
    "# Question 9 :- Write a python program to scrape mentioned details from https://www.dineout.co.in/delhi-restaurants/buffet-special  : i) Restaurant name, ii) Cuisine, iii) Location, iv) Ratings, v) Image URL\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ac6267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name Of Restaurant  \\\n",
      "1                    Castle Barbeque   \n",
      "2                    Jungle Jamboree   \n",
      "3                    Castle Barbeque   \n",
      "4                         Cafe Knosh   \n",
      "5               The Barbeque Company   \n",
      "6                        India Grill   \n",
      "7                     Delhi Barbeque   \n",
      "8   The Monarch - Bar Be Que Village   \n",
      "9                         World Cafe   \n",
      "10                 Indian Grill Room   \n",
      "11                   Mad 4 Bar B Que   \n",
      "12                       Barbeque 29   \n",
      "13                        Glasshouse   \n",
      "\n",
      "                                  Cuisine Names  \\\n",
      "1                         North Indian, Chinese   \n",
      "2                  North Indian, Asian, Italian   \n",
      "3                         Chinese, North Indian   \n",
      "4                          Italian, Continental   \n",
      "5                         North Indian, Chinese   \n",
      "6                         North Indian, Italian   \n",
      "7                                  North Indian   \n",
      "8                         North Indian, Chinese   \n",
      "9            North Indian, Chinese, Continental   \n",
      "10                        North Indian, Mughlai   \n",
      "11                                 North Indian   \n",
      "12   North Indian, Mughlai, Desserts, Beverages   \n",
      "13        European, Italian, Asian, Continental   \n",
      "\n",
      "                           Location Of the Restaurant Ratings  \\\n",
      "1                      Connaught Place, Central Delhi     3.5   \n",
      "2              3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
      "3              Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
      "4   The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "5                  Gardens Galleria,Sector 38A, Noida       4   \n",
      "6                Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "7      Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
      "8   Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.9   \n",
      "9    Vibe by The Lalit Traveller,Sector 35, Faridabad     4.2   \n",
      "10   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "11                               Sector 29, Faridabad     3.6   \n",
      "12                                     NIT, Faridabad     4.2   \n",
      "13  DoubleTree By Hilton Gurugram Baani Square,Sec...       4   \n",
      "\n",
      "                                       URLs of Images  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "# importing requests library, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def restaurant_info(ri):                                           # function definition\n",
    "    response=requests.get(ri)                                      # requesting response from the webpage server\n",
    "    html_content=response.content                                  # fetching html content of the webpage\n",
    "    soup=bs(html_content,\"html.parser\")                                # creating object of BeautifulSoup to parse through the webpage content\n",
    "    \n",
    "    container=soup.find('div',id=\"w1-restarant\")                   # fetching parent container where all the information about each restaurant is stored\n",
    "    restaurant_names=[]                                            # empty list to store names of the restaurants\n",
    "    locations=[]                                                   # empty list to store locations of the restaurants\n",
    "    ratings=[]                                                     # empty list to store ratings of the restaurants\n",
    "    image_urls=[]                                                  # empty list to store image urls of the restaurants\n",
    "    cuisines=[]                                                    # empty list to store cuisines of the restaurants\n",
    "    \n",
    "    restaurants=container.find_all('div',class_=\"restnt-card restaurant\")    # fetching whole information related to all the restaurants from webpage\n",
    "    for r in restaurants:\n",
    "        restaurant_info=r.find('div',class_=\"restnt-main-wrap\")              \n",
    "        restaurant_names.append(restaurant_info.find('div',class_=\"restnt-info cursor\").a.text)     # appending name of restaurant\n",
    "        locations.append(restaurant_info.find('div',class_=\"restnt-loc ellipsis\").text)             # appending location of restaurant\n",
    "        ratings.append(restaurant_info.find('div',class_=\"restnt-rating rating-4\").text)            # appending ratings of the restaurant\n",
    "        cuisines.append(restaurant_info.find('div',class_=\"detail-info\").ul.li.text.split('|')[1])  # appending cuisines of the restaurant\n",
    "        image_urls.append(restaurant_info.find('div',class_=\"img cursor\").img['data-src'])          # appending images of the restaurant\n",
    "     \n",
    "    # creating a dictionary to store all the scraped data \n",
    "    restaurant_info_dict={\"Name Of Restaurant\":restaurant_names, \"Cuisine Names\":cuisines, \"Location Of the Restaurant\":locations, \"Ratings\":ratings, \"URLs of Images\":image_urls}\n",
    "    df10=pd.DataFrame(restaurant_info_dict)        # creating a dataframe and passing dictionary into it\n",
    "    df10.index=range(1,len(df10)+1)                # assigning the indices to the dataframe\n",
    "    print(df10)                                    # printing the dataframe\n",
    "    \n",
    "                                                                        \n",
    "buffet_url=\"https://www.dineout.co.in/delhi-restaurants/buffet-special\"   # url of the webpage\n",
    "restaurant_info(buffet_url)                                               # passing the url as a paramter when the function is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed744d3e",
   "metadata": {},
   "source": [
    "# Question 10 :- Write a python program to scrape first 10 product details which include product name , price , Image URL from https://www.bewakoof.com/women-printed-t-shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6906730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Name Of The Product Prices Offered  \\\n",
      "1                          Gymedari Boyfriend T-Shirt          ₹ 299   \n",
      "2                     Forever Ninja Boyfriend T-Shirt          ₹ 299   \n",
      "3                     Forever Ninja Boyfriend T-Shirt          ₹ 299   \n",
      "4         Women's White Printed Oversized Fit T Shirt          ₹ 499   \n",
      "5                    Women's Pink Regular Fit T Shirt          ₹ 479   \n",
      "6           Women's Blue Typographic Slim Fit T Shirt          ₹ 527   \n",
      "7   Women's White Bengaluru Queen Victoria Print C...          ₹ 649   \n",
      "8                             Queen Boyfriend T-shirt          ₹ 399   \n",
      "9          Stay Motivated Varsity Half Sleeve T-shirt          ₹ 349   \n",
      "10  Women's Grey Melange Typography Slim Fit  T-shirt          ₹ 558   \n",
      "\n",
      "                                           Image URLs  \n",
      "1   https://images.bewakoof.com/t320/gymedari-boyf...  \n",
      "2   https://images.bewakoof.com/t320/forever-ninja...  \n",
      "3   https://images.bewakoof.com/t320/forever-ninja...  \n",
      "4   https://images.bewakoof.com/t320/women-s-white...  \n",
      "5   https://images.bewakoof.com/t320/campus-sutra-...  \n",
      "6   https://images.bewakoof.com/t320/dillinger-wom...  \n",
      "7   https://images.bewakoof.com/t320/nammur-women-...  \n",
      "8   https://images.bewakoof.com/t320/queen-boyfrie...  \n",
      "9   https://images.bewakoof.com/t320/stay-motivate...  \n",
      "10  https://images.bewakoof.com/t320/difference-of...  \n"
     ]
    }
   ],
   "source": [
    "# importing requests, BeautifulSoup library and pandas library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def women_product_details(wpd):                       # function definition\n",
    "    \n",
    "    response=requests.get(wpd)                        # requesting response from the webpage server\n",
    "    html_content=response.content                     # fetching html content from the webpage\n",
    "    soup=bs(html_content,\"html.parser\")               # creating BeautifulSoup object to parse through the html content of the webpage\n",
    "    \n",
    "    product_names=[]   # empty list to store names of the products\n",
    "    prod_prices=[]     # empty list to store prices of the products\n",
    "    image_urls=[]      # empty list to store urls of the images\n",
    "    \n",
    "    # logic to fetch and store data from the webpage in a container prod_name\n",
    "    prod_name=soup.find('div',class_=\"productGrid\").find_all('div',class_='plp-product-card')\n",
    "    \n",
    "    # logic to fetch and append respective data from prod_name to empty lists product_names, prod_prices and image_urls\n",
    "    for p in prod_name:\n",
    "            product_names.append(p.find('div',class_=\"productCardDetail\").h3.text) # appending names of the items to product_names\n",
    "            prod_prices.append(p.find('span',class_=\"discountedPriceText\").text)   # appending prices of the items to prod_prices\n",
    "            image_urls.append(p.find('div',class_=\"productCardImg\").img[\"src\"])    # appending urls of images of the items to image_urls\n",
    "    \n",
    "    # dictionary to store all the scraped data\n",
    "    Prod_dict={\"Name Of The Product\":product_names, \"Prices Offered\":prod_prices, \"Image URLs\":image_urls}\n",
    "    df11=pd.DataFrame(Prod_dict)                                          # creating a dataframe and passing dictionary in it\n",
    "    df11.index=range(1,len(df11)+1)                                       # assigning indices to the dataframe\n",
    "    print(df11)                                                           # printing dataframe\n",
    "  \n",
    "    \n",
    "    \n",
    "tshirt_url=\"https://www.bewakoof.com/women-printed-t-shirts\"             # url of the webpage\n",
    "women_product_details(tshirt_url)                                        # passing url of the webpage as a parameter when function is called"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
